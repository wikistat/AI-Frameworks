{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/wikistat/AI-Frameworks/blob/master/Text/2_words_embedding.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [IA Frameworks](https://github.com/wikistat/AI-Frameworks) - Natural Language Processing (NLP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<a href=\"http://www.insa-toulouse.fr/\" ><img src=\"https://www.insa-toulouse.fr/skins/Insa-v2/resources/img/logo-insa.jpg\" style=\"float:left; max-width: 320px; display: inline\" alt=\"INSA\"/></a> \n",
    "<a href=\"https://github.com/wikistat\" ><img src=\"https://avatars0.githubusercontent.com/u/20927455?s=200&v=4\" width=400, style=\"max-width: 100px; display: inline\"  alt=\"Wikistat\"/></a>\n",
    "<a href=\"http://www.math.univ-toulouse.fr/\" ><img src=\"https://perso.math.univ-toulouse.fr/riscope/files/2017/06/IMT.jpg\" width=300,  style=\"float:right;  display: inline\" alt=\"IMT\"/> </a>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data : Cdiscount's product description.\n",
    "\n",
    "This dataset has been released from Cdiscount for a data competition (type kaggle) on the french website [datascience.net](https://www.datascience.net/fr/challenge). <br>\n",
    "The test dataset of this competition has not been released, so we used a subset of 1M producted of the original train dataset(+15M rows) all along with the **Text Processing** lab.<br>\n",
    "The objective of this competition was to classify the text description of various products into various categories that compose the navigation tree of Cdiscount website. It is composed of 4,733 categories organized within 44 meta categories. <br>\n",
    "\n",
    "The objective of this lab is not to win the competition so we will only used the meta-categories."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2 : Words embedding. Application to text classification and semi-supervised learning.\n",
    "\n",
    "In this second notebook we will study three words embedding methods:\n",
    "\n",
    "* [Word2Vec](https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf)\n",
    "* [FastText](https://arxiv.org/pdf/1607.04606.pdf)\n",
    "* [Glove](https://nlp.stanford.edu/pubs/glove.pdf)\n",
    "\n",
    "For each of these three method we will:\n",
    "\n",
    "* Study their characteristics\n",
    "* Explore the embedding they produce\n",
    "* Check how they perform on classification problem\n",
    "* Check how they can overcome problem with few labeled data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Files & Data (Google Colab)\n",
    "\n",
    "If you're runing this notebook on Google colab, you do not have access to the `data` or `solutions` folder you get by cloning the repository localy. \n",
    "\n",
    "The following lines will allow you to build the folders and the files you need for this TP..\n",
    "\n",
    "**WARNING 1** Do not run this line localy.\n",
    "**WARNING 2** The magic command `%load` does not work work on google colab, you will have to copy-paste the solution on the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! mkdir data\n",
    "! wget -P data https://github.com/wikistat/AI-Frameworks/raw/master/Text/data/cdiscount_test.csv.zip\n",
    "! wget -P data https://github.com/wikistat/AI-Frameworks/raw/master/Text/data/cdiscount_train.csv.zip\n",
    "! mkdir data/metadata\n",
    "! wget -P data/metadata https://github.com/wikistat/AI-Frameworks/raw/master/Text/data/metadata/metadata_1.pkl\n",
    "! wget -P data/metadata https://github.com/wikistat/AI-Frameworks/raw/master/Text/data/metadata/metadata_2.pkl\n",
    "! wget -P data/metadata https://github.com/wikistat/AI-Frameworks/raw/master/Text/data/metadata/metadata_few_labeled_dataset.pkl\n",
    "! mkdir data/w2v_model\n",
    "! mkdir solution\n",
    "! wget -P solution https://github.com/wikistat/AI-Frameworks/raw/master/Text/solution/w2v_homme.py\n",
    "! wget -P solution https://github.com/wikistat/AI-Frameworks/raw/master/Text/solution/w2v_combination.py\n",
    "! wget -P solution https://github.com/wikistat/AI-Frameworks/raw/master/Text/solution/w2v_predict_output.py\n",
    "! wget -P solution https://github.com/wikistat/AI-Frameworks/raw/master/Text/solution/get_feature_mean.py\n",
    "! wget -P solution https://github.com/wikistat/AI-Frameworks/raw/master/Text/solution/clean.py\n",
    "\n",
    "! wget -P . https://github.com/wikistat/AI-Frameworks/raw/master/Text/vectorizer.py\n",
    "! wget -P . https://github.com/wikistat/AI-Frameworks/raw/master/Text/ml_model.py\n",
    "! wget -P . https://github.com/wikistat/AI-Frameworks/raw/master/Text/word_embedding.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "#Importation des librairies utilisées\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import collections\n",
    "import pickle\n",
    "import itertools\n",
    "import os\n",
    "import nltk\n",
    "import warnings\n",
    "import plotly.offline as pof\n",
    "import plotly.graph_objects as go\n",
    "warnings.filterwarnings('ignore')\n",
    "import sklearn.metrics as smet\n",
    "\n",
    "import sklearn.model_selection as sms\n",
    "from solution.clean import CleanText"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might need to download the nltk stopwords if you didn't do it on previous notebook or if you're running in google colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data\n",
    "\n",
    "We download the train and test data and generate the same cleaned columns and the same train/validation split as in part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct = CleanText()\n",
    "data = pd.read_csv(\"data/cdiscount_train.csv.zip\",sep=\",\", nrows=100000)\n",
    "ct.clean_df_column(data, \"Description\", \"Description_cleaned\")\n",
    "print(\"The train dataset is composed of %d lines\" %data.shape[0])\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test = pd.read_csv(\"data/cdiscount_test.csv.zip\",sep=\",\")\n",
    "ct.clean_df_column(data_test, \"Description\", \"Description_cleaned\")\n",
    "print(\"The train dataset is composed of %d lines\" %data_test.shape[0])\n",
    "data_test.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec\n",
    "\n",
    "In this part, we will generate`Word2Vec` model thanks to the [**gensim**](https://radimrehurek.com/gensim/index.html) python library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Word2Vec model\n",
    "\n",
    "The `gensim.models.Word2Vec` function allows to build  Word2Vec model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gensim.models.Word2Vec?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like many machine learning models, the `Word2Vec` function has a lot of parameters to set, here is some argument that will be fixed:\n",
    "\n",
    "\n",
    "* Features_dimension = 300 : It's the dimension of the features space (the hidden layer during training) that will be set.\n",
    "* min_count = 1 : The minimum number of occurrences of a token to consider it for the model\n",
    "* windows = 5 : The max distance between a target word and the other words in the sentence to be considered as a neighbors.\n",
    "* hs = 0 \n",
    "* negative = 10\n",
    "* iter = 10 -> (best results, after testing 5,10,15,20,25,30)\n",
    "\n",
    "**Q** What are the arguments *hs* and *negative* for? What does the values set for these arguments imply??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_dimension = 300\n",
    "min_count = 1\n",
    "window = 5\n",
    "hs = 0\n",
    "negative = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It takes list of tokens as an input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "array_token = [line.split(\" \") for line in data[\"Description_cleaned\"].values]\n",
    "test_array_token = [line.split(\" \") for line in data_test[\"Description_cleaned\"].values]\n",
    "array_token[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will train two models with the help of the class `WordEmbedding` within the `word_embedding.py` file:\n",
    "\n",
    "* One **skip-sgram**, sg = 1\n",
    "* One **CBOW** model, sg = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from word_embedding import WordEmbedding\n",
    "\n",
    "we_sg = WordEmbedding(word_embedding_type = \"word2vec\", \n",
    "                      args = dict(sentences = array_token, sg=1, hs=hs, negative=negative, min_count=min_count, size=features_dimension, window = window, iter=10))\n",
    "model_sg, training_time_sg = we_sg.train()\n",
    "print(\"Model Skip-gram trained in %.2f minutes\"%(training_time_sg/60))\n",
    "model_sg.save(\"data/w2v_model/model_sg_100k\")\n",
    "\n",
    "we_cbow = WordEmbedding(word_embedding_type = \"word2vec\", \n",
    "                      args = dict(sentences = array_token, sg=0, hs=hs, negative=negative, min_count=min_count, size=features_dimension, window = window, iter=10))\n",
    "model_cbow, training_time_cbow = we_cbow.train()\n",
    "print(\"Model CBOW trained in %.2f minutes\"%(training_time_cbow/60))\n",
    "model_cbow.save(\"data/w2v_model/model_cbow_100k\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q**: Why don't we split in a training a and validation dataset before training the models?\n",
    "\n",
    "**Q** What can you say about the learning time difference between theses two models? How do you explain the difference?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-Trained Model\n",
    "\n",
    "As for convolutional models, there exist pre-trained models on the internet. \n",
    "One of the most famous is probably the [`GoogleNewsVectors`](https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit) that has been trained over 100 billions of GoogleNews article. However, this model is in english and can't be used for the Cdiscount dataset\n",
    "\n",
    "\n",
    "We will use here a model from the following git project: [https://github.com/Kyubyong/wordvectors](https://github.com/Kyubyong/wordvectors) where the model has been learned on  1Giga of wikipedia's article in **Skip-Gram** mode.\n",
    "\n",
    "You can download it by clicking on this [link](https://drive.google.com/file/d/0B0ZXk88koS2KM0pVTktxdG15TkE/view).  unzip it and download it within the data folder with this direction *data/fr/fr.bin* \n",
    "\n",
    "**On google colab you can run the cell below to get the online model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! gdown https://drive.google.com/uc?id=0B0ZXk88koS2KM0pVTktxdG15TkE\n",
    "! mv fr.zip data/\n",
    "! unzip data/fr.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_pretrained_dir = \"data/fr/fr.bin\"\n",
    "model_pretrained = gensim.models.Word2Vec.load(model_pretrained_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Property\n",
    "\n",
    "\n",
    "We will now compare some properties of the three word2vec models we have:   (*CBOW*, *Skip-Gram* et the pre-trained model *online*)\n",
    "\n",
    "*Models that we have learned has been trained on tokenized words. Hence, we will need tokenized word to test their properties.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk \n",
    "stemmer=nltk.stem.SnowballStemmer('french')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Most similar world"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `most_similar`'s word function from **gensim** allows to retrieve the most similar words from a word or a combination of words.\n",
    "\n",
    "**Q** From this [documentation](https://radimrehurek.com/gensim/models/keyedvectors.html#gensim.models.keyedvectors.WordEmbeddingsKeyedVectors.most_similar) answer the following question:\n",
    "* What is the similarity measure used?\n",
    "* In which space is it computed ? \n",
    "* How does the the function work when several words are passed as parameters?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### One Word\n",
    "\n",
    "**Exercise** For each three models, display output of the `most_similar` word for the word `homme`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solution/w2v_homme.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q** Compare the outputs of the function with the models learned on cdiscount and the pre-trained model. What can you say about the quality of these outputs?\n",
    "\n",
    "**Q** What can you say about the output of the two models learned on cdiscount? \n",
    "\n",
    "**Exercice** Display now the output of the `most_similar`function for the word  *femme*. \n",
    "\n",
    "**Exercice** Display now the output of the `most_similar`function for words related specifically to the cdiscount dataset.  (ex. *xbox*, *pantalon*,..)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word Combination\n",
    "\n",
    "**Exercise** For each three models, display the outputs of the `most_similar` word for this combinations of words `femme`+ `roi` - `homme`. (Use the  *positive* and  *negative* argument of the function). \n",
    "Comment the quality of the outputs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solution/w2v_combination.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercice** Test other combinations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predict the output word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The predict `predict_output_word` function of **gensim** allows to predict word  from a word or a combination of word. <br>\n",
    "\n",
    "**Exercice** for the three models, display a prediction from common word (*homme*, *femme*) or word specifically related to the Cdiscount dataset (*coque*-*de*-*téléphone*). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solution/w2v_predict_output.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Features\n",
    "\n",
    "We will now create features matrices from the **Word2Vec** model we just learned in order to predict product categories..\n",
    "\n",
    "The model created allows to generate a vector in the feature space for each word `x` using following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_feature = model_sg['homm']\n",
    "print(x_feature.shape)\n",
    "x_feature[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our problem, the product descriptions we want to categorize are represented by a list of cleaned tokens from the previous notebook. <br>\n",
    "From those lists, there are various way to represent these descriptions with the **Word2Vec** model\n",
    "\n",
    "1. Mean of the features' vector of each token in the description. \n",
    "2. Weighted mean of the features' vector of each token in the description where the weights are the number of occurrences of each token within the description\n",
    "3. Weighted mean of the features' vector of each token in the description where the weights are `TFIDF` weights\n",
    "4. etc...\n",
    "\n",
    "It's the second solution we will use here.\n",
    "\n",
    "Let's first split the data (with `random_state=42`) to obtain the same split as in the first notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train, data_valid = sms.train_test_split(data, test_size=0.1, random_state=42)\n",
    "train_array_token = [line.split(\" \") for line in data_train[\"Description_cleaned\"].values]\n",
    "valid_array_token = [line.split(\" \") for line in data_valid[\"Description_cleaned\"].values]\n",
    "test_array_token = [line.split(\" \") for line in data_test[\"Description_cleaned\"].values]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise** Write a function that can generate a weighted mean of the feature's vector of the token within a description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solution/get_feature_mean.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_description = train_array_token[0]\n",
    "get_features_mean(token_description, model_sg).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For ease of use, the functions allowing to build vectors from a token list has been written within the `WordEmbedding` class of the `word_embedding.py`file\n",
    "* `get_features_mean` : return a mean vector within the embedding space of all tokens that composed a line.\n",
    "* `get_matrix_features_means` : apply `get_features_mean` on every element of the *X* matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cbow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_embedded_train_cbow, embedded_conversion_train_time_cbow = WordEmbedding.get_matrix_features_means(train_array_token, model_cbow)\n",
    "X_embedded_valid_cbow, embedded_conversion_valid_time_cbow = WordEmbedding.get_matrix_features_means(valid_array_token, model_cbow)\n",
    "X_embedded_test_cbow, embedded_conversion_test_time_cbow = WordEmbedding.get_matrix_features_means(test_array_token, model_cbow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Skip-Gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_embedded_train_sg, embedded_conversion_train_time_sg = WordEmbedding.get_matrix_features_means(train_array_token, model_sg)\n",
    "X_embedded_valid_sg, embedded_conversion_valid_time_sg = WordEmbedding.get_matrix_features_means(valid_array_token, model_sg)\n",
    "X_embedded_test_sg, embedded_conversion_test_time_sg = WordEmbedding.get_matrix_features_means(test_array_token, model_sg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Online model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct = CleanText(apply_stemming=False)\n",
    "ct.clean_df_column(data_train, \"Description\", \"Description_cleaned_no_stem\")\n",
    "ct.clean_df_column(data_valid, \"Description\", \"Description_cleaned_no_stem\")\n",
    "ct.clean_df_column(data_test, \"Description\", \"Description_cleaned_no_stem\")\n",
    "\n",
    "\n",
    "train_array_token_nostem = [line.split(\" \") for line in data_train[\"Description_cleaned_no_stem\"].values]\n",
    "valid_array_token_nostem = [line.split(\" \") for line in data_valid[\"Description_cleaned_no_stem\"].values]\n",
    "test_array_token_nostem = [line.split(\" \") for line in data_test[\"Description_cleaned_no_stem\"].values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_embedded_train_pretrained, embedded_conversion_train_time_pretrained = WordEmbedding.get_matrix_features_means(train_array_token_nostem, model_pretrained)\n",
    "X_embedded_valid_pretrained, embedded_conversion_valid_time_pretrained = WordEmbedding.get_matrix_features_means(valid_array_token_nostem, model_pretrained)\n",
    "X_embedded_test_pretrained, embedded_conversion_test_time_pretrained = WordEmbedding.get_matrix_features_means(test_array_token_nostem, model_pretrained)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have computed the features, let's train various classification models (the same than the ones used in the  previous notebook) on this feature!  \n",
    "\n",
    "The following code allows to train these models. Once again, the models have already been trained, and results save in the `data/metadata/metadata_2.pkl` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FORCE_TO_RUN = False\n",
    "\n",
    "from ml_model import MlModel\n",
    "\n",
    "we_models = [[model_sg, \"skip-gram\"],\n",
    "            [model_cbow, \"cbow\"],\n",
    "             [model_pretrained, \"pretrained\"]]\n",
    "\n",
    "model_parameters = [[\"lr\", {\"C\":[0.1, 1, 10]}],\n",
    "                     [\"rf\", {\"n_estimators\" : [100,500]}],\n",
    "                     [\"mlp\", {\"hidden_layer_sizes\" : [128, 256]}]\n",
    "                      ]\n",
    "\n",
    "if FORCE_TO_RUN:\n",
    "    metadata = {}\n",
    "    for we_model, we_name in we_models:\n",
    "        train_token = train_array_token if we_name !=\"pretrained\" else train_array_token_nostem\n",
    "        X_train, embedded_conversion_train_time = WordEmbedding.get_matrix_features_means(train_token, we_model)\n",
    "        Y_train = data_train.Categorie1.values\n",
    "        valid_token = valid_array_token if we_name !=\"pretrained\" else valid_array_token_nostem\n",
    "        X_valid, embedded_conversion_valid_time = WordEmbedding.get_matrix_features_means(valid_token, we_model)\n",
    "        Y_valid = data_valid.Categorie1.values\n",
    "        test_token = test_array_token if we_name !=\"pretrained\" else test_array_token_nostem\n",
    "        X_test, embedded_conversion_test_time = WordEmbedding.get_matrix_features_means(test_token, we_model)\n",
    "        Y_test = data_test.Categorie1.values\n",
    "\n",
    "        for ml_model_name, param_grid in model_parameters:\n",
    "            ml_class = MlModel(ml_model_name=ml_model_name, param_grid=param_grid)\n",
    "            best_model, best_metadata = ml_class.train_all_parameters(X_train, Y_train, X_valid, Y_valid, save_metadata=True)\n",
    "            test_score = best_model.score(X_test, Y_test)\n",
    "            accuracy_test = best_model.score(X_test, Y_test)\n",
    "            f1_macro_score_test = smet.f1_score(best_model.predict(X_test),Y_test, average='macro')\n",
    "            balanced_accuracy_test = smet.balanced_accuracy_score(best_model.predict(X_test),Y_test)\n",
    "            best_metadata.update({\"balanced_accuracy_test\":balanced_accuracy_test,\"accuracy_test\": accuracy_test, \"f1_macro_score_test\":f1_macro_score_test, \"embedded_conversion_train_time\": embedded_conversion_train_time, \"embedded_conversion_valid_time\": embedded_conversion_valid_time, \"embedded_conversion_test_time\": embedded_conversion_test_time})\n",
    "            metadata.update({(we_name, \"\",  ml_model_name): best_metadata})\n",
    "    pickle.dump(metadata, open(\"data/metadata/metadata_2.pkl\",\"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = pickle.load(open(\"data/metadata/metadata_1.pkl\",\"rb\"))\n",
    "metadata.update(pickle.load(open(\"data/metadata/metadata_2.pkl\",\"rb\")))\n",
    "\n",
    "# Create figure\n",
    "fig = go.Figure()\n",
    "\n",
    "# Add traces\n",
    "metrics = [\"accuracy_train\",'accuracy_valid', \"accuracy_test\", \"learning_time\", \"predict_time\",\"balanced_accuracy_test\",\"balanced_accuracy_valid\",\"balanced_accuracy_train\", \"f1_macro_score_test\", \"f1_macro_score_valid\", \"f1_macro_score_train\"]\n",
    "N_metrics = len(metrics)\n",
    "method_ml_names = ['lr','rf','mlp']\n",
    "N_method_ml_names = len(method_ml_names)\n",
    "\n",
    "buttons = []\n",
    "for i_metric, metric in enumerate(metrics):\n",
    "    for method_ml_name in method_ml_names:\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=[k[0]+\"_\"+str(k[1]) for k,v in metadata.items() if v['name']==method_ml_name],\n",
    "                y=[0 if ( not(k[0] in (\"skip-gram\",\"cbow\", \"pretrained\")) and metric.startswith(\"embedded\")) else v[metric] for k,v in metadata.items() if v['name']==method_ml_name],\n",
    "                mode=\"markers\",\n",
    "                marker=dict(size=10),\n",
    "                name = method_ml_name,\n",
    "            )\n",
    "        )\n",
    "    buttons.append(\n",
    "            dict(label=metric,\n",
    "                 method=\"update\",\n",
    "                 args=[{\"visible\": [True if i in [i_metric*N_method_ml_names + k for k in range(N_method_ml_names)] else False for i in range(N_method_ml_names * N_metrics)]},\n",
    "                       {\"title\": metric}]))\n",
    "    \n",
    "\n",
    "# Update remaining layout properties\n",
    "fig.update_layout(\n",
    "    title_text=metric,\n",
    "    updatemenus=[\n",
    "        dict(\n",
    "            active=1,\n",
    "            buttons=buttons\n",
    "        )]\n",
    ")\n",
    "\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q** What can you say about learning times for the different combinations of ML model X vectorisation/embedding learned? \n",
    "\n",
    "**Q** What can you say about the values of these different metrics : `accuracy`, `balanced_accuracy` and `weighted_accuracy` for the different combinations of ML model X vectorisation/embedding learned ?  Do these results seem logical for you?  \n",
    "\n",
    "**Q** What can you say about the optimized metrics?\n",
    "\n",
    "**Q** According to the best parameters selected for each metadata. What would you propose to improve these results? \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semi supervised learning.\n",
    "\n",
    "In the previous part, we learned two words embedding models on the training dataset composed of 100.000 lines. (For ease of exploration, and running time). \n",
    "\n",
    "We have seen that Wor2vec does not necessarily perform better than the simple vectorizer model. <br>\n",
    "But word embeddings models required a lot of data to learn similarity between words. We used a pre-trained model but it appears that our dataset is not really a natural **language dataset**. \n",
    "\n",
    "However one of the advantages of the word embedding models is that they do not require labeled data to be trained. <br>\n",
    "Hence we will consider that we have the complete original train dataset of the Cdiscount context composed of 15M of lines, and we consider that it's an unlabeled dataset.<br> \n",
    "With the script `train_w2V_all_data.csv.py` we train two words2vec with the same parameters than the model learned above on the complete dataset. <br> \n",
    "\n",
    "This script takes several hours to run. **You do not have to run it**. If you're interested on running it again, you can ask your teacher to get the complete dataset. <br> \n",
    "\n",
    "Those model can be downloaded by following these links:\n",
    "\n",
    "* full model sg : [link](https://we.tl/t-eEjWF9ZRc7)\n",
    "* full model cbow:  [link](https://we.tl/t-zZLQV5Ht7E)\n",
    "\n",
    "Download the models and move it to the `data/w2v_model`folder.\n",
    "\n",
    "You can use the lines below if you're using google colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! gdown https://drive.google.com/uc?id=1uOIu76Ye2V2zpaAiZ5dYodXa5FoO1t2b\n",
    "! gdown https://drive.google.com/uc?id=1wm-AU8ygiPufIzAkmD3assX7JtKNMaKD\n",
    "! gdown https://drive.google.com/uc?id=11cYbvhLYhH2NmcZAYivsvRSiROddTka3g\n",
    "! gdown https://drive.google.com/uc?id=1PBUxn97zmjtkqU7nJnU-86dU7l26lTXA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how thus new training performs in a different usecase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "model_sg_full = KeyedVectors.load(\"data/w2v_model/full_model_sg\")\n",
    "model_cbow_full = KeyedVectors.load(\"data/w2v_model/full_model_cbow\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q** Do these models perform differently on the different function tests such that `most_similar_word`, `predict_output_word`, etc.?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Product classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FORCE_TO_RUN=False\n",
    "\n",
    "from ml_model import MlModel\n",
    "\n",
    "we_models = [[model_sg_full, \"skip-gram\"],\n",
    "            [model_cbow_full, \"cbow\"]]\n",
    "\n",
    "model_parameters = [[\"lr\", {\"C\":[0.1, 1, 10]}],\n",
    "                     [\"rf\", {\"n_estimators\" : [100,500]}],\n",
    "                     [\"mlp\", {\"hidden_layer_sizes\" : [128, 256]}]\n",
    "                      ]\n",
    "\n",
    "if FORCE_TO_RUN:\n",
    "    metadata = {}\n",
    "    for we_model, we_name in we_models:\n",
    "        train_token = train_array_token if we_name !=\"pretrained\" else train_array_token_nostem\n",
    "        X_train, embedded_conversion_train_time = WordEmbedding.get_matrix_features_means(train_token, we_model)\n",
    "        Y_train = data_train.Categorie1.values\n",
    "        valid_token = valid_array_token if we_name !=\"pretrained\" else valid_array_token_nostem\n",
    "        X_valid, embedded_conversion_valid_time = WordEmbedding.get_matrix_features_means(valid_token, we_model)\n",
    "        Y_valid = data_valid.Categorie1.values\n",
    "        test_token = test_array_token if we_name !=\"pretrained\" else test_array_token_nostem\n",
    "        X_test, embedded_conversion_test_time = WordEmbedding.get_matrix_features_means(test_token, we_model)\n",
    "        Y_test = data_test.Categorie1.values\n",
    "\n",
    "        for ml_model_name, param_grid in model_parameters:\n",
    "            ml_class = MlModel(ml_model_name=ml_model_name, param_grid=param_grid)\n",
    "            best_model, best_metadata = ml_class.train_all_parameters(X_train, Y_train, X_valid, Y_valid, save_metadata=True)\n",
    "            accuracy_test = best_model.score(X_test, Y_test)\n",
    "            f1_macro_score_test = smet.f1_score(best_model.predict(X_test),Y_test, average='macro')\n",
    "            balanced_accuracy_test = smet.balanced_accuracy_score(best_model.predict(X_test),Y_test)\n",
    "            best_metadata.update({\"balanced_accuracy_test\":balanced_accuracy_test,\"accuracy_test\": accuracy_test, \"f1_macro_score_test\":f1_macro_score_test, \n",
    "                                  \"embedded_conversion_train_time\": embedded_conversion_train_time, \"embedded_conversion_valid_time\": embedded_conversion_valid_time, \"embedded_conversion_test_time\": embedded_conversion_test_time})\n",
    "            metadata.update({(we_name+\"_full\", \"\",  ml_model_name): best_metadata})\n",
    "    pickle.dump(metadata, open(\"data/metadata/metadata_2bis.pkl\",\"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importation des librairies utilisées\n",
    "metadata = pickle.load(open(\"data/metadata/metadata_1.pkl\",\"rb\"))\n",
    "metadata.update(pickle.load(open(\"data/metadata/metadata_2.pkl\",\"rb\")))\n",
    "metadata.update(pickle.load(open(\"data/metadata/metadata_2bis.pkl\",\"rb\")))\n",
    "\n",
    "# Create figure\n",
    "fig = go.Figure()\n",
    "\n",
    "# Add traces\n",
    "metrics = [\"accuracy_train\",'accuracy_valid', \"accuracy_test\", \"learning_time\", \"predict_time\",\"balanced_accuracy_test\",\"balanced_accuracy_valid\",\"balanced_accuracy_train\", \"f1_macro_score_test\", \"f1_macro_score_valid\", \"f1_macro_score_train\"]\n",
    "N_metrics = len(metrics)\n",
    "method_ml_names = ['lr','rf','mlp']\n",
    "N_method_ml_names = len(method_ml_names)\n",
    "\n",
    "buttons = []\n",
    "for i_metric, metric in enumerate(metrics):\n",
    "    for method_ml_name in method_ml_names:\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=[k[0]+\"_\"+str(k[1]) for k,v in metadata.items() if v['name']==method_ml_name],\n",
    "                y=[0 if ( not(k[0] in (\"skip-gram\",\"cbow\", \"pretrained\")) and metric.startswith(\"embedded\")) else v[metric] for k,v in metadata.items() if v['name']==method_ml_name],\n",
    "                mode=\"markers\",\n",
    "                marker=dict(size=10),\n",
    "                name = method_ml_name,\n",
    "            )\n",
    "        )\n",
    "    buttons.append(\n",
    "            dict(label=metric,\n",
    "                 method=\"update\",\n",
    "                 args=[{\"visible\": [True if i in [i_metric*N_method_ml_names + k for k in range(N_method_ml_names)] else False for i in range(N_method_ml_names * N_metrics)]},\n",
    "                       {\"title\": metric}]))\n",
    "    \n",
    "\n",
    "# Update remaining layout properties\n",
    "fig.update_layout(\n",
    "    title_text=metric,\n",
    "    updatemenus=[\n",
    "        dict(\n",
    "            active=1,\n",
    "            buttons=buttons\n",
    "        )]\n",
    ")\n",
    "\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata[('skip-gram_full', '', 'mlp')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**\n",
    "\n",
    "**Q** Comment the results with w2v features learned over the complete unlabeled dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Few labeled dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have seen that training WordEmbedding dataset on an unsupervised dataset can improve the results of the classification (supervised) problem. \n",
    "\n",
    "However, 100.000 is already a high number of rows and the difference of the different metrics using the full words embedding model or the other words embedding model is not high.\n",
    "\n",
    "TO see how it can performe in a situation where we are a very small labeled dataset; let's re run model for different size of training dataset for the best model combination parameters for the three metrics studied and for one words emebdding (full an simple) and one vectorizer model. ie:\n",
    "\n",
    "**accuracy**\n",
    "* *Word Embedding* : [[model_sg, \"skip-gram\"], [\"mlp\", {\"hidden_layer_sizes\": 256}]]\n",
    "* *Word Embedding full* : [[model_sg_full, \"skip-gram-full\"], [\"mlp\", {\"hidden_layer_sizes\": 256}]]\n",
    "* *Vectorizer: [[tfidf,'None'], ['lr', {\"C\":10}]]\n",
    "\n",
    "**balanced accuracy**\n",
    "* *Word Embedding*: accuracy_test = [[model_sg, \"skip-gram\"], [\"rf\", {\"n_estimators\": 500}]]\n",
    "* *Word Embedding full*: accuracy_test = [[model_sg_full, \"skip-gram-full\"], [\"rf\", {\"n_estimators\": 500}]]\n",
    "* *Vectorizer: [[tfidf,'None'], ['lr', {\"C\":10}]]\n",
    "\n",
    "**f1 macro score**\n",
    "* *Word Embedding* : [[model_sg, \"skip-gram\"], [\"mlp\", {\"hidden_layer_sizes\": 256}]]\n",
    "* *Word Embedding full* : [[model_sg_full, \"skip-gram-full\"], [\"mlp\", {\"hidden_layer_sizes\": 256}]]\n",
    "* *Vectorizer: [[tfidf,'mlp'], [\"hidden_layer_sizes\": [256]]]\n",
    "\n",
    "*You may have to change these values if the results are different for you*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "The following code allows to train the different models defined above on different training size of dataset.<br>\n",
    "In order to save time, the model have already been trained, and data saved within the `data/metadata/metadata_few_labeled_dataset.pkl` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FORCE_TO_RUN = False\n",
    "from ml_model import MlModel\n",
    "from vectorizer import Vectorizer\n",
    "\n",
    "args = [[\"we\", [\"skip-gram_full\", model_sg_full, ], [\"rf\", {\"n_estimators\": [500]}]],\n",
    "        [\"we\", [\"skip-gram_full\", model_sg_full, ], [\"mlp\", {\"hidden_layer_sizes\": [256]}]],\n",
    "        [\"we\", [\"skip-gram\", model_sg, ], [\"rf\", {\"n_estimators\": [500]}]],\n",
    "        [\"we\", [\"skip-gram\", model_sg, ], [\"mlp\", {\"hidden_layer_sizes\": [256]}]],\n",
    "        [\"vect\", [\"tfidf\", \"None\"], [\"mlp\", {\"hidden_layer_sizes\": [256]}]],\n",
    "         [\"vect\", [\"tfidf\", \"None\"], [\"lr\", {\"C\": [10]}]]]\n",
    "train_sizes = [100, 500, 1000, 5000, 10000, 50000, 100000]\n",
    "if FORCE_TO_RUN:\n",
    "    metadata = {}\n",
    "    for vect_type, (vect_name, vect_arg), (ml_model_name, param_grid) in args:\n",
    "        for train_size in train_sizes:\n",
    "            print(vect_name, ml_model_name, train_size)\n",
    "            if vect_type == \"we\":\n",
    "                we_model = vect_arg\n",
    "                train_token = train_array_token[:train_size] \n",
    "                X_train, embedded_conversion_train_time = WordEmbedding.get_matrix_features_means(train_token, we_model)\n",
    "                valid_token = valid_array_token\n",
    "                X_valid, embedded_conversion_valid_time = WordEmbedding.get_matrix_features_means(valid_token, we_model)\n",
    "                test_token = test_array_token\n",
    "                X_test, embedded_conversion_test_time = WordEmbedding.get_matrix_features_means(test_token, we_model)\n",
    "            else:\n",
    "                nb_hash = vect_arg\n",
    "                vect_method = Vectorizer(vectorizer_type=vect_name, nb_hash=nb_hash)\n",
    "                X_train = vect_method.load_dataframe(\"train\")[:train_size]\n",
    "                X_valid = vect_method.load_dataframe(\"valid\")\n",
    "                X_test = vect_method.load_dataframe(\"test\")\n",
    "\n",
    "            Y_train = data_train.Categorie1.values[:train_size]\n",
    "            Y_valid = data_valid.Categorie1.values\n",
    "            Y_test = data_test.Categorie1.values\n",
    "\n",
    "            # model\n",
    "            ml_class = MlModel(ml_model_name=ml_model_name, param_grid=param_grid)\n",
    "            best_model, best_metadata = ml_class.train_all_parameters(X_train, Y_train, X_valid, Y_valid, save_metadata=True)\n",
    "            accuracy_test = best_model.score(X_test, Y_test)\n",
    "            f1_macro_score_test = smet.f1_score(best_model.predict(X_test),Y_test, average='macro')\n",
    "            balanced_accuracy_test = smet.balanced_accuracy_score(best_model.predict(X_test),Y_test)\n",
    "            best_metadata.update({\"balanced_accuracy_test\": balanced_accuracy_test, \"accuracy_test\": accuracy_test,\n",
    "                                  \"f1_macro_score_test\": f1_macro_score_test,\n",
    "                                  \"embedded_conversion_train_time\": embedded_conversion_train_time,\n",
    "                                  \"embedded_conversion_valid_time\": embedded_conversion_valid_time,\n",
    "                                  \"embedded_conversion_test_time\": embedded_conversion_test_time})\n",
    "            metadata.update({(vect_name, ml_model_name, train_size): best_metadata})\n",
    "            pickle.dump(metadata, open(\"data/metadata/metadata_few_labeled_dataset.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = pickle.load(open(\"data/metadata/metadata_few_labeled_dataset.pkl\", \"rb\"))\n",
    "# Create figure\n",
    "fig = go.Figure()\n",
    "\n",
    "# Add traces\n",
    "metrics = [\"accuracy_train\",'accuracy_valid', \"accuracy_test\", \"learning_time\", \"predict_time\",\"balanced_accuracy_test\",\"balanced_accuracy_valid\",\"balanced_accuracy_train\", \"f1_macro_score_test\", \"f1_macro_score_valid\", \"f1_macro_score_train\"]\n",
    "N_metrics = len(metrics)\n",
    "method_vect_ml_names = [['skip-gram_full','rf'],['skip-gram_full','mlp'],['skip-gram','rf'],['skip-gram','mlp'],[\"tfidf\",\"lr\"],[\"tfidf\",\"mlp\"]]\n",
    "N_method_vect_ml_names = len(method_vect_ml_names)\n",
    "\n",
    "buttons = []\n",
    "for i_metric, metric in enumerate(metrics):\n",
    "    for vect_name, ml_name in method_vect_ml_names:\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=  train_sizes,\n",
    "                y= [metadata[(vect_name,ml_name, x)][metric] for x in train_sizes],\n",
    "                mode=\"markers+lines\",\n",
    "                marker=dict(size=10),\n",
    "                name = vect_name+\"_\"+ml_name,\n",
    "            )\n",
    "        )\n",
    "    buttons.append(\n",
    "            dict(label=metric,\n",
    "                 method=\"update\",\n",
    "                 args=[{\"visible\": [True if i in [i_metric*N_method_ml_names + k for k in range(N_method_vect_ml_names)] else False for i in range(N_method_vect_ml_names * N_metrics)]},\n",
    "                       {\"title\": metric}]))\n",
    "    \n",
    "\n",
    "# Update remaining layout properties\n",
    "fig.update_layout(\n",
    "    xaxis_type=\"log\",\n",
    "    title_text=metric,\n",
    "    updatemenus=[\n",
    "        dict(\n",
    "            active=1,\n",
    "            buttons=buttons\n",
    "        )]\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Glove\n",
    "\n",
    "Glove is an algorithm developed by [Standford's researcher](https://nlp.stanford.edu/projects/glove/) in [C language](https://github.com/stanfordnlp/GloVe). There exists no standard python library widely used so far. \n",
    "\n",
    "For ease of use we will use the code developed [here](https://github.com/WenchenLi/GloVePyWrapper). The authors developed a python class called `GloveWrapper`that allows to call C original code in python. <br>\n",
    "This repo has been added to the *ÌA-Frameworks* and can be imported easily on this notebook (see codes below).\n",
    "\n",
    "To train models, the code requires a file with all text with no punctuation and the words separated from each other by a blank space. The code below enables to generate such a file from the original cdiscount dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct = CleanText()\n",
    "data = pd.read_csv(\"data/cdiscount_train.csv.zip\",sep=\",\")\n",
    "ct.clean_df_column(data, \"Description\", \"Description_cleaned\")\n",
    "data[\"Description_cleaned\"].to_csv(\"data/cdiscount_train_glove\", sep=\" \", index=False, quotechar=\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TO generate the model, we apply successively these three steps:\n",
    "* **vocab_count** : get all vocabulary and number of appearance of each words\n",
    "* **cooccur** : compute the co-occurence matrix.\n",
    "* **shuffle** : the train dataset\n",
    "* **glove** : train the model using glove algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from GloVePyWrapper.glove_pywrapper import  GloveWrapper\n",
    "\n",
    "glove = GloveWrapper(\n",
    "    corpus =\"data/cdiscount_train_glove\" ,\n",
    "    name = \"cdiscount_train\" ,\n",
    "    train_dir = \"data/glove/\",\n",
    "    builddir='GloVePyWrapper/build',\n",
    "    vocab_min_count=1,\n",
    "    vector_size=300,\n",
    "    window_size=5)\n",
    "#prepare vocabulary count\n",
    "glove.vocab_count()\n",
    "#prepare co-occurrence matrix\n",
    "glove.cooccur()\n",
    "#reshuffle\n",
    "glove.shuffle()\n",
    "#glove train\n",
    "glove.glove()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q**: What are the `vocab_min_count`, `vector_size`, `window_size` arguments used  are ? Open the python code and check the different arguments used by glove function.\n",
    "\n",
    "**Q**: For each of the four steps, check the files that were been generated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`gensim`library does not contains code to train glove models. <br>\n",
    "However, once a glove model has been trained, it can be loaded via gensim. We can use the same function as for other words embeddings models such that `Word2vec`or `FastText` using the `glove2word2vec`function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100001, 300)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "glove_input_file = 'data/glove/cdiscount_train_vectors.txt'\n",
    "word2vec_output_file = 'data/glove/cdiscount_train_vectors.txt.word2vec'\n",
    "glove2word2vec(glove_input_file, word2vec_output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: Use the different codes above to train words embedding model using `Glove`instead of `word2vec`. \n",
    "Compare performance of word prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FastText\n",
    "\n",
    "`FastText` is an extension of Word2Vec proposed by the same authors. It works quite the same that gensim but words are represented as subwords of n characters. \n",
    "\n",
    "It is not usefull here as we do not really handle Natural Language processing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gensim.models.FastText?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Use the different codes above to train words embedding models using `FastText`instead of `word2vec`. \n",
    "Compare the performance of the word prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise** You can now try any of this word embedding models on DEFI-IA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "nav_menu": {
    "height": "279px",
    "width": "252px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
