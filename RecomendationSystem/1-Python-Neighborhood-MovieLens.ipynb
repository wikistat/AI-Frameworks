{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI-Frameworks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<a href=\"http://www.insa-toulouse.fr/\" ><img src=\"http://www.math.univ-toulouse.fr/~besse/Wikistat/Images/logo-insa.jpg\" style=\"float:left; max-width: 120px; display: inline\" alt=\"INSA\"/></a> \n",
    "<a href=\"http://wikistat.fr/\" ><img src=\"http://www.math.univ-toulouse.fr/~besse/Wikistat/Images/wikistat.jpg\" width=400, style=\"max-width: 150px; display: inline\"  alt=\"Wikistat\"/></a>\n",
    "<a href=\"http://www.math.univ-toulouse.fr/\" ><img src=\"http://www.math.univ-toulouse.fr/~besse/Wikistat/Images/logo_imt.jpg\" width=400,  style=\"float:right;  display: inline\" alt=\"IMT\"/> </a>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LAB 5 Introduction to Recommendation System with Collaborative Filtering  -  Part 1 : Neighborhood-Based Methods with `Surprise` Python Library.\n",
    "\n",
    "The objectives of this notebook are the following : \n",
    "\n",
    "* Discover and Explore `MovieLens` Dataset\n",
    "* Discover `Surprise`python library\n",
    "* Use Neigborhood-Based Methods (User-User and Item-Item Filters) methods to learn similarity between User an Item and use it to apply recommendation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import pickle\n",
    "import random\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.sparse as scsparse\n",
    "import scipy.stats as scstats\n",
    "import sklearn.metrics.pairwise as smp\n",
    "import surprise\n",
    "import surprise.model_selection as sms\n",
    "import surprise.prediction_algorithms as spa\n",
    "\n",
    "#Plotly\n",
    "import plotly.graph_objects as go\n",
    "import plotly.offline as pof\n",
    "\n",
    "#Matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Seaborn\n",
    "import seaborn as sb\n",
    "sb.set(color_codes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data : Movielens dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `movielens` dataset is a famous and widely used dataset furnish by *GroupLens* company : (https://grouplens.org/).\n",
    "\n",
    "The dataset is compose of ratings of movies made by a set of User collected over vairous periods of time. \n",
    "\n",
    "They are various dataset of different size avalaible on their website : https://grouplens.org/datasets/movielens/.  \n",
    "\n",
    "We will used, all along the diffrent TPs of this lab, the small dataset (100k ratings) for test and exploration and the stable dataset (20 Millions ratings) for testing performances. \n",
    "\n",
    "\n",
    "* Small Dataset :  *movielens_small folder*\n",
    "    * 100,000 ratings. \n",
    "    * 9742 movies. \n",
    "    * 610 users.\n",
    "    \n",
    "* Stable Dataset : \n",
    "    * 20 million ratings.\n",
    "    * 59.047 movies.\n",
    "    * 162.541 users.\n",
    "    \n",
    "Those datasets are also composed of genre information of movies and other metadata (tags on the movie, information about the user: age, sex, ..), that can be used to improve the recommendation system. We won't use those data as the methods we cover in the course does not handle metadata."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Presentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ratings\n",
    "The `ratings.csv`files is composed of fours columns:\n",
    "\n",
    "* userId : Int. Unique id of the user.\n",
    "* movieId : Int. Unique id of the movie.\n",
    "* rating : Int(0-5). Rate given by an user to a movie.\n",
    "* timestamp : time at which the rate has been given by. \n",
    "\n",
    "We won't use *timestamp* columns during this lab. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"movielens_small/\"\n",
    "rating = pd.read_csv(DATA_DIR + \"ratings.csv\")\n",
    "nb_entries = rating.shape[0]\n",
    "print(\"Number of entries : %d \" %nb_entries)\n",
    "rating.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_user = len(rating.userId.unique())\n",
    "print(\"Number of unique User : %d\" %nb_user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_movie = len(rating.movieId.unique())\n",
    "print(\"Number of unique Movies : %d\" %nb_movie)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Movies\n",
    "\n",
    "The `movies.csv`files is composed of three columns:\n",
    "\n",
    "* movieId : Int. Unique id of the movie.\n",
    "* title : string. The title of the movie.\n",
    "* genres : the genre(s) of the movies.\n",
    "\n",
    "We won't use *genres* columns during this lab. We won't use title in our algorithm but we will use it to display information and give more sense to our prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies = pd.read_csv(DATA_DIR + \"movies.csv\")\n",
    "print(\"Number of movies in the dictionary : %d\" %(len(movies.movieId.unique())))\n",
    "movies.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a `id_to_title` dictionary to convert id to their title."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_to_title = dict(movies[[\"movieId\",\"title\"]].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We add a *movie* columns to the rating dataset in order to display directly the information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rating[\"movie\"] = [id_to_title[x] for x in rating[\"movieId\"].values]\n",
    "rating.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploration\n",
    "\n",
    "Let's make some quick exploration to have some intuitions about these data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User\n",
    "We look at the distribution number of rating per user. We create a groupby pandas object where row are group by users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rating_gb_user = rating.groupby(\"userId\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of rating per user.\n",
    "\n",
    "We will display the distribution of number of rating per user.\n",
    "\n",
    "Plot are display using:\n",
    "* **Matplotlib** : Default python library. \n",
    "* **Seaborn**: A library based on matplotlib that can easily enable more beautiful an readble plot.\n",
    "* **Plotly** :   A library available in python, javascript R which allow to build interactive graph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plotly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = rating_gb_user.count()[\"rating\"].values\n",
    "data = [go.Histogram(x=x,\n",
    "                    xbins=dict( # bins used for histogram\n",
    "                    start=x.min(),\n",
    "                    end=x.max(),\n",
    "                    size=5,\n",
    "                ))]\n",
    "fig = go.Figure(data=data)\n",
    "fig.update_layout(\n",
    "    title_text='Number of rate per user distribution', # title of plot\n",
    "    bargap=0.2, # gap between bars of adjacent location coordinates\n",
    "    bargroupgap=0.1 # gap between bars of the same location coordinates\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Matplotlib. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = rating_gb_user.count()[\"rating\"].values\n",
    "fig = plt.figure(figsize=(30,5))\n",
    "ax = fig.add_subplot(1,1,1)\n",
    "plt.hist(x,bins = np.arange(x.min(),x.max()+5,5))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = rating_gb_user.count()[\"rating\"].values\n",
    "fig = plt.figure(figsize=(30,5))\n",
    "ax = fig.add_subplot(1,1,1)\n",
    "sb.distplot(x, ax=ax, kde=False, bins = np.arange(x.min(),x.max()+5,5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question** What can you say about the distribution? What is the minimum number of rate a user has given?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: Find the most *complaisant*  and the most *harsh* users and display their notation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/exercise_1_1.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most \"Hard\" user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Movie\n",
    "We look at the distribution number of rating recieved per movie. We create a groupby pandas object where row are groupby movie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rating_gb_movie = rating.groupby(\"movie\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of rating per movie.\n",
    "We will display the distribution of number of rating per user."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = rating_gb_movie.count()[\"userId\"].values\n",
    "data = [go.Histogram(x=x,\n",
    "                    xbins=dict( # bins used for histogram\n",
    "                    start=x.min(),\n",
    "                    end=x.max(),\n",
    "                    size=2,\n",
    "                ))]\n",
    "fig = go.Figure(data=data)\n",
    "fig.update_layout(\n",
    "    title_text='Number of rate per movie', # title of plot\n",
    "    bargap=0.2, # gap between bars of adjacent location coordinates\n",
    "    bargroupgap=0.1 # gap between bars of the same location coordinates\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = rating_gb_user.count()[\"rating\"].values\n",
    "fig = plt.figure(figsize=(30,5))\n",
    "ax = fig.add_subplot(1,1,1)\n",
    "plt.hist(x,bins = np.arange(x.min(),x.max()+5,5))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = rating_gb_user.count()[\"rating\"].values\n",
    "fig = plt.figure(figsize=(30,5))\n",
    "ax = fig.add_subplot(1,1,1)\n",
    "sb.distplot(x, ax=ax, kde=False, bins = np.arange(x.min(),x.max()+5,5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question** What can you say about the distribution of the movie? What is the minimum number of rate a movie can have?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercices** Display the Top 10 most rated movies, top 10 better and worst movies (for movies with at least 10 rates)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/exercise_1_2.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Suprise\n",
    "\n",
    "Surprise is a python library http://surpriselib.com/, that contains various algorithm dedicated to Recommendation.  We will use it to apply neighborhood-based algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Surprise contains various function that enable to load directly the movielens dataset and create train/text partition. However we won't use those methods.\n",
    "The movielens-100K dataset is changing and we want it to be the same to compare the methods with different library over the notebooks of this lab. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First We create train and test dataset and we save and updated version of the *ratings/csv* filest with a new *Test/train* columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rating[\"test_train\"] = [\"test\" if random.random()<=0.1 else \"train\" for _ in range(rating.shape[0])]\n",
    "rating[\"test_train\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rating.to_csv(\"movielens_small/ratings_updated.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then use the `load_from_df` methods that require data Nx3 matrices where N is the number of entries and the 3 columns are the users, the items and the rates. This correspond to the rating dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = surprise.Reader(rating_scale=(0, 5))\n",
    "rating_train = rating[rating.test_train==\"train\"]\n",
    "data = surprise.Dataset.load_from_df(rating_train[['userId', 'movieId', 'rating']], reader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then use the `build_full_trainset` to convert the Surprise Dataset object to a Surprise Trainset object that can be fitted. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = data.build_full_trainset()\n",
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rating_test = rating[rating.test_train==\"test\"]\n",
    "test = list([tuple(x) for x in rating_test[['userId', 'movieId', 'rating']].values])\n",
    "test[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# User-User Filter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Main assumption** : customers with a similar profile will have similar tastes.\n",
    "\n",
    "\n",
    "For a customer u, the aim is to find a subset of customers with a close profile and predicting the missing mark of a product i on customer u relies on a convex linear aggregation of marks of customers with close profile.\n",
    "\n",
    "\n",
    "$$\\hat{r}_{u,i} = \\bar{r}_u + \\frac{\\sum_{u'\\in S_u} s(u,u')\\cdot (r_{u',i}-\\bar{r_{u'}})}{\\sum_{u'\\in S_u} |s(u,u')| }$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit the User-User similarity Matrix\n",
    "\n",
    "Have a look ad the surprise \"knn inspired\" algorithm documentation :  https://surprise.readthedocs.io/en/stable/knn_inspired.html to understand the different algorithm available.\n",
    "\n",
    "**Exercise** :  Initialize a method that perform a **user-user** filter based on the formula above (i.e. that **take means** into account) with:\n",
    "* **pearson** similarity distance\n",
    "* **k** (number of neighboor) to 40."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "UUFilter = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/exercise_1_3.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can know easily fit the algorithm and compute the results on test with the dedicated `surprise` methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the algorithm on the trainset, and predict ratings for the testset\n",
    "UUFilter.fit(train)\n",
    "predictions = UUFilter.test(test)\n",
    "\n",
    "# Then compute RMSE\n",
    "surprise.accuracy.rmse(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use the User-User similarity Matrix\n",
    "\n",
    "A big advantage of this methods is that it quite easy to explore the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nearest user\n",
    "\n",
    "The surprise library furnish a `get_neighbors`method that allow you to get directly the closest id of a given id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "userId=1\n",
    "nearest_userId = UUFilter.get_neighbors(userId,k=1)[0]\n",
    "print(\"user %d is the closest user of user %d\" %(nearest_userId,userId))\n",
    "print(\"User %d\" %userId)\n",
    "display(rating[rating.userId==userId][[\"movie\",\"rating\"]].sort_values(by=\"rating\"))\n",
    "print(\"User %d\" %nearest_userId)\n",
    "rating[rating.userId==neirest_userId][[\"movie\",\"rating\"]].sort_values(by=\"rating\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recommendation\n",
    "\n",
    "**Exercise** Build the list of the 10 most recommended movies for the user with the estimated rate. use the `predict`method of the `UUfilter`object that give you the rate for a couple (userId,itemId)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "UUFilter.predict?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/exercise_1_4.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Item-Item Filter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main assumption : the customers will prefer products that share a high similarity with those already well appreciated. Prediction of product j : aggregate\n",
    "with a linear convex combination of products Sj that are closed to product j."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\hat{r}_{ui} = \\mu_i + \\frac{ \\sum\\limits_{j \\in N^k_u(i)}\n",
    "\\text{sim}(i, j) \\cdot (r_{uj} - \\mu_j)} {\\sum\\limits_{j \\in\n",
    "N^k_u(i)} \\text{sim}(i, j)}$$\n",
    "\n",
    "We just have one parameter to change (user_based=False) in order to perform Item-Item Filter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IIFilter = spa.knns.KNNWithMeans(k=40, \n",
    "                      min_k =1, \n",
    "                      sim_options = {'name': 'pearson',\n",
    "                                     'user_based': False},\n",
    "                     verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the algorithm on the trainset, and predict ratings for the testset\n",
    "IIFilter.fit(train)\n",
    "predictions = IIFilter.test(test)\n",
    "\n",
    "# Then compute RMSE\n",
    "surprise.accuracy.rmse(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Questions** The method is quite slower than the previous one. Why is that? \n",
    "What can you say about the performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get an example prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use the Item-Item similarity Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nearest user\n",
    "\n",
    "The same `get_neighbors` can be used and now show closest item of a given item."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movieId = 2\n",
    "print(\"Selected Movie : %s\" %(id_to_title[movieId]))\n",
    "nearest_movieId = IIFilter.get_neighbors(movieId,k=10)\n",
    "print(\"10 most similar movies\")\n",
    "pd.DataFrame([id_to_title[k] for k in nearest_movieId if k in id_to_title])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction \n",
    "\n",
    "Same code that above can be used to recommend 10 movies to the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "userId=1\n",
    "# Get list of movies already rated by the user\n",
    "idmovies_rated_per_user = rating[rating.userId==userId][\"movieId\"].values\n",
    "# get prediction fo all movies for movies that are not already rated\n",
    "predicted = [[mid,IIFilter.predict(userId, mid)] for mid in movies.movieId.values if not(mid in idmovies_rated_per_user)]\n",
    "# sort predicted list according to the estimation computed\n",
    "recommendation = sorted(predicted, key=lambda x : x[1].est, reverse=True)\n",
    "#display the most 10 prediciton with a dataframe\n",
    "pd.DataFrame([(id_to_title[r[0]], r[1].est) for r in recommendation[:10]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare results for different parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare parameters\n",
    "results = []\n",
    "for k in [10,25,50,100]:\n",
    "    for user_based in [True, False]:\n",
    "        for sim_options_name in [\"pearson\",\"cosine\",\"msd\"]:\n",
    "            tstart = time.time()\n",
    "            Filter = spa.knns.KNNWithMeans(k=k,\n",
    "                                  sim_options = {'name': sim_options_name,\n",
    "                                                 'user_based': user_based}, \n",
    "                                verbose=0)\n",
    "            Filter.fit(train)\n",
    "            predictions = Filter.test(test)\n",
    "            rmse = surprise.accuracy.rmse(predictions)\n",
    "            results.append([k, user_based, sim_options_name, rmse])\n",
    "            tend = time.time()\n",
    "            print(\"%s, %s, %s computed in %d seconds\" %(k, user_based, sim_options_name, tend-tstart))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=[]\n",
    "color_dict = {True:\"green\",False:\"red\"}\n",
    "marker_dict = {\"pearson\":\"x\",\"cosine\":0,\"msd\":\"triangle-up\"}\n",
    "for user_based in [True, False]:\n",
    "    for sim_options_name in [\"pearson\",\"cosine\",\"msd\"]:\n",
    "        result_ = [r for r in results if r[1]==user_based and r[2] == sim_options_name]\n",
    "        x = [r[0] for r in result_]\n",
    "        y = [r[3] for r in result_]\n",
    "        user_string = \"User_User\" if user_based else \"Item Item\"\n",
    "        \n",
    "        data.append(go.Scatter(x=x,\n",
    "                               y=y,\n",
    "                               marker =dict(color=color_dict[user_based], symbol=marker_dict[sim_options_name]),\n",
    "                               name = \"%s Filter with %s similarity\" %(user_string, sim_options_name)\n",
    "                        ))\n",
    "fig = go.Figure(data=data)\n",
    "fig.update_layout(\n",
    "    title_text='MSE according to parameters'\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig=plt.figure(figsize=(30,10))\n",
    "ax = fig.add_subplot(1,1,1)\n",
    "color_dict = {True:\"green\",False:\"red\"}\n",
    "marker_dict = {\"pearson\":\"x\",\"cosine\":0,\"msd\":\"^\"}\n",
    "for user_based in [True, False]:\n",
    "    for sim_options_name in [\"pearson\",\"cosine\",\"msd\"]:\n",
    "        result_ = [r for r in results if r[1]==user_based and r[2] == sim_options_name]\n",
    "        x = [r[0] for r in result_]\n",
    "        y = [r[3] for r in result_]\n",
    "        user_string = \"User_User\" if user_based else \"Item Item\"\n",
    "        ax.plot(x,y, color=color_dict[user_based], marker = marker_dict[sim_options_name], label = \"%s Filter with %s similarity\" %(user_string, sim_options_name))\n",
    "ax.set_title(\"MSE according to parameters\", fontsize=20)\n",
    "plt.legend(fontsize=15)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question** Which algorithm perform the best? With which parameters?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will see that these results are not that bad compare to other methods. \n",
    "However, this method would take to many time and requires to many computation power to be applied on the complete dataset of (25 Millions of row). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (Optionnal)Run code on complete dataset \n",
    "\n",
    "**Exercise**\n",
    "\n",
    "* Download the complete and stable dataset by clicking here : http://files.grouplens.org/datasets/movielens/ml-25m.zip. \n",
    "* Move the dataset to the current file (RecomendationSystem).\n",
    "* Load the data and create a train/test dataset.\n",
    "* Fit a neighborhood based algorithm with the best parameter according to the results find on small dataset (**It may take a while**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/exercise_1_5.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tstart=time.time()\n",
    "IIFilter = spa.knns.KNNWithMeans(k=100, \n",
    "                      min_k =1, \n",
    "                      sim_options = {'name': 'msd',\n",
    "                                     'user_based': False},\n",
    "                     verbose=1)\n",
    "# Train the algorithm on the trainset, and predict ratings for the testset\n",
    "IIFilter.fit(train)\n",
    "predictions = UUFilter.test(test)\n",
    "\n",
    "# Then compute RMSE\n",
    "surprise.accuracy.rmse(predictions)\n",
    "tend=time.time()\n",
    "print(tend-tstart)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
