{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<a href=\"http://www.insa-toulouse.fr/\" ><img src=\"http://www.math.univ-toulouse.fr/~besse/Wikistat/Images/logo-insa.jpg\" style=\"float:left; max-width: 120px; display: inline\" alt=\"INSA\"/></a> \n",
    "\n",
    "<a href=\"http://www.hupi.fr/\" ><img src=\"http://www.hupi.fr/wp-content/uploads/2016/03/hupi_logo_vectoris_menu.png\" style=\"max-width: 300px; display: inline\"  alt=\"Wikistat\"/></a>\n",
    "\n",
    "<a href=\"http://www.math.univ-toulouse.fr/\" ><img src=\"http://www.math.univ-toulouse.fr/~besse/Wikistat/Images/logo_imt.jpg\" style=\"float:right; max-width: 200px; display: inline\" alt=\"IMT\"/> </a>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reconnaissance de caractères manuscrits ([MNIST](http://yann.lecun.com/exdb/mnist/)) avec [Spark](http://spark.apache.org/) et [MLlib](http://spark.apache.org/mllib/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 1 Introduction test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Le site de Yann Le Cun: [MNIST DataBase](http://yann.lecun.com/exdb/mnist/), est la source des données étudiées, il  décrit précisément le problème et les modes d'acquisition. Il tient également à jour la liste des publications proposant des solutions avec la qualité de prévision obtenue. \n",
    "\n",
    "De façon très schématique, plusieurs stratégies sont développées dans une vaste littérature sur ces données.  \n",
    "* Utiliser une méthode classique (k-nn, random forest...) sans trop raffiner mais avec des temps d'apprentissage rapide conduit à un taux d'erreur autour de 3%.\n",
    "* Ajouter  ou intégrer un pré-traitement des données permettant de recaler les images par des distorsions plus ou moins complexes.\n",
    "* Construire une mesure de distance adaptée au problème, par exemple invariante par rotation, translation, homothétie, puis l'intégrer dans une technique d'apprentissage classique (k-nn). \n",
    "* Utiliser une méthode intégrant les propriétés d'invariance (réseau de neurones \"profond\", scattering) avec une optimisation fine des paramètres. \n",
    "* ...\n",
    "\n",
    "L'**objectif** n'est pas de minimiser le taux d'erreur avec des méthodes sophistiquées mais d'utiliser ces données relativement volumineuses pour tester diverses implémentations des méthodes d'apprentissage classiques. \n",
    "Le [scénario](http://wikistat.fr/pdf/st-atelier-MINST.pdf) de [wikistat](http://wikistat.fr) propose de comparer des versions [R](http://www.math.univ-toulouse.fr/~besse/Wikistat/Notebooks/Atelier1-MNIST-R.html), [python](http://www.math.univ-toulouse.fr/~besse/Wikistat/Notebooks/Atelier1-MNIST-python.html). Ce calepin compléte les comparaisons en utilisant la librairie [MLlib](http://spark.apache.org/mllib/) de [Spark](http://spark.apache.org/), solution adaptée à un traitement distribué des données.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Importation des packages\n",
    "import time\n",
    "from numpy import array\n",
    "from pyspark.mllib.regression import LabeledPoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Gestion des données"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Importation et transformation des données au format RDD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les données doivent être stockées à un emplacement accessibles de tous les noeuds du cluster pour permettre la construction de la base de données réparties (RDD). Elles sont déjà partagée en une partie apprentissage et une test utilisée pour les comparaisons entre méthodes dans les publications. Ce sont bine les données du site MNIST mais transformée au format .csv pour en faciliter la lecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60000"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Transformation du fichier texte en RDD de valeurs\n",
    "## Données d'apprentissage\n",
    "path='/user/philippe.besse/Data-rdd/'\n",
    "trainMnist = sc.textFile(path+\"mnist_train.csv\").map(lambda l: [float(x) for x in l.split(',')])\n",
    "trainMnist.count() # taille de l'échantillon\n",
    "#test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60000"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " # Mise au format \"labeled point\" spécifique de MLlib \"(label (v1, v2...vp])\"\n",
    "trainMnistLP=trainMnist.map(lambda line: LabeledPoint(line[-1],[line[0:753]]))\n",
    "trainMnistLP.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Même chose pour les données de test\n",
    "testMnist = sc.textFile(path+'mnist_test.csv').map(lambda l: [float(x) for x in l.split(',')])\n",
    "testMnist.count() # taille de l'échantillon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[LabeledPoint(8.0, [0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,5.0,127.0,231.0,194.0,83.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,35.0,203.0,253.0,253.0,237.0,237.0,199.0,6.0,0.0,53.0,53.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,224.0,253.0,228.0,123.0,18.0,89.0,247.0,54.0,13.0,213.0,236.0,27.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,91.0,254.0,242.0,43.0,0.0,0.0,50.0,159.0,0.0,182.0,253.0,244.0,33.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,98.0,254.0,233.0,0.0,0.0,0.0,0.0,0.0,99.0,248.0,253.0,120.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,38.0,255.0,246.0,48.0,0.0,0.0,0.0,80.0,254.0,254.0,133.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,46.0,254.0,253.0,226.0,72.0,95.0,140.0,232.0,253.0,253.0,58.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,68.0,254.0,253.0,253.0,253.0,253.0,254.0,253.0,253.0,210.0,40.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,19.0,102.0,211.0,253.0,253.0,254.0,227.0,49.0,9.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,175.0,253.0,253.0,208.0,31.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,96.0,224.0,254.0,254.0,196.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,5.0,217.0,253.0,234.0,248.0,245.0,78.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,127.0,253.0,198.0,6.0,128.0,254.0,203.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,16.0,246.0,253.0,85.0,0.0,45.0,254.0,236.0,12.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,135.0,254.0,200.0,18.0,0.0,0.0,178.0,253.0,115.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,25.0,231.0,255.0,76.0,0.0,0.0,0.0,156.0,254.0,175.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,40.0,253.0,254.0,39.0,0.0,0.0,53.0,239.0,251.0,86.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,12.0,182.0,254.0,179.0,79.0,168.0,235.0,254.0,233.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,83.0,254.0,253.0,253.0,253.0,253.0,207.0,70.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,81.0,238.0,253.0,200.0,118.0,23.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0])]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testData=testMnist.map(lambda line: LabeledPoint(line[-1],[line[0:753]]))\n",
    "testData.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Sous-échantillon d'apprentissage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Extraction d'un sous-échantillon d'apprentissage pour tester les programmes sur des données plus petites. Itérer cette démarche permet d'étudier l'évolution de l'erreur de prévision en fonction de la taille de l'échantillon d'apprentissage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60000"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tauxEch=0.1 # tester pour des tailles croissantes d'échantillon d'apprentissage\n",
    "(trainData, DropDatal) = trainMnistLP.randomSplit([tauxEch, 1-tauxEch])\n",
    "trainData.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 2.3 Régression logistique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exemple d'utilisation pour expliciter la syntaxe mais sans grand intérêt pour ces données qui ne satisfont pas à des frontières de discrimination linéaires. Par défaut, ce sont 10 modèles qui sont estimés, une classe contre les autres. Deux [algorithmes d'optimisation](http://spark.apache.org/docs/latest/api/python/pyspark.mllib.html#pyspark.mllib.classification.LogisticRegressionWithLBFGS) sont proposés (LBFGS et SGD); ils autorisent des pénalisations L1 ou L2 (par défaut) qu'il faudrait en principe optimiser."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR prend 168 s\n"
     ]
    }
   ],
   "source": [
    "## Logistic Regression\n",
    "import pyspark.mllib.regression\n",
    "from pyspark.mllib.classification import LogisticRegressionWithLBFGS\n",
    "# Les paramètres ont leurs valeurs par défaut\n",
    "time_start=time.time()\n",
    "model_lrm = LogisticRegressionWithLBFGS.train(trainData, \n",
    "            iterations=100, initialWeights=None, regParam=0.01, \n",
    "            regType='l2', intercept=False, corrections=10, tolerance=0.0001, \n",
    "            validateData=True, numClasses=10)\n",
    "#Parameters:\n",
    "# data – The training data, an RDD of LabeledPoint.\n",
    "# iterations – The number of iterations (default: 100).\n",
    "# initialWeights – The initial weights (default: None).\n",
    "# regParam – The regularizer parameter (default: 0.01).\n",
    "# regType – The type of regularizer used for training our model.“l1” for using L1 regularization\n",
    "#           “l2” for using L2 regularization None for no regularization(default: “l2”)\n",
    "# intercept – Boolean parameter which indicates the use or not of the augmented representation for training data \n",
    "#             (i.e. whether bias features are activated or not, default: False).\n",
    "# corrections – The number of corrections used in the LBFGS update (default: 10).\n",
    "# tolerance – The convergence tolerance of iterations for L-BFGS (default: 1e-4).\n",
    "# validateData – Boolean parameter which indicates if the algorithm should validate data before training.(default:True)\n",
    "# numClasses – The number of classes (i.e., outcomes) a label can take in Multinomial Logistic Regression (default: 2).\n",
    " \n",
    "time_end=time.time()\n",
    "time_lrm=(time_end - time_start)\n",
    "print(\"LR prend %d s\" %(time_lrm)) # (104s avec taux=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Erreur sur l'échantillon test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error = 0.0806\n"
     ]
    }
   ],
   "source": [
    "predictions = model_lrm.predict(testData.map(lambda x: x.features))\n",
    "labelsAndPredictions = testData.map(lambda lp: lp.label).zip(predictions)\n",
    "testErr = labelsAndPredictions.filter(lambda (v, p): v != p).count() / float(testData.count())\n",
    "print('Test Error = ' + str(testErr)) # (0.08 avec taux =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#### Problème non résolu avec ces lignes de commandes qui marchent pour la \n",
    "# régression logistique mais plus ensuite pour les arbres et RF !\n",
    "# Importation des critères d'erreur\n",
    "# from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "# concatanation de la prévision avec le vrai label\n",
    "# predictionAndLabels = testData.map(lambda lp: (float(model_lrm.predict(lp.features)), lp.label))\n",
    "# metrics = MulticlassMetrics(predictionAndLabels)\n",
    "# erreur=1-metrics.precision()\n",
    "# print(\"Taux d'erreur: \" + str(erreur))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# metrics.confusionMatrix().toArray() pas très lisible. Faire un graphique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Arbre binaire de décision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Même chose pour un arbre de discrimination. Comme pour l'implémentation de scikit-learn, les arbres ne peuvent être optimisés par un élagage basé sur une pénalisation de la complexité. Ce paramètre n'est pas présent, seule la profondeur max ou le nombre minimal d'observations par feuille peut contrôler la complexité. Noter l'apparition d'un nouveau paramètre: *maxBins* qui, schématiquement, rend qualitative ordinale à maxBins classes toute variable quantitative.  D'autre part, il n'y a pas de représentation graphique. Cette implémentation d'arbre est issue d'un [projet Google](http://static.googleusercontent.com/media/research.google.com/fr//pubs/archive/36296.pdf) pour adapter cet algorithme aux contraintes *mapreduce* de données sous Hadoop. Elle vaut surtout pour permettre de construire une implémentation des forêts aléatoires.\n",
    "\n",
    "Estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DT takes 106 s\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.tree import DecisionTree\n",
    "time_start=time.time()\n",
    "model_dt = DecisionTree.trainClassifier(trainData, \n",
    "        numClasses = 10, categoricalFeaturesInfo = {} , impurity='gini', \n",
    "        maxDepth=10,maxBins=32, minInstancesPerNode=1, minInfoGain=0.0)\n",
    "# Parameters:\n",
    "#•data – Training data: RDD of LabeledPoint. Labels are integers {0,1,...,numClasses}.\n",
    "#•numClasses – Number of classes for classification.\n",
    "#•categoricalFeaturesInfo – Map from categorical feature index to number of categories. Any feature not in this map \n",
    "# is treated as continuous.\n",
    "#•impurity – Supported values: “entropy” or “gini”\n",
    "#•maxDepth – Max depth of tree. E.g., depth 0 means 1 leaf node. Depth 1 means 1 internal node + 2 leaf nodes.\n",
    "#•maxBins – Number of bins used for finding splits at each node.\n",
    "#•minInstancesPerNode – Min number of instances required at child nodes to create the parent split\n",
    "#•minInfoGain – Min info gain required to create a split\n",
    " \n",
    "time_end=time.time()\n",
    "time_dt=(time_end - time_start)\n",
    "print(\"DT takes %d s\" %(time_dt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Erreur sur l'échantillon test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error = 0.1383\n"
     ]
    }
   ],
   "source": [
    "predictions = model_dt.predict(testData.map(lambda x: x.features))\n",
    "labelsAndPredictions = testData.map(lambda lp: lp.label).zip(predictions)\n",
    "testErr = labelsAndPredictions.filter(lambda (v, p): v != p).count() / float(testData.count())\n",
    "print('Test Error = ' + str(testErr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les $k$-nn ne sont pas \"scalables\" et donc pas présents. Voici la syntaxe et les paramètres associés à l'algorithme des forêts aléatoires. Parmi ceux \"classiques\" se trouvent *numTrees*, *featureSubsetStrategy*, *impurity*, *maxdepth* et en plus *maxbins* comme pour les arbres. Les valeurs du paramètres *maxDepth* est critique pour la qualité de la prévision. en principe, il n'est pas contraint, un arbre peut se déployer sans \"limite\" mais face à des données massives cela peut provoquer des plantages intempestifs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF takes 903 s\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.tree import RandomForest\n",
    "time_start=time.time()\n",
    "model_rf = RandomForest.trainClassifier(trainData, numClasses = 10,\n",
    "        categoricalFeaturesInfo = {}, numTrees = 100, \n",
    "        featureSubsetStrategy='auto', impurity='gini', maxDepth=12,\n",
    "        maxBins=32, seed=None)\n",
    "#Parameters:\n",
    "#•data – Training dataset: RDD of LabeledPoint. Labels should take values {0, 1, ..., numClasses-1}.\n",
    "#•numClasses – number of classes for classification.\n",
    "#•categoricalFeaturesInfo – Map storing arity of categorical features. E.g., an entry (n -> k) indicates that feature \n",
    "# n is categorical with k categories indexed from 0: {0, 1, ..., k-1}.\n",
    "#•numTrees – Number of trees in the random forest.\n",
    "#•featureSubsetStrategy – Number of features to consider for splits at each node. Supported: “auto” (default), “all”, \n",
    "# “sqrt”, “log2”, “onethird”. If “auto” is set, this parameter is set based on numTrees: if numTrees == 1,set to “all”; \n",
    "# if numTrees > 1 (forest) set to “sqrt”.\n",
    "#•impurity – Criterion used for information gain calculation. Supported values: “gini” (recommended) or “entropy”.\n",
    "#•maxDepth – Maximum depth of the tree. E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes. \n",
    "#(default: 4)\n",
    "#•maxBins – maximum number of bins used for splitting features (default: 32)\n",
    "#•seed – Random seed for bootstrapping and choosing feature subsets.\n",
    " \n",
    "model_rf.numTrees()\n",
    "model_rf.totalNumNodes()\n",
    "time_end=time.time()\n",
    "time_rf=(time_end - time_start)\n",
    "print(\"RF takes %d s\" %(time_rf))#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Erreur sur l'échantillon test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error = 0.0426\n"
     ]
    }
   ],
   "source": [
    "predictions = model_rf.predict(testData.map(lambda x: x.features))\n",
    "labelsAndPredictions = testData.map(lambda lp: lp.label).zip(predictions)\n",
    "testErr = labelsAndPredictions.filter(lambda (v, p): v != p).count() / float(testData.count())\n",
    "print('Test Error = ' + str(testErr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Même traitement sur la totalité de l'échantillon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Quelques résultats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "100 arbres, sélection automatique, maxDepth=9\n",
    "\n",
    "maxBins | Temps |  Erreur \n",
    "--------|-------|---------\n",
    "32 | 259 |  0.067 \n",
    "64 | 264 |  0.068 \n",
    "128 | 490 | 0.065\n",
    "\n",
    "100 arbres, sélection automatique, maxBins=32\n",
    "\n",
    "maxDepth | Temps | Erreur\n",
    "---------|-------|-------\n",
    "4 | 55 | 0.21\n",
    "9 | 259 |  0.067\n",
    "18 | 983 | **0.035**\n",
    "\n",
    "Le nombre de variables tirées à chaque noeud n'a pas été optimisé. \n",
    "\n",
    "Le paramètre maxBins ne semble pas trop influencer la précision du modèle, au contriare de la profondeur maximum des arbres. Avec une prfondeur suffisante, on retrouve les résultats classiques des forêts aléatoires sur ces données.\n",
    "\n",
    "L'implémentation de random forest dans Scikit learn se montre très efficace (temps d'exécution) sur ces données (cf. [calepin](http://www.math.univ-toulouse.fr/~besse/Wikistat/Notebooks/Atelier1-MNIST-python.html)). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark 1G ",
   "language": "python",
   "name": "pyspark1g"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
