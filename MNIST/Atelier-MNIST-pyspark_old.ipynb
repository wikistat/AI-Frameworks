{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<a href=\"http://www.insa-toulouse.fr/\" ><img src=\"http://www.math.univ-toulouse.fr/~besse/Wikistat/Images/logo-insa.jpg\" style=\"float:left; max-width: 120px; display: inline\" alt=\"INSA\"/></a> \n",
    "\n",
    "<a href=\"http://wikistat.fr/\" ><img src=\"http://www.math.univ-toulouse.fr/~besse/Wikistat/Images/wikistat.jpg\" style=\"max-width: 250px; display: inline\"  alt=\"Wikistat\"/></a>\n",
    "\n",
    "\n",
    "<a href=\"http://www.hupi.fr/\" ><img src=\"http://www.hupi.fr/wp-content/uploads/2016/03/hupi_logo_vectoris_menu.png\" style=\"float:right; max-width: 300px; display: inline\"  alt=\"Wikistat\"/></a>\n",
    "\n",
    " </a>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Ateliers: Technologies des grosses data](https://github.com/wikistat/Ateliers-Big-Data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Reconnaissance de caractères manuscrits](https://github.com/wikistat/Ateliers-Big-Data/2-MNIST) ([MNIST](http://yann.lecun.com/exdb/mnist/)) avec <a href=\"http://spark.apache.org/\"><img src=\"http://spark.apache.org/images/spark-logo-trademark.png\" style=\"max-width: 100px; display: inline\" alt=\"Spark\"/> </a> et [MLlib](http://spark.apache.org/mllib/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Introduction\n",
    "\n",
    "Même traitement en utilisant la librairie MLlib de Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Importation des packages\n",
    "import time\n",
    "from numpy import array\n",
    "from pyspark.mllib.regression import LabeledPoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Gestion des données"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Importation et transformation des données au format RDD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les données sont déjà partagée en une partie apprentissage et une test utilisée pour les comparaisons entre méthodes dans les publications. Ce sont bien les données du site MNIST mais transformée au format .csv pour en faciliter la lecture. \n",
    "\n",
    "Elles doivent être stockées à un emplacement accessibles de tous les noeuds du cluster pour permettre la construction de la base de données réparties (RDD). \n",
    "\n",
    "Dans une utilisation monoposte (*standalone*) de *Spark*, elles sont simplement chargées dans le répertoire courant. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Chargement des fichiers\n",
    "import urllib\n",
    "f = urllib.urlretrieve(\"https://www.math.univ-toulouse.fr/~besse/Wikistat/data/mnist_train.csv\",\"mnist_train.csv.csv\")\n",
    "f = urllib.urlretrieve(\"https://www.math.univ-toulouse.fr/~besse/Wikistat/data/mnist_test.csv\",\"mnist_test.csv.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformation du fichier texte en RDD de valeurs\n",
    "## Données d'apprentissage\n",
    "# Utiliser la bonne définition du chemin\n",
    "# Répertoire courant ou répertoire accessible de tous les \"workers\" du cluster\n",
    "path=\"\"\n",
    "# Transformation ou étape map de séparation des champs\n",
    "trainMnist = sc.textFile(path+\"mnist_train.csv\").map(lambda l: [float(x) for x in l.split(',')])\n",
    "# Action\n",
    "trainMnist.count() # taille de l'échantillon\n",
    "#test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mise au format \"labeled point\" spécifique de MLlib \"(label (v1, v2...vp])\"\n",
    "trainMnistLP=trainMnist.map(lambda line: LabeledPoint(line[-1],[line[0:753]]))\n",
    "trainMnistLP.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Même chose pour les données de test\n",
    "testMnist = sc.textFile(path+'mnist_test.csv').map(lambda l: [float(x) for x in l.split(',')])\n",
    "testMnist.count() # taille de l'échantillon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testData=testMnist.map(lambda line: LabeledPoint(line[-1],[line[0:753]]))\n",
    "testData.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Sous-échantillon d'apprentissage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Extraction d'un sous-échantillon d'apprentissage pour tester les programmes sur des données plus petites. Itérer cette démarche permet d'étudier l'évolution de l'erreur de prévision en fonction de la taille de l'échantillon d'apprentissage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tauxEch=0.1 # tester pour des tailles croissantes d'échantillon d'apprentissage\n",
    "(trainData, DropDatal) = trainMnistLP.randomSplit([tauxEch, 1-tauxEch])\n",
    "trainData.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 2.3 Régression logistique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exemple d'utilisation pour expliciter la syntaxe mais sans grand intérêt pour ces données qui ne satisfont pas à des frontières de discrimination linéaires. Par défaut, ce sont 10 modèles qui sont estimés, une classe contre les autres. Deux [algorithmes d'optimisation](http://spark.apache.org/docs/latest/api/python/pyspark.mllib.html#pyspark.mllib.classification.LogisticRegressionWithLBFGS) sont proposés (LBFGS et SGD); ils autorisent des pénalisations L1 ou L2 (par défaut) qu'il faudrait en principe optimiser."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Logistic Regression\n",
    "import pyspark.mllib.regression\n",
    "from pyspark.mllib.classification import LogisticRegressionWithLBFGS\n",
    "# Les paramètres ont leurs valeurs par défaut\n",
    "time_start=time.time()\n",
    "model_lrm = LogisticRegressionWithLBFGS.train(trainData, \n",
    "            iterations=100, initialWeights=None, regParam=0.01, \n",
    "            regType='l2', intercept=False, corrections=10, tolerance=0.0001, \n",
    "            validateData=True, numClasses=10)\n",
    "#Parameters:\n",
    "# data – The training data, an RDD of LabeledPoint.\n",
    "# iterations – The number of iterations (default: 100).\n",
    "# initialWeights – The initial weights (default: None).\n",
    "# regParam – The regularizer parameter (default: 0.01).\n",
    "# regType – The type of regularizer used for training our model.“l1” for using L1 regularization\n",
    "#           “l2” for using L2 regularization None for no regularization(default: “l2”)\n",
    "# intercept – Boolean parameter which indicates the use or not of the augmented representation for training data \n",
    "#             (i.e. whether bias features are activated or not, default: False).\n",
    "# corrections – The number of corrections used in the LBFGS update (default: 10).\n",
    "# tolerance – The convergence tolerance of iterations for L-BFGS (default: 1e-4).\n",
    "# validateData – Boolean parameter which indicates if the algorithm should validate data before training.(default:True)\n",
    "# numClasses – The number of classes (i.e., outcomes) a label can take in Multinomial Logistic Regression (default: 2).\n",
    " \n",
    "time_end=time.time()\n",
    "time_lrm=(time_end - time_start)\n",
    "print(\"LR prend %d s\" %(time_lrm)) # (104s avec taux=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Erreur sur l'échantillon test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model_lrm.predict(testData.map(lambda x: x.features))\n",
    "labelsAndPredictions = testData.map(lambda lp: lp.label).zip(predictions)\n",
    "testErr = labelsAndPredictions.filter(lambda (v, p): v != p).count() / float(testData.count())\n",
    "print('Test Error = ' + str(testErr)) # (0.08 avec taux =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Problème non résolu avec ces lignes de commandes qui marchent pour la \n",
    "# régression logistique mais plus ensuite pour les arbres et RF !\n",
    "# Importation des critères d'erreur\n",
    "# from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "# concatanation de la prévision avec le vrai label\n",
    "# predictionAndLabels = testData.map(lambda lp: (float(model_lrm.predict(lp.features)), lp.label))\n",
    "# metrics = MulticlassMetrics(predictionAndLabels)\n",
    "# erreur=1-metrics.precision()\n",
    "# print(\"Taux d'erreur: \" + str(erreur))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# metrics.confusionMatrix().toArray() pas très lisible. Faire un graphique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Arbre binaire de décision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Même chose pour un arbre de discrimination. Comme pour l'implémentation de scikit-learn, les arbres ne peuvent être optimisés par un élagage basé sur une pénalisation de la complexité. Ce paramètre n'est pas présent, seule la profondeur max ou le nombre minimal d'observations par feuille peut contrôler la complexité. Noter l'apparition d'un nouveau paramètre: *maxBins* qui, schématiquement, rend qualitative ordinale à maxBins classes toute variable quantitative.  D'autre part, il n'y a pas de représentation graphique. Cette implémentation d'arbre est issue d'un [projet Google](http://static.googleusercontent.com/media/research.google.com/fr//pubs/archive/36296.pdf) pour adapter cet algorithme aux contraintes *mapreduce* de données sous Hadoop. Elle vaut surtout pour permettre de construire une implémentation des forêts aléatoires.\n",
    "\n",
    "Estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.tree import DecisionTree\n",
    "time_start=time.time()\n",
    "model_dt = DecisionTree.trainClassifier(trainData, \n",
    "        numClasses = 10, categoricalFeaturesInfo = {} , impurity='gini', \n",
    "        maxDepth=10,maxBins=32, minInstancesPerNode=1, minInfoGain=0.0)\n",
    "# Parameters:\n",
    "#•data – Training data: RDD of LabeledPoint. Labels are integers {0,1,...,numClasses}.\n",
    "#•numClasses – Number of classes for classification.\n",
    "#•categoricalFeaturesInfo – Map from categorical feature index to number of categories. Any feature not in this map \n",
    "# is treated as continuous.\n",
    "#•impurity – Supported values: “entropy” or “gini”\n",
    "#•maxDepth – Max depth of tree. E.g., depth 0 means 1 leaf node. Depth 1 means 1 internal node + 2 leaf nodes.\n",
    "#•maxBins – Number of bins used for finding splits at each node.\n",
    "#•minInstancesPerNode – Min number of instances required at child nodes to create the parent split\n",
    "#•minInfoGain – Min info gain required to create a split\n",
    " \n",
    "time_end=time.time()\n",
    "time_dt=(time_end - time_start)\n",
    "print(\"DT takes %d s\" %(time_dt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Erreur sur l'échantillon test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model_dt.predict(testData.map(lambda x: x.features))\n",
    "labelsAndPredictions = testData.map(lambda lp: lp.label).zip(predictions)\n",
    "testErr = labelsAndPredictions.filter(lambda (v, p): v != p).count() / float(testData.count())\n",
    "print('Test Error = ' + str(testErr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les $k$-nn ne sont pas \"scalables\" et donc pas présents. Voici la syntaxe et les paramètres associés à l'algorithme des forêts aléatoires. Parmi ceux \"classiques\" se trouvent *numTrees*, *featureSubsetStrategy*, *impurity*, *maxdepth* et en plus *maxbins* comme pour les arbres. Les valeurs du paramètres *maxDepth* est critique pour la qualité de la prévision. en principe, il n'est pas contraint, un arbre peut se déployer sans \"limite\" mais face à des données massives cela peut provoquer des plantages intempestifs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.tree import RandomForest\n",
    "time_start=time.time()\n",
    "model_rf = RandomForest.trainClassifier(trainData, numClasses = 10,\n",
    "        categoricalFeaturesInfo = {}, numTrees = 100, \n",
    "        featureSubsetStrategy='auto', impurity='gini', maxDepth=12,\n",
    "        maxBins=32, seed=None)\n",
    "#Parameters:\n",
    "#•data – Training dataset: RDD of LabeledPoint. Labels should take values {0, 1, ..., numClasses-1}.\n",
    "#•numClasses – number of classes for classification.\n",
    "#•categoricalFeaturesInfo – Map storing arity of categorical features. E.g., an entry (n -> k) indicates that feature \n",
    "# n is categorical with k categories indexed from 0: {0, 1, ..., k-1}.\n",
    "#•numTrees – Number of trees in the random forest.\n",
    "#•featureSubsetStrategy – Number of features to consider for splits at each node. Supported: “auto” (default), “all”, \n",
    "# “sqrt”, “log2”, “onethird”. If “auto” is set, this parameter is set based on numTrees: if numTrees == 1,set to “all”; \n",
    "# if numTrees > 1 (forest) set to “sqrt”.\n",
    "#•impurity – Criterion used for information gain calculation. Supported values: “gini” (recommended) or “entropy”.\n",
    "#•maxDepth – Maximum depth of the tree. E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes. \n",
    "#(default: 4)\n",
    "#•maxBins – maximum number of bins used for splitting features (default: 32)\n",
    "#•seed – Random seed for bootstrapping and choosing feature subsets.\n",
    " \n",
    "model_rf.numTrees()\n",
    "model_rf.totalNumNodes()\n",
    "time_end=time.time()\n",
    "time_rf=(time_end - time_start)\n",
    "print(\"RF takes %d s\" %(time_rf))#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Erreur sur l'échantillon test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model_rf.predict(testData.map(lambda x: x.features))\n",
    "labelsAndPredictions = testData.map(lambda lp: lp.label).zip(predictions)\n",
    "testErr = labelsAndPredictions.filter(lambda (v, p): v != p).count() / float(testData.count())\n",
    "print('Test Error = ' + str(testErr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Même traitement sur la totalité de l'échantillon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Quelques résultats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "100 arbres, sélection automatique, maxDepth=9\n",
    "\n",
    "maxBins | Temps |  Erreur \n",
    "--------|-------|---------\n",
    "32 | 259 |  0.067 \n",
    "64 | 264 |  0.068 \n",
    "128 | 490 | 0.065\n",
    "\n",
    "100 arbres, sélection automatique, maxBins=32\n",
    "\n",
    "maxDepth | Temps | Erreur\n",
    "---------|-------|-------\n",
    "4 | 55 | 0.21\n",
    "9 | 259 |  0.067\n",
    "18 | 983 | **0.035**\n",
    "\n",
    "Le nombre de variables tirées à chaque noeud n'a pas été optimisé. \n",
    "\n",
    "Le paramètre maxBins ne semble pas trop influencer la précision du modèle, au contriare de la profondeur maximum des arbres. Avec une profondeur suffisante, on retrouve (presque) les résultats classiques des forêts aléatoires sur ces données.\n",
    "\n",
    "COmparer les résultats obtenus pour les trois environnements. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "PySpark (Spark 2.2.0)",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  },
  "toc": {
   "nav_menu": {
    "height": "226px",
    "width": "252px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
