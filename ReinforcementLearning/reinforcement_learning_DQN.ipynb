{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Ateliers: Technologies de l'intelligence Artificielle](https://github.com/wikistat/AI-Frameworks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<a href=\"http://www.insa-toulouse.fr/\" ><img src=\"http://www.math.univ-toulouse.fr/~besse/Wikistat/Images/logo-insa.jpg\" style=\"float:left; max-width: 120px; display: inline\" alt=\"INSA\"/></a> \n",
    "<a href=\"http://wikistat.fr/\" ><img src=\"http://www.math.univ-toulouse.fr/~besse/Wikistat/Images/wikistat.jpg\" width=400, style=\"max-width: 150px; display: inline\"  alt=\"Wikistat\"/></a>\n",
    "<a href=\"http://www.math.univ-toulouse.fr/\" ><img src=\"http://www.math.univ-toulouse.fr/~besse/Wikistat/Images/logo_imt.jpg\" width=400,  style=\"float:right;  display: inline\" alt=\"IMT\"/> </a>\n",
    "    \n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LAB 4 Introduction to Reinforcement Learning  -  Part 1 : Policy Gradient Algorithm\n",
    "\n",
    "The objectives of this noteboks are the following : \n",
    "\n",
    "* Q-Learning on simple Markov Decision Process\n",
    "* DQN\n",
    " \n",
    "Source : [https://github.com/ageron/handson-ml](https://github.com/ageron/handson-ml) and https://github.com/breeko/Simple-Reinforcement-Learning-with-Tensorflow/blob/master/Part%202%20-%20Policy-based%20Agents%20with%20Keras.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we will be using [OpenAI gym](https://gym.openai.com/), a great toolkit for developing and comparing Reinforcement Learning algorithms. It provides many environments for your learning *agents* to interact with."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import librairies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "def reset_graph(seed=42):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "# To plot figures and animations\n",
    "%matplotlib inline\n",
    "%matplotlib nbagg\n",
    "import matplotlib\n",
    "import matplotlib.animation as animation\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import HTML\n",
    "\n",
    "\n",
    "plt.rcParams['axes.labelsize'] = 14\n",
    "plt.rcParams['xtick.labelsize'] = 12\n",
    "plt.rcParams['ytick.labelsize'] = 12\n",
    "\n",
    "\n",
    "import tensorflow.keras.models as km\n",
    "import tensorflow.keras.layers as kl\n",
    "import tensorflow.keras.initializers as ki\n",
    "import tensorflow.keras.optimizers as ko\n",
    "import tensorflow.keras.losses as klo\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "\n",
    "# Gym Librairy\n",
    "import gym\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def update_scene(num, frames, patch):\n",
    "    patch.set_data(frames[num])\n",
    "    return patch,\n",
    "\n",
    "def plot_animation(frames, repeat=False, interval=400):\n",
    "    plt.close()  # or else nbagg sometimes plots in the previous cell\n",
    "    fig = plt.figure()\n",
    "    patch = plt.imshow(frames[0])\n",
    "    plt.axis('off')\n",
    "    return animation.FuncAnimation(fig, update_scene, fargs=(frames, patch), frames=len(frames), repeat=repeat, interval=interval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q-Learning on Markov Decision Process\n",
    "\n",
    "## Definition\n",
    "\n",
    "We will first define a simple markov process on wich we will apply Q-learning algorithm.\n",
    "\n",
    "here is an illustration of the MDP that we will define.\n",
    "\n",
    "![images](images/mdp.png)\n",
    "\n",
    "\n",
    "We store the different action and transition probabilities within a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "transition_probabilities = [\n",
    "        [[0.7, 0.3, 0.0], [1.0, 0.0, 0.0], [0.8, 0.2, 0.0]], \n",
    "        [[0.0, 1.0, 0.0], None, [0.0, 0.0, 1.0]],\n",
    "        [None, [0.8, 0.1, 0.1], None],\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use pandas DataFrame visualisation in order to display this table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Actions</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>State</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[0.7, 0.3, 0.0]</td>\n",
       "      <td>[1.0, 0.0, 0.0]</td>\n",
       "      <td>[0.8, 0.2, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[0.0, 1.0, 0.0]</td>\n",
       "      <td>None</td>\n",
       "      <td>[0.0, 0.0, 1.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>None</td>\n",
       "      <td>[0.8, 0.1, 0.1]</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Actions                0                1                2\n",
       "State                                                     \n",
       "0        [0.7, 0.3, 0.0]  [1.0, 0.0, 0.0]  [0.8, 0.2, 0.0]\n",
       "1        [0.0, 1.0, 0.0]             None  [0.0, 0.0, 1.0]\n",
       "2                   None  [0.8, 0.1, 0.1]             None"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transition_probabilities_df = pd.DataFrame(transition_probabilities).rename_axis('Actions', axis=1)\n",
    "transition_probabilities_df.index.name=\"State\"\n",
    "transition_probabilities_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also define the rewards, each actions couple state/action will lead to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Actions</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>State</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[10, 0, 0]</td>\n",
       "      <td>[0, 0, 0]</td>\n",
       "      <td>[0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[0, 0, 0]</td>\n",
       "      <td>[0, 0, 0]</td>\n",
       "      <td>[0, 0, -50]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[0, 0, 0]</td>\n",
       "      <td>[40, 0, 0]</td>\n",
       "      <td>[0, 0, 0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Actions           0           1            2\n",
       "State                                       \n",
       "0        [10, 0, 0]   [0, 0, 0]    [0, 0, 0]\n",
       "1         [0, 0, 0]   [0, 0, 0]  [0, 0, -50]\n",
       "2         [0, 0, 0]  [40, 0, 0]    [0, 0, 0]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rewards = [\n",
    "        [[+10, 0, 0], [0, 0, 0], [0, 0, 0]],\n",
    "        [[0, 0, 0], [0, 0, 0], [0, 0, -50]],\n",
    "        [[0, 0, 0], [+40, 0, 0], [0, 0, 0]],\n",
    "    ]\n",
    "\n",
    "rewards_df = pd.DataFrame(rewards).rename_axis('Actions', axis=1)\n",
    "rewards_df.index.name=\"State\"\n",
    "rewards_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the list of possible action that can be taken at each state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "possible_actions = [[0, 1, 2], [0, 2], [1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we define now a class that will act as a Gym environment. \n",
    "\n",
    "* The environent is the MDP.\n",
    "* The observation is the current step.\n",
    "* The action possible are the three actions we previously define."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MDPEnvironment(object):\n",
    "    def __init__(self, start_state=0):\n",
    "        self.start_state=start_state\n",
    "        self.reset()\n",
    "    def reset(self):\n",
    "        self.total_rewards = 0\n",
    "        self.state = self.start_state\n",
    "    def step(self, action):\n",
    "        next_state = np.random.choice(range(3), p=transition_probabilities[self.state][action])\n",
    "        reward = rewards[self.state][action][next_state]\n",
    "        self.state = next_state\n",
    "        self.total_rewards += reward\n",
    "        return self.state, reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_episode(policy, n_steps, start_state=0):\n",
    "    env = MDPEnvironment()\n",
    "    for step in range(n_steps):\n",
    "        action = policy(env.state)\n",
    "        state, reward = env.step(action)\n",
    "    return env.total_rewards\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hard Coded Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary: mean=-25.6, std=86.104750, min=-320, max=220\n"
     ]
    }
   ],
   "source": [
    "def policy_random(state):\n",
    "    return np.random.choice(possible_actions[state])\n",
    "\n",
    "\n",
    "all_score = []\n",
    "for episode in range(1000):\n",
    "    all_score.append(run_episode(policy_random, n_steps=100))\n",
    "print(\"Summary: mean={:.1f}, std={:1f}, min={}, max={}\".format(np.mean(all_score), np.std(all_score), np.min(all_score), np.max(all_score)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise** Which policy will be the safest? The more risky? Implement it and test it. What can you say about their results?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's know implement Q-learning algorithm to learn a bette policy!\n",
    "\n",
    "Q-Learning works by watching an agent play (e.g., randomly) and gradually improving its estimates of the Q-Values. \n",
    "Once it has accurate Q-Value estimates (or close enough), then the optimal policy consists in choosing the action that has the highest Q-Value (i.e., the greedy policy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0.,   0.,   0.],\n",
       "       [  0., -inf,   0.],\n",
       "       [-inf,   0., -inf]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_states = 3\n",
    "n_actions = 3\n",
    "n_steps = 20\n",
    "alpha = 0.01  #<-- Learning Rate\n",
    "gamma = 0.99  #<-- The discount rate\n",
    "\n",
    "\n",
    " \n",
    "exploration_policy = policy_random #<-- Policy that we will play during exploration\n",
    "q_values = np.full((n_states, n_actions), -np.inf) #<-- Policy that we will be updated\n",
    "for state, actions in enumerate(possible_actions):\n",
    "    q_values[state][actions]=0\n",
    "q_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learning Q_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 9.90000000e-02,  9.90000000e-04,  1.93089501e-03],\n",
       "       [ 0.00000000e+00,            -inf, -1.47322029e+00],\n",
       "       [           -inf,  1.19079301e+00,            -inf]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = MDPEnvironment()\n",
    "for step in range(n_steps):    \n",
    "    action = exploration_policy(env.state) \n",
    "    state = env.state\n",
    "    next_state, reward = env.step(action)\n",
    "    next_value = np.max(q_values[next_state]) # greedy policy\n",
    "    q_values[state, action] = (1-alpha)*q_values[state, action] + alpha*(reward + gamma * next_value)\n",
    "q_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise** How do we defined the optimal policy from the computed Q_values? Implement it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimal_policy(state):\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/exercise2_2.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute its performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not NoneType",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-fc69254f4d6c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mall_totals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepisode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mall_totals\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_episode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimal_policy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Summary: mean={:.1f}, std={:1f}, min={}, max={}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_totals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_totals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_totals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_totals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-dda21e9dcbf7>\u001b[0m in \u001b[0;36mrun_episode\u001b[0;34m(policy, n_steps, start_state)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtotal_rewards\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-e25e3f9fcd72>\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mnext_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransition_probabilities\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0mreward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnext_state\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: list indices must be integers or slices, not NoneType"
     ]
    }
   ],
   "source": [
    "all_totals = []\n",
    "for episode in range(1000):\n",
    "    all_totals.append(run_episode(optimal_policy, n_steps=100))\n",
    "print(\"Summary: mean={:.1f}, std={:1f}, min={}, max={}\".format(np.mean(all_totals), np.std(all_totals), np.min(all_totals), np.max(all_totals)))\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q** We have Q learning iteration algorithm to learn the best policy. Would it have been possible to use a different algorithm here? Why?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DEEP Q Learning on *Pacman-Like* Game\n",
    "\n",
    "We have seen how to learn an optimal policy on a Markov Decision Process. \n",
    "\n",
    "Let's now try to learn how to play a Video Game Like Pacman. \n",
    "\n",
    "We won't use directly Pacman here (Even if it's available on Gym environment (see below)). The Pacman game requires GPU to be trained and may need some try and parameter optimization to easily converged. \n",
    "\n",
    "Hence, we will start with and simpler environment which looks like Pacman : the [gridworld](https://github.com/awjuliani/DeepRL-Agents/blob/master/gridworld.py) environment.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid World environment\n",
    "\n",
    "We have a 5x5 blocs where blue, green and red squares. In the environment the agent controls a blue square, and the goal is to navigate to the green squares (reward +1) while avoiding the red squares (reward -1).\n",
    "\n",
    "### Observation\n",
    "\n",
    "The observation is the image itself which is a 84x84x3 images.\n",
    "\n",
    "### Actions\n",
    "\n",
    "Num | Action\n",
    "--- | ---\n",
    "0 | Go Up\n",
    "1 | Go Down\n",
    "2 | Go Left\n",
    "3 | Go Right\n",
    "\n",
    "\n",
    "### Reward\n",
    "Reward is 1 for every green square taken and -1 when a red square is taken\n",
    "\n",
    "### End epsiode\n",
    "There are no condition limit for an episode to finish"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We initiate the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP4AAAD/CAYAAADRymv0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAA05JREFUeJzt3TEKQkEQBUFXvP+Vx1BBzNQRuio1+Js0kwjvzMwFaLluPwD4PeFDkPAhSPgQJHwIEj4ECR+ChA9Bwoeg29J3/V0Qvu+8+8HFhyDhQ5DwIUj4ECR8CBI+BAkfgoQPQcKHIOFDkPAhSPgQJHwIEj4ECR+ChA9Bwocg4UOQ8CFI+BAkfAgSPgQJH4KED0HChyDhQ5DwIUj4ECR8CBI+BG3NZK855+1yMHzczH8uwrv4ECR8CBI+BAkfgoQPQcKHIOFDkPAhSPgQJHwIEj4ECR+ChA9Bwocg4UOQ8CFI+BAkfAgSPgQJH4KED0HChyDhQ5DwIUj4ECR8CBI+BAkfgoQPQcKHoNxM9q7NyWTz4Dy4+BAkfAgSPgQJH4KED0HChyDhQ5DwIUj4ECR8CBI+BAkfgoQPQcKHIOFDkPAhSPgQJHwIEj4ECR+ChA9Bwocg4UOQ8CFI+BAkfAgSPgQJH4KED0HCh6DeTPbiUvWcvanqs7nQvc1C+AsXH4KED0HChyDhQ5DwIUj4ECR8CBI+BAkfgoQPQcKHIOFDkPAhSPgQJHwIEj4ECR+ChA9Bwocg4UOQ8CFI+BAkfAgSPgQJH4KED0HChyDhQ5DwIUj4ECR8CLptP+DXZnErfXWm3UY8T1x8CBI+BAkfgoQPQcKHIOFDkPAhSPgQJHwIEj4ECR+ChA9Bwocg4UOQ8CFI+BAkfAgSPgQJH4KED0HChyDhQ5DwIUj4ECR8CBI+BAkfgoQPQcKHIOFDUG4m21p00Gw/4P+4+BAkfAgSPgQJH4KED0HChyDhQ5DwIUj4ECR8CBI+BAkfgoQPQcKHIOFDkPAhSPgQJHwIEj4ECR+ChA9Bwocg4UOQ8CFI+BAkfAgSPgQJH4KED0HCh6DcTPaMzWRw8SFI+BAkfAgSPgQJH4KED0HChyDhQ5DwIUj4ECR8CBI+BAkfgoQPQcKHIOFDkPAhSPgQJHwIEj4ECR+ChA9Bwocg4UOQ8CFI+BAkfAgSPgQJH4KED0FbM9ln6bvAxcWHJOFDkPAhSPgQJHwIEj4ECR+ChA9Bwocg4UOQ8CFI+BAkfAgSPgQJH4KED0HChyDhQ5DwIUj4ECR8CBI+BAkfgoQPQcKHIOFD0B092RIMz+Z7qQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from gridworld import gameEnv\n",
    "env = gameEnv(partial=False, size=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's play a game by, fill the actions_list according to the intialised environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-651ec31d4145>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mcum_reward\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mcum_reward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mHTML\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mplot_animation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_html5_video\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-1-52314c313ed2>\u001b[0m in \u001b[0;36mplot_animation\u001b[0;34m(frames, repeat, interval)\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# or else nbagg sometimes plots in the previous cell\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0mfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m     \u001b[0mpatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'off'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0manimation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFuncAnimation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupdate_scene\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrepeat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minterval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minterval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "frames = []\n",
    "cum_reward=0\n",
    "actions=[]\n",
    "for step in actions:\n",
    "    state, reward , end = env.step(step)\n",
    "    frames.append(state)\n",
    "    cum_reward+=reward\n",
    "cum_reward\n",
    "HTML(plot_animation(frames).to_html5_video())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original shape: (84, 84, 3)\n",
      "Processed shape: (84, 84, 1)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP4AAAD/CAYAAADRymv0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAA4RJREFUeJzt3bGNIkEQQNHp06ZBQkgkQkCQCCIhAumz1sTb3UL677ljVDtf5YxUa+99AC3/ph8A/D3hQ5DwIUj4ECR8CBI+BAkfgoQPQcKHoK+huX4XhN+33n2w8SFI+BAkfAgSPgQJH4KED0HChyDhQ5DwIUj4ECR8CBI+BAkfgoQPQcKHIOFDkPAhSPgQJHwIEj4ECR+ChA9Bwocg4UOQ8CFI+BAkfAgSPgQJH4KED0FTZ7LHrPX2cjD8uL0/8yK8jQ9Bwocg4UOQ8CFI+BAkfAgSPgQJH4KED0HChyDhQ5DwIUj4ECR8CBI+BAkfgoQPQcKHIOFDkPAhSPgQJHwIEj4ECR+ChA9Bwocg4UOQ8CFI+BAkfAjKncmedDqdxma/Xq+x2XweGx+ChA9Bwocg4UOQ8CFI+BAkfAgSPgQJH4KED0HChyDhQ5DwIUj4ECR8CBI+BAkfgoQPQcKHIOFDkPAhSPgQJHwIEj4ECR+ChA9Bwocg4UOQ8CFI+BC09t4Tc0eGHsdx3O/3qdHH8/kcm30+n8dmT7ter2Ozh/r6tt59sPEhSPgQJHwIEj4ECR+ChA9Bwocg4UOQ8CFI+BAkfAgSPgQJH4KED0HChyDhQ5DwIUj4ECR8CBI+BAkfgoQPQcKHIOFDkPAhSPgQJHwIEj4ECR+ChA9BwoegNXS/e+xo+OVymRp9PB6PsdnMGOrr23r3wcaHIOFDkPAhSPgQJHwIEj4ECR+ChA9Bwocg4UOQ8CFI+BAkfAgSPgQJH4KED0HChyDhQ5DwIUj4ECR8CBI+BAkfgoQPQcKHIOFDkPAhSPgQJHwIEj4EfU0/4K85Vd1zu92mn/BxbHwIEj4ECR+ChA9Bwocg4UOQ8CFI+BAkfAgSPgQJH4KED0HChyDhQ5DwIUj4ECR8CBI+BAkfgoQPQcKHIOFDkPAhSPgQJHwIEj4ECR+ChA9Bwocg4UNQ7kz23nv6CTDOxocg4UOQ8CFI+BAkfAgSPgQJH4KED0HChyDhQ5DwIUj4ECR8CBI+BAkfgoQPQcKHIOFDkPAhSPgQJHwIEj4ECR+ChA9Bwocg4UOQ8CFI+BAkfAgSPgRNncleQ3OBw8aHJOFDkPAhSPgQJHwIEj4ECR+ChA9Bwocg4UOQ8CFI+BAkfAgSPgQJH4KED0HChyDhQ5DwIUj4ECR8CBI+BAkfgoQPQcKHIOFD0H84Iynb7tS7dgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def process_state(state):\n",
    "    \"\"\" This processes the state and converts to black & white. This will help speed up learning\n",
    "        as the number of parameters will be reduced.\n",
    "    \"\"\"\n",
    "    state = state/255\n",
    "    return np.expand_dims(np.dot(state[...,:3], [0.2989, 0.5870, 0.1140]),axis=-1)\n",
    "    return state\n",
    "\n",
    "\n",
    "plt.imshow(process_state(env.state)[:,:,0],cmap=plt.get_cmap('gray'))\n",
    "plt.axis(\"off\")\n",
    "\n",
    "print(\"Original shape: {}\".format(env.state.shape))\n",
    "print(\"Processed shape: {}\".format(process_state(env.state).shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* It could just take a state-action pair (s,a) as input, and out‐ put an estimate of the corresponding Q-Value Q(s,a), but since the actions are dis‐ crete it is more convenient to use a neural network that takes only a state s as input and outputs one Q-Value estimate per action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning to Play MsPacman Using the DQN Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Warning**: Unfortunately, the first version of the book contained two important errors in this section.\n",
    "\n",
    "1. The actor DQN and critic DQN should have been named _online DQN_ and _target DQN_ respectively. Actor-critic algorithms are a distinct class of algorithms.\n",
    "2. The online DQN is the one that learns and is copied to the target DQN at regular intervals. The target DQN's only role is to estimate the next state's Q-Values for each possible action. This is needed to compute the target Q-Values for training the online DQN, as shown in this equation:\n",
    "\n",
    "$y(s,a) = \\text{r} + \\gamma . \\underset{a'}{\\max} \\, Q_\\text{target}(s', a')$\n",
    "\n",
    "* $y(s,a)$ is the target Q-Value to train the online DQN for the state-action pair $(s, a)$.\n",
    "* $r$ is the reward actually collected after playing action $a$ in state $s$.\n",
    "* $\\gamma$ is the discount rate.\n",
    "* $s'$ is the state actually reached after played action $a$ in state $s$.\n",
    "* $a'$ is one of the possible actions in state $s'$.\n",
    "* $Q_\\text{target}(s', a')$ is the target DQN's estimate of the Q-Value of playing action $a'$ while in state $s'$.\n",
    "\n",
    "I hope these errors did not affect you, and if they did, I sincerely apologize."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the MsPacman environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(210, 160, 3)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = gym.make(\"MsPacman-v0\")\n",
    "obs = env.reset()\n",
    "obs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(9)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing the images is optional but greatly speeds up training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(88, 80, 1)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mspacman_color = 210 + 164 + 74\n",
    "\n",
    "def preprocess_observation(obs):\n",
    "    img = obs[1:176:2, ::2] # crop and downsize\n",
    "    img = img.sum(axis=2) # to greyscale\n",
    "    img[img==mspacman_color] = 0 # Improve contrast\n",
    "    img = (img // 3 - 128).astype(np.int8) # normalize from -128 to 127\n",
    "    return img.reshape(88, 80, 1)/128\n",
    "\n",
    "img = preprocess_observation(obs)\n",
    "img.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: the `preprocess_observation()` function is slightly different from the one in the book: instead of representing pixels as 64-bit floats from -1.0 to 1.0, it represents them as signed bytes (from -128 to 127). The benefit is that the replay memory will take up roughly 8 times less RAM (about 6.5 GB instead of 52 GB). The reduced precision has no visible impact on training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 792x504 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(11, 7))\n",
    "plt.subplot(121)\n",
    "plt.title(\"Original observation (160×210 RGB)\")\n",
    "plt.imshow(obs)\n",
    "plt.axis(\"off\")\n",
    "plt.subplot(122)\n",
    "plt.title(\"Preprocessed observation (88×80 greyscale)\")\n",
    "plt.imshow(img.reshape(88, 80), interpolation=\"nearest\", cmap=\"gray\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build DQN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: instead of using `tf.contrib.layers.convolution2d()` or `tf.contrib.layers.conv2d()` (as in the first version of the book), we now use the `tf.layers.conv2d()`, which did not exist when this chapter was written. This is preferable because anything in contrib may change or be deleted without notice, while `tf.layers` is part of the official API. As you will see, the code is mostly the same, except that the parameter names have changed slightly:\n",
    "* the `num_outputs` parameter was renamed to `filters`,\n",
    "* the `stride` parameter was renamed to `strides`,\n",
    "* the `_fn` suffix was removed from parameter names that had it (e.g., `activation_fn` was renamed to `activation`),\n",
    "* the `weights_initializer` parameter was renamed to `kernel_initializer`,\n",
    "* the weights variable was renamed to `\"kernel\"` (instead of `\"weights\"`), and the biases variable was renamed from `\"biases\"` to `\"bias\"`,\n",
    "* and the default `activation` is now `None` instead of `tf.nn.relu`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_height = 88\n",
    "input_width = 80\n",
    "input_channels = 1\n",
    "initializer =  ki.VarianceScaling()\n",
    "\n",
    "\n",
    "\n",
    "def q_network():\n",
    "    q_model = km.Sequential()\n",
    "    q_model.add(kl.Conv2D(32,kernel_size=(8,8),strides=(4,4),padding=\"SAME\", activation=\"relu\", kernel_initializer = initializer, \n",
    "                          input_shape=(input_height, input_width, input_channels), data_format=\"channels_last\"))\n",
    "    q_model.add(kl.Conv2D(64,kernel_size=(4,4),strides=(2,2),padding=\"SAME\", activation=\"relu\",  kernel_initializer = initializer))\n",
    "    q_model.add(kl.Conv2D(64,kernel_size=(3,4),strides=(1,1),padding=\"SAME\", activation=\"relu\",  kernel_initializer = initializer))\n",
    "    q_model.add(kl.Flatten())\n",
    "    q_model.add(kl.Dense(512, activation=\"relu\", kernel_initializer=initializer))\n",
    "    q_model.add(kl.Dense(9, activation=\"linear\", kernel_initializer=initializer))\n",
    "    \n",
    "    learning_rate = 0.001\n",
    "    momentum = 0.95\n",
    "    Opt = ko.SGD(lr=learning_rate, momentum=momentum , decay=0.0, nesterov=True)\n",
    "    q_model.compile(loss='mean_squared_error',optimizer=Opt) # later change to square loss if error >1 else linear\n",
    "\n",
    "\n",
    "\n",
    "    return q_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if False : \n",
    "    with tf.variable_scope(\"train\"):\n",
    "        X_action = tf.placeholder(tf.int32, shape=[None])\n",
    "        y = tf.placeholder(tf.float32, shape=[None, 1])\n",
    "        q_value = tf.reduce_sum(online_q_values * tf.one_hot(X_action, n_outputs),\n",
    "                                axis=1, keepdims=True)\n",
    "        error = tf.abs(y - q_value)\n",
    "        clipped_error = tf.clip_by_value(error, 0.0, 1.0)\n",
    "        linear_error = 2 * (error - clipped_error)\n",
    "        loss = tf.reduce_mean(tf.square(clipped_error) + linear_error)\n",
    "\n",
    "        global_step = tf.Variable(0, trainable=False, name='global_step')\n",
    "        optimizer = tf.train.MomentumOptimizer(learning_rate, momentum, use_nesterov=True)\n",
    "        training_op = optimizer.minimize(loss, global_step=global_step)\n",
    "\n",
    "    init = tf.global_variables_initializer()\n",
    "    saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: in the first version of the book, the loss function was simply the squared error between the target Q-Values (`y`) and the estimated Q-Values (`q_value`). However, because the experiences are very noisy, it is better to use a quadratic loss only for small errors (below 1.0) and a linear loss (twice the absolute error) for larger errors, which is what the code above computes. This way large errors don't push the model parameters around as much. Note that we also tweaked some hyperparameters (using a smaller learning rate, and using Nesterov Accelerated Gradients rather than Adam optimization, since adaptive gradient algorithms may sometimes be bad, according to this [paper](https://arxiv.org/abs/1705.08292)). We also tweaked a few other hyperparameters below (a larger replay memory, longer decay for the $\\epsilon$-greedy policy, larger discount rate, less frequent copies of the online DQN to the target DQN, etc.)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use this `ReplayMemory` class instead of a `deque` because it is much faster for random access (thanks to @NileshPS who contributed it). Moreover, we default to sampling with replacement, which is much faster than sampling without replacement for large replay memories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExperienceReplay:\n",
    "    def __init__(self,buffer_size=500000):\n",
    "        \"\"\" Data structure used to hold game experiences \"\"\"\n",
    "        # Buffer will contain [state,action,reward,next_state,done]\n",
    "        self.buffer = []\n",
    "        self.buffer_size = buffer_size\n",
    "    \n",
    "    def add(self, experience):\n",
    "        \"\"\" Adds list of experiences to the buffer \"\"\"\n",
    "        # Extend the stored experiences\n",
    "        self.buffer.extend(experience)\n",
    "        # Keep the last buffer_size number of experiences\n",
    "        self.buffer = self.buffer[-self.buffer_size:]\n",
    "        \n",
    "    def sample(self, size):\n",
    "        \"\"\" Returns a sample of experiences from the buffer \"\"\"\n",
    "        sample_idxs = np.random.randint(len(self.buffer),size=size)\n",
    "        sample_output = [self.buffer[idx] for idx in sample_idxs]\n",
    "        sample_output = np.reshape(sample_output,(size,-1))\n",
    "        return sample_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_memories(batch_size):\n",
    "    cols = [[], [], [], [], []] # state, action, reward, next_state, continue\n",
    "    for memory in replay_memory.sample(batch_size):\n",
    "        for col, value in zip(cols, memory):\n",
    "            col.append(value)\n",
    "    cols = [np.array(col) for col in cols]\n",
    "    return cols[0], cols[1], cols[2].reshape(-1, 1), cols[3], cols[4].reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps_min = 0.1\n",
    "eps_max = 1.0\n",
    "eps_decay_steps = 2000000\n",
    "\n",
    "def epsilon_greedy(q_values, step):\n",
    "    epsilon = max(eps_min, eps_max - (eps_max-eps_min) * step/eps_decay_steps)\n",
    "    if np.random.rand() < epsilon:\n",
    "        return np.random.randint(n_outputs) # random action\n",
    "    else:\n",
    "        return np.argmax(q_values) # optimal action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_random_start = 1\n",
    "prob_random_end = 0.1\n",
    "annealing_steps = 2000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_target_graph(main_graph, target_graph, tau):\n",
    "    updated_weights = (np.array(main_graph.get_weights()) * tau) + \\\n",
    "        (np.array(target_graph.get_weights()) * (1 - tau))\n",
    "    target_graph.set_weights(updated_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_episodes = 4000000  # total number of training steps\n",
    "pre_train_episodes = 1000  # start training after 10,000 game iterations\n",
    "update_freq = 4  # run a training step every 4 game iterations\n",
    "save_steps = 1000  # save the model every 1,000 training steps\n",
    "copy_steps = 10000  # copy online DQN to target DQN every 10,000 training steps\n",
    "y = 0.99\n",
    "skip_start = 90  # Skip the start of every game (it's just waiting time).\n",
    "batch_size = 50\n",
    "iteration = 0  # game iterations\n",
    "num_epochs=20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A few variables for tracking progress:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_val = np.infty\n",
    "game_length = 0\n",
    "total_max_q = 0\n",
    "mean_max_q = 0.0\n",
    "tau=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num episode: 100 Mean reward: 213.0000 Prob random: 1.0000, Loss: 0.0000\n",
      "Num episode: 200 Mean reward: 221.3000 Prob random: 1.0000, Loss: 0.0000\n",
      "Num episode: 300 Mean reward: 194.7000 Prob random: 1.0000, Loss: 0.0000\n",
      "Num episode: 400 Mean reward: 203.3000 Prob random: 1.0000, Loss: 0.0000\n",
      "Num episode: 500 Mean reward: 218.0000 Prob random: 1.0000, Loss: 0.0000\n",
      "Num episode: 600 Mean reward: 200.1000 Prob random: 1.0000, Loss: 0.0000\n",
      "Num episode: 700 Mean reward: 218.6000 Prob random: 1.0000, Loss: 0.0000\n",
      "Num episode: 800 Mean reward: 224.8000 Prob random: 1.0000, Loss: 0.0000\n",
      "Num episode: 900 Mean reward: 226.2000 Prob random: 1.0000, Loss: 0.0000\n",
      "Num episode: 1000 Mean reward: 203.1000 Prob random: 1.0000, Loss: 0.0000\n",
      "Num episode: 1100 Mean reward: 209.4000 Prob random: 1.0000, Loss: 0.5013\n",
      "Num episode: 1200 Mean reward: 209.2000 Prob random: 0.9999, Loss: 0.6541\n",
      "Num episode: 1300 Mean reward: 215.1000 Prob random: 0.9999, Loss: 0.6445\n",
      "Num episode: 1400 Mean reward: 227.5000 Prob random: 0.9998, Loss: 0.7420\n",
      "Num episode: 1500 Mean reward: 212.2000 Prob random: 0.9998, Loss: 0.7383\n",
      "Num episode: 1600 Mean reward: 214.8000 Prob random: 0.9997, Loss: 0.6655\n",
      "Num episode: 1700 Mean reward: 210.5000 Prob random: 0.9997, Loss: 0.6805\n",
      "Num episode: 1800 Mean reward: 213.5000 Prob random: 0.9996, Loss: 0.5509\n",
      "Num episode: 1900 Mean reward: 203.6000 Prob random: 0.9996, Loss: 0.6090\n",
      "Num episode: 2000 Mean reward: 213.8000 Prob random: 0.9996, Loss: 0.6161\n",
      "Num episode: 2100 Mean reward: 230.9000 Prob random: 0.9995, Loss: 0.5686\n",
      "Num episode: 2200 Mean reward: 205.0000 Prob random: 0.9995, Loss: 0.6165\n",
      "Num episode: 2300 Mean reward: 202.6000 Prob random: 0.9994, Loss: 0.6146\n",
      "Num episode: 2400 Mean reward: 204.1000 Prob random: 0.9994, Loss: 0.6273\n",
      "Num episode: 2500 Mean reward: 221.3000 Prob random: 0.9993, Loss: 0.6867\n",
      "Num episode: 2600 Mean reward: 204.8000 Prob random: 0.9993, Loss: 0.6517\n",
      "Num episode: 2700 Mean reward: 208.4000 Prob random: 0.9992, Loss: 0.6643\n",
      "Num episode: 2800 Mean reward: 218.3000 Prob random: 0.9992, Loss: 0.6524\n",
      "Num episode: 2900 Mean reward: 206.2000 Prob random: 0.9991, Loss: 0.7132\n",
      "Num episode: 3000 Mean reward: 219.4000 Prob random: 0.9991, Loss: 0.7256\n",
      "Num episode: 3100 Mean reward: 194.8000 Prob random: 0.9991, Loss: 0.8196\n",
      "Num episode: 3200 Mean reward: 212.1000 Prob random: 0.9990, Loss: 0.7993\n",
      "Num episode: 3300 Mean reward: 199.1000 Prob random: 0.9990, Loss: 0.6697\n",
      "Num episode: 3400 Mean reward: 212.5000 Prob random: 0.9989, Loss: 0.6327\n",
      "Num episode: 3500 Mean reward: 222.0000 Prob random: 0.9989, Loss: 0.4627\n",
      "Num episode: 3600 Mean reward: 213.6000 Prob random: 0.9988, Loss: nan\n",
      "Num episode: 3700 Mean reward: 201.1000 Prob random: 0.9988, Loss: nan\n",
      "Num episode: 3800 Mean reward: 202.8000 Prob random: 0.9987, Loss: nan\n",
      "Num episode: 3900 Mean reward: 198.8000 Prob random: 0.9987, Loss: nan\n",
      "Num episode: 4000 Mean reward: 207.7000 Prob random: 0.9987, Loss: nan\n",
      "Num episode: 4100 Mean reward: 218.4000 Prob random: 0.9986, Loss: nan\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-08e18ac0fe9e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;31m# Take the action and retrieve the next state, reward and done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m         \u001b[0mnext_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocess_observation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0;31m# Setup the episode to be stored in the episode buffer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-dbf8a55f6933>\u001b[0m in \u001b[0;36mpreprocess_observation\u001b[0;34m(obs)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mpreprocess_observation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m176\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# crop and downsize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# to greyscale\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mimg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0mmspacman_color\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;31m# Improve contrast\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0;36m3\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m128\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint8\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# normalize from -128 to 127\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/INSA/lib/python3.6/site-packages/numpy/core/_methods.py\u001b[0m in \u001b[0;36m_sum\u001b[0;34m(a, axis, dtype, out, keepdims, initial)\u001b[0m\n\u001b[1;32m     34\u001b[0m def _sum(a, axis=None, dtype=None, out=None, keepdims=False,\n\u001b[1;32m     35\u001b[0m          initial=_NoValue):\n\u001b[0;32m---> 36\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mumr_sum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m def _prod(a, axis=None, dtype=None, out=None, keepdims=False,\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Reset everything\n",
    "K.clear_session()\n",
    "\n",
    "# Setup our Q-networks\n",
    "online_q_values = q_network()\n",
    "target_q_values = q_network()\n",
    "\n",
    "\n",
    "# Make the networks equal\n",
    "update_target_graph(online_q_values, target_q_values, 1)\n",
    "\n",
    "# Setup our experience replay\n",
    "experience_replay = ExperienceReplay()\n",
    "\n",
    "# We'll begin by acting complete randomly. As we gain experience and improve,\n",
    "# we will begin reducing the probability of acting randomly, and instead\n",
    "# take the actions that our Q network suggests\n",
    "prob_random = prob_random_start\n",
    "prob_random_drop = (prob_random_start - prob_random_end) / annealing_steps\n",
    "\n",
    "num_steps = [] # Tracks number of steps per episode\n",
    "rewards = [] # Tracks rewards per episode\n",
    "total_steps = 0 # Tracks cumulative steps taken in training\n",
    "\n",
    "print_every = 100 # How often to print status\n",
    "save_every = 5 # How often to save\n",
    "\n",
    "losses = [0] # Tracking training losses\n",
    "\n",
    "num_episode = 0\n",
    "\n",
    "while num_episode < num_episodes:\n",
    "    # Create an experience replay for the current episode\n",
    "    episode_buffer = ExperienceReplay()\n",
    "    \n",
    "     # Get the game state from the environment\n",
    "        \n",
    "        \n",
    "    state = env.reset()\n",
    "    for skip in range(skip_start): # skip the start of each game\n",
    "        state, reward, done, info = env.step(0)\n",
    "    state = preprocess_observation(state)\n",
    "    \n",
    "    done = False # Game is complete\n",
    "    sum_rewards = 0 # Running sum of rewards in episode\n",
    "    cur_step = 0 # Running sum of number of steps taken in episode\n",
    "    \n",
    "    \n",
    "    while not(done):\n",
    "        cur_step += 1\n",
    "        total_steps += 1\n",
    "\n",
    "        if np.random.rand() < prob_random or \\\n",
    "            num_episode < pre_train_episodes:\n",
    "                # Act randomly based on prob_random or if we\n",
    "                # have not accumulated enough pre_train episodes\n",
    "                action = np.random.randint(9)\n",
    "        else:\n",
    "            # Decide what action to take from the Q network\n",
    "            action = np.argmax(online_q_values.predict(np.array([state])))\n",
    "\n",
    "        # Take the action and retrieve the next state, reward and done\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        next_state = preprocess_observation(next_state)\n",
    "\n",
    "        # Setup the episode to be stored in the episode buffer\n",
    "        episode = np.array([[state],action,reward,[next_state],done])\n",
    "        episode = episode.reshape(1,-1)\n",
    "\n",
    "        # Store the experience in the episode buffer\n",
    "        episode_buffer.add(episode)\n",
    "\n",
    "        # Update the running rewards\n",
    "        sum_rewards += reward\n",
    "\n",
    "        # Update the state\n",
    "        state = next_state\n",
    "    if num_episode > pre_train_episodes:\n",
    "        # Training the network\n",
    "        if prob_random > prob_random_end:\n",
    "                # Drop the probability of a random action\n",
    "                prob_random -= prob_random_drop\n",
    "                \n",
    "        if num_episode % update_freq == 0:\n",
    "            for num_epoch in range(num_epochs):\n",
    "                # Train batch is [[state,action,reward,next_state,done],...]\n",
    "                train_batch = experience_replay.sample(batch_size)\n",
    "\n",
    "                # Separate the batch into its components\n",
    "                train_state, train_action, train_reward, \\\n",
    "                    train_next_state, train_done = train_batch.T\n",
    "\n",
    "                # Convert the action array into an array of ints so they can be used for indexing\n",
    "                train_action = train_action.astype(np.int)\n",
    "\n",
    "                # Stack the train_state and train_next_state for learning\n",
    "                train_state = np.vstack(train_state)\n",
    "                train_next_state = np.vstack(train_next_state)\n",
    "\n",
    "                # Our predictions (actions to take) from the main Q network\n",
    "                target_q = target_q_values.predict(train_state)\n",
    "\n",
    "                # The Q values from our target network from the next state\n",
    "                target_q_next_state = online_q_values.predict(train_next_state)\n",
    "                train_next_state_action = np.argmax(target_q_next_state,axis=1)\n",
    "                train_next_state_action = train_next_state_action.astype(np.int)\n",
    "\n",
    "                # Tells us whether game over or not\n",
    "                # We will multiply our rewards by this value\n",
    "                # to ensure we don't train on the last move\n",
    "                train_gameover = train_done == 0\n",
    "\n",
    "                # Q value of the next state based on action\n",
    "                train_next_state_values = target_q_next_state[range(batch_size), train_next_state_action]\n",
    "\n",
    "                # Reward from the action chosen in the train batch\n",
    "                actual_reward = train_reward + (y * train_next_state_values * train_gameover)\n",
    "                target_q[range(batch_size), train_action] = actual_reward\n",
    "\n",
    "                # Train the main model\n",
    "                loss = online_q_values.train_on_batch(train_state, target_q)\n",
    "                losses.append(loss)\n",
    "\n",
    "            # Update the target model with values from the main model\n",
    "            update_target_graph(online_q_values, target_q_values, tau)\n",
    "\n",
    "            #if (num_episode + 1) % save_every == 0:\n",
    "            #    # Save the model\n",
    "            #    online_q_values.model.save_weights(main_weights_file)\n",
    "            #    target_q_values.model.save_weights(target_weights_file)\n",
    "\n",
    "    \n",
    "    # Increment the episode\n",
    "    num_episode += 1\n",
    "    \n",
    "    experience_replay.add(episode_buffer.buffer)\n",
    "    num_steps.append(cur_step)\n",
    "    rewards.append(sum_rewards)\n",
    "    \n",
    "    if num_episode % print_every == 0:\n",
    "        # Print progress\n",
    "        mean_loss = np.mean(losses[-(print_every * num_epochs):])\n",
    "\n",
    "        print(\"Num episode: {} Mean reward: {:0.4f} Prob random: {:0.4f}, Loss: {:0.04f}\".format(\n",
    "            num_episode, np.mean(rewards[-print_every:]), prob_random, mean_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    if num_episode > pre_train_episodes:\n",
    "        # Training the network\n",
    "\n",
    "        if prob_random > prob_random_end:\n",
    "            # Drop the probability of a random action\n",
    "            prob_random -= prob_random_drop\n",
    "\n",
    "        if num_episode % update_freq == 0:\n",
    "            for num_epoch in range(num_epochs):\n",
    "                # Train batch is [[state,action,reward,next_state,done],...]\n",
    "                train_batch = experience_replay.sample(batch_size)\n",
    "\n",
    "                # Separate the batch into its components\n",
    "                train_state, train_action, train_reward, \\\n",
    "                    train_next_state, train_done = train_batch.T\n",
    "                    \n",
    "                # Convert the action array into an array of ints so they can be used for indexing\n",
    "                train_action = train_action.astype(np.int)\n",
    "\n",
    "                # Stack the train_state and train_next_state for learning\n",
    "                train_state = np.vstack(train_state)\n",
    "                train_next_state = np.vstack(train_next_state)\n",
    "\n",
    "                # Our predictions (actions to take) from the main Q network\n",
    "                target_q = target_qn.model.predict(train_state)\n",
    "                \n",
    "                # The Q values from our target network from the next state\n",
    "                target_q_next_state = main_qn.model.predict(train_next_state)\n",
    "                train_next_state_action = np.argmax(target_q_next_state,axis=1)\n",
    "                train_next_state_action = train_next_state_action.astype(np.int)\n",
    "                \n",
    "                # Tells us whether game over or not\n",
    "                # We will multiply our rewards by this value\n",
    "                # to ensure we don't train on the last move\n",
    "                train_gameover = train_done == 0\n",
    "\n",
    "                # Q value of the next state based on action\n",
    "                train_next_state_values = target_q_next_state[range(batch_size), train_next_state_action]\n",
    "\n",
    "                # Reward from the action chosen in the train batch\n",
    "                actual_reward = train_reward + (y * train_next_state_values * train_gameover)\n",
    "                target_q[range(batch_size), train_action] = actual_reward\n",
    "                \n",
    "                # Train the main model\n",
    "                loss = main_qn.model.train_on_batch(train_state, target_q)\n",
    "                losses.append(loss)\n",
    "                \n",
    "            # Update the target model with values from the main model\n",
    "            update_target_graph(main_qn.model, target_qn.model, tau)\n",
    "\n",
    "            if (num_episode + 1) % save_every == 0:\n",
    "                # Save the model\n",
    "                main_qn.model.save_weights(main_weights_file)\n",
    "                target_qn.model.save_weights(target_weights_file)\n",
    "    \n",
    "\n",
    "    # Increment the episode\n",
    "    num_episode += 1\n",
    "\n",
    "    experience_replay.add(episode_buffer.buffer)\n",
    "    num_steps.append(cur_step)\n",
    "    rewards.append(sum_rewards)\n",
    "        \n",
    "    if num_episode % print_every == 0:\n",
    "        # Print progress\n",
    "        mean_loss = np.mean(losses[-(print_every * num_epochs):])\n",
    "\n",
    "        print(\"Num episode: {} Mean reward: {:0.4f} Prob random: {:0.4f}, Loss: {:0.04f}\".format(\n",
    "            num_episode, np.mean(rewards[-print_every:]), prob_random, mean_loss))\n",
    "        if np.mean(rewards[-print_every:]) >= goal:\n",
    "            print(\"Training complete!\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now the main training loop!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    step = global_step.eval()\n",
    "    if step >= n_steps:\n",
    "        break\n",
    "    iteration += 1\n",
    "    print(\"\\rIteration {}\\tTraining step {}/{} ({:.1f})%\\tLoss {:5f}\\tMean Max-Q {:5f}   \".format(\n",
    "        iteration, step, n_steps, step * 100 / n_steps, loss_val, mean_max_q), end=\"\")\n",
    "    if done: # game over, start again\n",
    "        obs = env.reset()\n",
    "        for skip in range(skip_start): # skip the start of each game\n",
    "            obs, reward, done, info = env.step(0)\n",
    "        state = preprocess_observation(obs)\n",
    "\n",
    "    # Online DQN evaluates what to do\n",
    "    q_values = online_q_values.eval(feed_dict={X_state: [state]})\n",
    "    action = epsilon_greedy(q_values, step)\n",
    "\n",
    "    # Online DQN plays\n",
    "    obs, reward, done, info = env.step(action)\n",
    "    next_state = preprocess_observation(obs)\n",
    "\n",
    "    # Let's memorize what happened\n",
    "    replay_memory.append((state, action, reward, next_state, 1.0 - done))\n",
    "    state = next_state\n",
    "\n",
    "    # Compute statistics for tracking progress (not shown in the book)\n",
    "    total_max_q += q_values.max()\n",
    "    game_length += 1\n",
    "    if done:\n",
    "        mean_max_q = total_max_q / game_length\n",
    "        total_max_q = 0.0\n",
    "        game_length = 0\n",
    "\n",
    "    if iteration < training_start or iteration % training_interval != 0:\n",
    "        continue # only train after warmup period and at regular intervals\n",
    "\n",
    "    # Sample memories and use the target DQN to produce the target Q-Value\n",
    "    X_state_val, X_action_val, rewards, X_next_state_val, continues = (\n",
    "        sample_memories(batch_size))\n",
    "    next_q_values = target_q_values.eval(\n",
    "        feed_dict={X_state: X_next_state_val})\n",
    "    max_next_q_values = np.max(next_q_values, axis=1, keepdims=True)\n",
    "    y_val = rewards + continues * discount_rate * max_next_q_values\n",
    "\n",
    "    # Train the online DQN\n",
    "    _, loss_val = sess.run([training_op, loss], feed_dict={\n",
    "        X_state: X_state_val, X_action: X_action_val, y: y_val})\n",
    "\n",
    "    # Regularly copy the online DQN to the target DQN\n",
    "    if step % copy_steps == 0:\n",
    "        copy_online_to_target.run()\n",
    "\n",
    "    # And save regularly\n",
    "    if step % save_steps == 0:\n",
    "        saver.save(sess, checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can interrupt the cell above at any time to test your agent using the cell below. You can then run the cell above once again, it will load the last parameters saved and resume training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = []\n",
    "n_max_steps = 10000\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, checkpoint_path)\n",
    "\n",
    "    obs = env.reset()\n",
    "    for step in range(n_max_steps):\n",
    "        state = preprocess_observation(obs)\n",
    "\n",
    "        # Online DQN evaluates what to do\n",
    "        q_values = online_q_values.eval(feed_dict={X_state: [state]})\n",
    "        action = np.argmax(q_values)\n",
    "\n",
    "        # Online DQN plays\n",
    "        obs, reward, done, info = env.step(action)\n",
    "\n",
    "        img = env.render(mode=\"rgb_array\")\n",
    "        frames.append(img)\n",
    "\n",
    "        if done:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_animation(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
