{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<a href=\"http://www.insa-toulouse.fr/\" ><img src=\"http://www.math.univ-toulouse.fr/~besse/Wikistat/Images/logo-insa.jpg\" style=\"float:left; max-width: 120px; display: inline\" alt=\"INSA\"/></a> \n",
    "\n",
    "<a href=\"http://wikistat.fr/\" ><img src=\"http://www.math.univ-toulouse.fr/~besse/Wikistat/Images/wikistat.jpg\" style=\"max-width: 250px; display: inline\"  alt=\"Wikistat\"/></a>\n",
    "\n",
    "<a href=\"http://www.math.univ-toulouse.fr/\" ><img src=\"http://www.math.univ-toulouse.fr/~besse/Wikistat/Images/logo_imt.jpg\" style=\"float:right; max-width: 200px; display: inline\" alt=\"IMT\"/> </a>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Ateliers: Technologies des grosses data](https://github.com/wikistat/Ateliers-Big-Data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction à la librairie  [SparkML](https://spark.apache.org/docs/latest/ml-guide.html) (ou MLlib  DataFrame-based API) de  <a href=\"http://spark.apache.org/\"><img src=\"http://spark.apache.org/images/spark-logo-trademark.png\" style=\"max-width: 100px; display: inline\" alt=\"Spark\"/> </a> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depuis Spark 2.0 la librairie MLlib, qui maniupule éxclusivement des RDD est maintenant en maintenance. Elle peut toujours être utilisé, mais il n'y aura pas de nouveaux outils développé pour cette librairie.\n",
    "\n",
    "La principale librairie de machine learning pour spark est maintenant SparkML. SparkML est une librairie qui manipule exclusivement des DataFrame, plus facile d'utilisation que les RDD et qui permet l'utilistation d'autres services spark tel que Spark Datasources, SQL queries etc...\n",
    "Si MLlib est aujourd'hui plus complète, SparkML devrait posséder exactement les même fonctions que MLlib dans la version 2.3 de Spark (Aujourd'hui la version 2.2 est disponible). La librairie MLlib, qui manipule exclusivement des RDD n'existera plus dans Spark 3.0\n",
    "\n",
    "\n",
    "SparkML n'est pas le nom définitif que portera la librairie de machine learning de spark. C'est un nom temporaire créer pour déveloper un outil basé sur les DataFrame. MLlib sera le nom définitif de cette librairie."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Résumé**: Ce tutoriel continue l'initiation à [Spark](https://spark.apache.org/) à l'aide de commandes en Python en utilisant l'API  [`PySpark`](http://spark.apache.org/docs/latest/api/python/). Dans ce calepein Nous allons manier majoritairement des DataFrame telles que celles décrite dans le calepin précédent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Statistique élémentaire avec <a href=\"http://spark.apache.org/\"><img src=\"http://spark.apache.org/images/spark-logo-trademark.png\" style=\"max-width: 100px; display: inline\" alt=\"Spark\"/> </a> et  [SparkML](https://spark.apache.org/docs/latest/ml-guide.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La plupart des fonctions de statistique élémentaire de *MLlib*, décrites dans le calepin 2 de cette introduction à pyspark ne sont pas encore disponibles dans la librairie *SparkML*. Seule la fonction de correlation et le test d'hypothèse du χ² sont disponible. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation\n",
    "\n",
    "La fonction `pyspark.ml.stat.Correlation` permet de calculer la correlation entre chaque colonne d'une DataFrame. Les correlations disponible sont celles de *Pearson* et *Spearman*. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.stat import Correlation\n",
    "\n",
    "data = [(Vectors.sparse(4, [(0, 1.0), (3, -2.0)]),),\n",
    "        (Vectors.dense([4.0, 5.0, 0.0, 3.0]),),\n",
    "        (Vectors.dense([6.0, 7.0, 0.0, 8.0]),),\n",
    "        (Vectors.sparse(4, [(0, 9.0), (3, 1.0)]),)]\n",
    "\n",
    "\n",
    "df = spark.createDataFrame(data, [\"features\"])\n",
    "\n",
    "r1 = Correlation.corr(df, \"features\").head()\n",
    "print(\"Pearson correlation matrix:\\n\" + str(r1[0]))\n",
    "\n",
    "r2 = Correlation.corr(df, \"features\", \"spearman\").head()\n",
    "print(\"Spearman correlation matrix:\\n\" + str(r2[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Test d'Hypothèse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark.ml supporte actuellement le test du Chi-2 de Pearson. Ce test permet d'effectuer un test d'indépendance pour chaque *features* vis à vis du *label*. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.stat import ChiSquareTest\n",
    "\n",
    "data = [(0.0, Vectors.dense(0.5, 10.0)),\n",
    "        (0.0, Vectors.dense(1.5, 20.0)),\n",
    "        (1.0, Vectors.dense(1.5, 30.0)),\n",
    "        (0.0, Vectors.dense(3.5, 30.0)),\n",
    "        (0.0, Vectors.dense(3.5, 40.0)),\n",
    "        (1.0, Vectors.dense(3.5, 40.0))]\n",
    "df = spark.createDataFrame(data, [\"label\", \"features\"])\n",
    "\n",
    "r = ChiSquareTest.test(df, \"features\", \"label\").head()\n",
    "print(\"pValues: \" + str(r.pValues))\n",
    "print(\"degreesOfFreedom: \" + str(r.degreesOfFreedom))\n",
    "print(\"statistics: \" + str(r.statistics))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## ML Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La librairie *SparkML*, contrairement à *MLlib*, est basé sur la notion de **ML Pipeline**. \n",
    "\n",
    "Un **ML Pipeline** permet de combiner différentes étapes de traitement, allant du nettoyage des données jusqu'a l'étape d'apprentissage en un seul objet appelé *pipeline* ou *workflow*. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimator, Transformer, and Param\n",
    "\n",
    "La construction d'un **ML Pipeline** est effectué à partir de trois type d'objets décrits ci-dessous :\n",
    "\n",
    "\n",
    " * **Transformer**: C'est un algorithme qui permet de transformer un *DataFrame* en un autre *DataFrame*. Dans la pluspart des cas, le nouveau *DataFrame* est identique au premier avec une colonne supplémentaire. Exemple de **Transformer**: \n",
    "     * Un modèle d'apprentissage va prendre en entrée un DataFrame de variables et retourner un nouveau DataFrame avec les variables et une nouvelle colonne correspondant à la prédiction.\n",
    "     * Le Transformer *StringIndexer* va prendre en entrée un DataFrame possédant une colonne de texte et retourner un DataFrame avec la même colonne texte et une nouvelle colonne ou les textes sont remplacés par une valeur numérique.\n",
    " \n",
    "* **Estimator**: C'est un algorithme qui peut-être appliqué sur un DataFrame afin de produire un **Transformer**. Exemple d'**Estimator**:\n",
    "    * Un algorithme d'apprentissage est un **Estimator**. Une fois appliqué sur un DataFrame, il va produire un modèle d'apprentissage qui sera un **Transformer**, comme décrit précédemment.\n",
    "\n",
    " * **Parameter**: Chaque **Transformer** et **Estimators** partage une même API pour spécifier leurs paramètres."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exemple : Regression Logistique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "# DataFrame D'apprentissage\n",
    "training = spark.createDataFrame([\n",
    "    (1.0, Vectors.dense([0.0, 1.1, 0.1])),\n",
    "    (0.0, Vectors.dense([2.0, 1.0, -1.0])),\n",
    "    (0.0, Vectors.dense([2.0, 1.3, 1.0])),\n",
    "    (1.0, Vectors.dense([0.0, 1.2, -0.5]))], [\"label\", \"features\"])\n",
    "\n",
    "# Création d'un objet LogisticRegression en spécifiant ses paramètres. Cet objet est un Estimator\n",
    "lr = LogisticRegression(maxIter=10, regParam=0.01, featuresCol='features', labelCol='label', predictionCol='prediction', probabilityCol='probability')\n",
    "print(\"LogisticRegression parameters:\\n\" + lr.explainParams() + \"\\n\")\n",
    "\n",
    "# Appliquer l'Estimator sur le DataFrame d'apprentissage. \n",
    "# L'objet qui en résulte, le modèle d'apprentissage, est un Transformer.\n",
    "model = lr.fit(training)\n",
    "\n",
    "\n",
    "# DataFrame de Test\n",
    "test = spark.createDataFrame([\n",
    "    (1.0, Vectors.dense([-1.0, 1.5, 1.3])),\n",
    "    (0.0, Vectors.dense([3.0, 2.0, -0.1])),\n",
    "    (1.0, Vectors.dense([0.0, 2.2, -1.5]))], [\"label\", \"features\"])\n",
    "\n",
    "# Appliquer le Transformer sur le DataFrame de Test 'test'.\n",
    "prediction = model.transform(test)\n",
    "\n",
    "# Il en resulte un nouveauDataFrame 'prediction' qui correspond au DataFrame 'tesu' auquel \n",
    "# on été ajoutés une colonne \"prediction\" et une colonne \"probability\".\n",
    "# Les noms de ces nouvelles colonnes ont été spécifiés dans les paramètres lors de l'instanciation \n",
    "# de l'Estimator 'lr' puis transmis au Transformer 'model'.\n",
    "\n",
    "# Affichage des résultats.\n",
    "result = prediction.select(\"features\", \"label\", \"probability\", \"prediction\") \\\n",
    "    .collect()\n",
    "\n",
    "for row in result:\n",
    "    print(\"features=%s, label=%s -> prob=%s, prediction=%s\"\n",
    "          % (row.features, row.label, row.probability, row.prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un **Pipeline** est un enchainement de plusieurs **Transformers** et **Estimators** afin de spécifier un processus entier de Machine Learning. Par exemple, pour effectuer de l'apprentissage statistique sur des données textuelles, ces différentes étapes sont appliquées les unes à la suite des autres: \n",
    "\n",
    " * Découpage du texte en liste de mot\n",
    " * Conversion en variable numérique\n",
    " * Aprentissahe sur les données numérique\n",
    " * Prédiction \n",
    "\n",
    "Toutes ces étapes peuvent être résumé dans un seul objet appelé **Pipeline**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exemple : Tokenize, Hash et Regression logistique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.feature import HashingTF, Tokenizer\n",
    "\n",
    "# DataFrame d'Apprentissage\n",
    "training = spark.createDataFrame([\n",
    "    (0, \"a b c d e spark\", 1.0),\n",
    "    (1, \"b d\", 0.0),\n",
    "    (2, \"spark f g h\", 1.0),\n",
    "    (3, \"hadoop mapreduce\", 0.0)\n",
    "], [\"id\", \"text\", \"label\"])\n",
    "\n",
    "# Configure un pipeline qui consiste en 3 étapes :\n",
    "\n",
    "# 1/ Tokenizer. Spécifier la colonne d'entrée 'text' et la colonne de sortie 'words'.\n",
    "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
    "# 2/ Hash. La colonne d'entrée est spécifiée ici comme étant la colonne de sortie de l'étape précédente.\n",
    "hashingTF = HashingTF(inputCol=tokenizer.getOutputCol(), outputCol=\"features\")\n",
    "# 3/ Regression Logistique\n",
    "lr = LogisticRegression(maxIter=10, regParam=0.001)\n",
    "\n",
    "# Configuration du Pipeline comme la succession des étapes précédentes.\n",
    "pipeline = Pipeline(stages=[tokenizer, hashingTF, lr])\n",
    "\n",
    "# Appliquer toutes les étapes sur le DataFrame d'apprentissage.\n",
    "model = pipeline.fit(training)\n",
    "\n",
    "# DataFrame Test.\n",
    "test = spark.createDataFrame([\n",
    "    (4, \"spark i j k\"),\n",
    "    (5, \"l m n\"),\n",
    "    (6, \"spark hadoop spark\"),\n",
    "    (7, \"apache hadoop\")\n",
    "], [\"id\", \"text\"])\n",
    "\n",
    "# Prediction.\n",
    "prediction = model.transform(test)\n",
    "selected = prediction.select(\"id\", \"text\", \"probability\", \"prediction\")\n",
    "for row in selected.collect():\n",
    "    rid, text, prob, prediction = row\n",
    "    print(\"(%d, %s) --> prob=%s, prediction=%f\" % (rid, text, str(prob), prediction))"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  },
  "toc": {
   "nav_menu": {
    "height": "457px",
    "width": "252px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
