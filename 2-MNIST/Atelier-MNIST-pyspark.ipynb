{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<a href=\"http://www.insa-toulouse.fr/\" ><img src=\"http://www.math.univ-toulouse.fr/~besse/Wikistat/Images/logo-insa.jpg\" style=\"float:left; max-width: 120px; display: inline\" alt=\"INSA\"/></a> \n",
    "\n",
    "<a href=\"http://wikistat.fr/\" ><img src=\"http://www.math.univ-toulouse.fr/~besse/Wikistat/Images/wikistat.jpg\" style=\"max-width: 250px; display: inline\"  alt=\"Wikistat\"/></a>\n",
    "\n",
    "\n",
    "<a href=\"http://www.hupi.fr/\" ><img src=\"http://www.hupi.fr/wp-content/uploads/2016/03/hupi_logo_vectoris_menu.png\" style=\"float:right; max-width: 300px; display: inline\"  alt=\"Wikistat\"/></a>\n",
    "\n",
    " </a>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Ateliers: Technologies des grosses data](https://github.com/wikistat/Ateliers-Big-Data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Reconnaissance de caractères manuscrits](https://github.com/wikistat/Ateliers-Big-Data/2-MNIST) ([MNIST](http://yann.lecun.com/exdb/mnist/)) avec <a href=\"http://spark.apache.org/\"><img src=\"http://spark.apache.org/images/spark-logo-trademark.png\" style=\"max-width: 100px; display: inline\" alt=\"Spark\"/> </a> et [SparkML](https://spark.apache.org/docs/latest/ml-statistics.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Même traitement en utilisant la librairie SparkML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Importation des packages\n",
    "import time\n",
    "from numpy import array\n",
    "# Répertoire courant ou répertoire accessible de tous les \"workers\" du cluster\n",
    "DATA_PATH=\"\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gestion des données"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importation et transformation des données au format RDD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les données sont déjà partagée en une partie apprentissage et une test utilisée pour les comparaisons entre méthodes dans les publications. Ce sont bien les données du site MNIST mais transformée au format .csv pour en faciliter la lecture. \n",
    "\n",
    "Elles doivent être stockées à un emplacement accessibles de tous les noeuds du cluster pour permettre la construction de la base de données réparties (RDD). \n",
    "\n",
    "Dans une utilisation monoposte (*standalone*) de *Spark*, elles sont simplement chargées dans le répertoire courant. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Chargement des fichiers\n",
    "import urllib.request\n",
    "f = urllib.request.urlretrieve(\"https://www.math.univ-toulouse.fr/~besse/Wikistat/data/mnist_train.csv\",DATA_PATH+\"mnist_train.csv\")\n",
    "f = urllib.request.urlretrieve(\"https://www.math.univ-toulouse.fr/~besse/Wikistat/data/mnist_test.csv\",DATA_PATH+\"mnist_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Transformation du fichier texte en RDD de valeurs\n",
    "## Données d'apprentissage\n",
    "# Transformation ou étape map de séparation des champs\n",
    "trainRDD = sc.textFile(DATA_PATH+\"mnist_train.csv\").map(lambda l: [float(x) for x in l.split(',')])\n",
    "# Action\n",
    "trainRDD.count() # taille de l'échantillon\n",
    "#test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conversion des données au format DataFrame\n",
    "\n",
    "Pour pouvoir être intérprété par les différentes méthodes de classification de la librairie SparkML, les données doivent être converties en objet DataFrame.\n",
    "\n",
    "Pour plus d'information sur l'utilisation de ces DataFrames, reportez vous aux calepins 1-Intro-PySpark/Cal3-PySpark-SQL.ipynb et 1-Intro-PySpark/Cal4-PySpark-Statelem&Pipeline-SparkML.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Transformation du de la RDD en DataFrame\n",
    "\n",
    "from pyspark.sql import Row\n",
    "from pyspark.ml.linalg import Vectors\n",
    "\n",
    "#Cette fonction va permettre de transformer chaque ligne de la RDD en une \"Row\" pyspark.sql. \n",
    "\n",
    "def list_to_Row(l):    \n",
    "    #Creation d'un vecteur sparse pour les features\n",
    "    features = Vectors.sparse(784,dict([(i,v) for i,v in enumerate(l[:-1]) if v!=0]))\n",
    "    row = Row(label = l[-1], features= features)\n",
    "    return row\n",
    "\n",
    "trainDF = trainRDD.map(list_to_Row).toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Exemple de ligne\n",
    "trainDF.take(1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Même chose pour les données de test\n",
    "testRDD = sc.textFile(DATA_PATH+'mnist_test.csv').map(lambda l: [float(x) for x in l.split(',')])\n",
    "testRDD.count() # taille de l'échantillon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "testDF = testRDD.map(list_to_Row).toDF()\n",
    "testDF.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sous-échantillon d'apprentissage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Extraction d'un sous-échantillon d'apprentissage pour tester les programmes sur des données plus petites. Itérer cette démarche permet d'étudier l'évolution de l'erreur de prévision en fonction de la taille de l'échantillon d'apprentissage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tauxEch=0.1 # tester pour des tailles croissantes d'échantillon d'apprentissage\n",
    "(trainData, DropDatal) = trainRDD.randomSplit([tauxEch, 1-tauxEch])\n",
    "trainData.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Méthode de classification\n",
    "\n",
    "Les méthodes de classifications de la librairie SparkML suivent le même shéma d'utilisation. \n",
    "\n",
    "Il faut dans un premier temps crée un objets **Estimators** pour configurer les paramètres de la méthode.\n",
    "Dans un second temps on réalise l'apprentissage en appliquant la fonction **fit** de l'Estimators sur la DataFrame d'apprentissage. Cette commande créé un objet différent, le **Transformers** qui permettra de réaliser les prédictions. \n",
    "\n",
    "Par défaut les différentes méthodes considère que les noms des colonnes correspondants aux variables et au prédicants du jeux d'apprentissage sont respectivement \"features\" et \"label\". Tandis que les prédictions seront automatiquement assigné à une colonne de nom \"prediction\". \n",
    "Il est conseillé de garder cette terminiologie, mais ces attributs par défaut peuvent être modifié en spécifiant les paramètres  *featuresCol*, *labelCol* et *predictionCol* de chaque méthode."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Régression logistique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exemple d'utilisation pour expliciter la syntaxe mais sans grand intérêt pour ces données qui ne satisfont pas à des frontières de discrimination linéaires. L'algorithme permettant de réaliser une regression logistique multinomial est l'algorithme [*softmax*](https://spark.apache.org/docs/latest/ml-classification-regression.html#multinomial-logistic-regression)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Logistic Regression\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "### Configuraiton des paramètres de la méthode\n",
    "time_start=time.time()\n",
    "lr = LogisticRegression(maxIter=100, regParam=0.01, fitIntercept=False, tol=0.0001,\n",
    "            family = \"multinomial\", elasticNetParam=0.0) #0 for L2 penalty, 1 for L1 penalty\n",
    "\n",
    "### Génération du modèle\n",
    "model_lr = lr.fit(trainDF)\n",
    " \n",
    "time_end=time.time()\n",
    "time_lrm=(time_end - time_start)\n",
    "print(\"LR prend %d s\" %(time_lrm)) # (104s avec taux=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Erreur sur l'échantillon test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictionsRDD = model_lr.transform(testDF)\n",
    "labelsAndPredictions = predictionsRDD.select(\"label\",\"prediction\").collect()\n",
    "nb_good_prediction = sum([r[0]==r[1] for r in labelsAndPredictions])\n",
    "nb_test = testDF.count()\n",
    "testErr = 1-nb_good_prediction/nb_test\n",
    "print('Test Error = ' + str(testErr)) # (0.08 avec taux =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#LogisticRegressionTrainingSummary provides a summary for a LogisticRegressionModel. Currently, only binary classification is supported. Support for multiclass model summaries will be added in the future."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Arbre binaire de décision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Même chose pour un arbre de discrimination. Comme pour l'implémentation de scikit-learn, les arbres ne peuvent être optimisés par un élagage basé sur une pénalisation de la complexité. Ce paramètre n'est pas présent, seule la profondeur max ou le nombre minimal d'observations par feuille peut contrôler la complexité. Noter l'apparition d'un nouveau paramètre: *maxBins* qui, schématiquement, rend qualitative ordinale à maxBins classes toute variable quantitative.  D'autre part, il n'y a pas de représentation graphique. Cette implémentation d'arbre est issue d'un [projet Google](http://static.googleusercontent.com/media/research.google.com/fr//pubs/archive/36296.pdf) pour adapter cet algorithme aux contraintes *mapreduce* de données sous Hadoop. Elle vaut surtout pour permettre de construire une implémentation des forêts aléatoires."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Decision Tree\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "\n",
    "### Configuraiton des paramètres de la méthode\n",
    "time_start=time.time()\n",
    "dt = DecisionTreeClassifier(impurity='gini',maxDepth=5,maxBins=32, minInstancesPerNode=1,\n",
    "                            minInfoGain=0.0)\n",
    "\n",
    "### Génération du modèle\n",
    "model_dt = dt.fit(trainDF)\n",
    "\n",
    "time_end=time.time()\n",
    "time_dt=(time_end - time_start)\n",
    "print(\"DT takes %d s\" %(time_dt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Erreur sur l'échantillon test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictionsRDD = model_dt.transform(testDF)\n",
    "labelsAndPredictions = predictionsRDD.select(\"label\",\"prediction\").collect()\n",
    "nb_good_prediction = sum([r[0]==r[1] for r in labelsAndPredictions])\n",
    "nb_test = testDF.count()\n",
    "testErr = 1-nb_good_prediction/nb_test\n",
    "print('Test Error = ' + str(testErr)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les $k$-nn ne sont pas \"scalables\" et donc pas présents. Voici la syntaxe et les paramètres associés à l'algorithme des forêts aléatoires. Parmi ceux \"classiques\" se trouvent *numTrees*, *featureSubsetStrategy*, *impurity*, *maxdepth* et en plus *maxbins* comme pour les arbres. Les valeurs du paramètres *maxDepth* est critique pour la qualité de la prévision. en principe, il n'est pas contraint, un arbre peut se déployer sans \"limite\" mais face à des données massives cela peut provoquer des plantages intempestifs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Random Forest\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "\n",
    "### Configuraiton des paramètres de la méthode\n",
    "time_start=time.time()\n",
    "rf = RandomForestClassifier(numTrees = 2, impurity='gini', maxDepth=12,\n",
    "                            maxBins=32, seed=None)\n",
    "\n",
    "### Génération du modèle\n",
    "model_rf = rf.fit(trainDF)\n",
    "\n",
    "time_end=time.time()\n",
    "time_rf=(time_end - time_start)\n",
    "print(\"RF takes %d s\" %(time_rf))#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Erreur sur l'échantillon test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Erreur sur l'échantillon test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictionsRDD = model_rf.transform(testDF)\n",
    "labelsAndPredictions = predictionsRDD.select(\"label\",\"prediction\").collect()\n",
    "nb_good_prediction = sum([r[0]==r[1] for r in labelsAndPredictions])\n",
    "nb_test = testDF.count()\n",
    "testErr = 1-nb_good_prediction/nb_test\n",
    "print('Test Error = ' + str(testErr)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Même traitement sur la totalité de l'échantillon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Quelques résultats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "100 arbres, sélection automatique, maxDepth=9\n",
    "\n",
    "maxBins | Temps |  Erreur \n",
    "--------|-------|---------\n",
    "32 | 259 |  0.067 \n",
    "64 | 264 |  0.068 \n",
    "128 | 490 | 0.065\n",
    "\n",
    "100 arbres, sélection automatique, maxBins=32\n",
    "\n",
    "maxDepth | Temps | Erreur\n",
    "---------|-------|-------\n",
    "4 | 55 | 0.21\n",
    "9 | 259 |  0.067\n",
    "18 | 983 | **0.035**\n",
    "\n",
    "Le nombre de variables tirées à chaque noeud n'a pas été optimisé. \n",
    "\n",
    "Le paramètre maxBins ne semble pas trop influencer la précision du modèle, au contriare de la profondeur maximum des arbres. Avec une profondeur suffisante, on retrouve (presque) les résultats classiques des forêts aléatoires sur ces données.\n",
    "\n",
    "COmparer les résultats obtenus pour les trois environnements. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  },
  "toc": {
   "nav_menu": {
    "height": "226px",
    "width": "252px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
