{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Ateliers: Technologies de l'intelligence Artificielle](https://github.com/wikistat/AI-Frameworks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<a href=\"http://www.insa-toulouse.fr/\" ><img src=\"http://www.math.univ-toulouse.fr/~besse/Wikistat/Images/logo-insa.jpg\" style=\"float:left; max-width: 120px; display: inline\" alt=\"INSA\"/></a> \n",
    "<a href=\"http://wikistat.fr/\" ><img src=\"http://www.math.univ-toulouse.fr/~besse/Wikistat/Images/wikistat.jpg\" width=400, style=\"max-width: 150px; display: inline\"  alt=\"Wikistat\"/></a>\n",
    "<a href=\"http://www.math.univ-toulouse.fr/\" ><img src=\"http://www.math.univ-toulouse.fr/~besse/Wikistat/Images/logo_imt.jpg\" width=400,  style=\"float:right;  display: inline\" alt=\"IMT\"/> </a>\n",
    "    \n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Traitement Naturel du Langage (NLP) : Catégorisation de Produits Cdiscount\n",
    "\n",
    "Il s'agit d'une version simplifiée du concours proposé par Cdiscount et paru sur le site [datascience.net](https://www.datascience.net/fr/challenge). Les données d'apprentissage sont accessibles sur demande auprès de Cdiscount mais les solutions de l'échantillon test du concours ne sont pas et ne seront pas rendues publiques. Un échantillon test est donc construit pour l'usage de ce tutoriel.  L'objectif est de prévoir la catégorie d'un produit à partir de son descriptif (*text mining*). Seule la catégorie principale (1er niveau, 47 classes) est prédite au lieu des trois niveaux demandés dans le concours. L'objectif est plutôt de comparer les performances des méthodes et technologies en fonction de la taille de la base d'apprentissage ainsi que d'illustrer sur un exemple complexe le prétraitement de données textuelles. \n",
    "\n",
    "Le jeux de données complet (15M produits) permet un test en vrai grandeur du **passage à l'échelle volume** des phases de préparation (*munging*), vectorisation (hashage, TF-IDF) et d'apprentissage en fonction de la technologie utilisée.\n",
    "\n",
    "La synthèse des résultats obtenus est développée par [Besse et al. 2016](https://hal.archives-ouvertes.fr/hal-01350099) (section 5)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partie 1-1 : Exploration et Nettoyage de données textuelles\n",
    "\n",
    "Dans ce premier notebook nous verrons différent traitements généralement opérés sur des données textuelles :\n",
    "\n",
    "* **Nettoyage** : Suppression des caractères mal codés et de ponctuation, transformation des majuscules en minuscules, en remarquant que ces transformations ne seraient pas pertinentes pour un objectif de détection de pourriels.\n",
    "* **StopWord** : Suppression des mots inutiles ou mots de liaison, articles qui n'ont a priori pas de pouvoir discriminant.\n",
    "* **Stemming** (ou Racinisation): Les mots sont réduits à leur seule racine afin de réduire la taille du dictionnaire."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Librairies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "#Importation des librairies utilisées\n",
    "import unicodedata \n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import nltk\n",
    "import re \n",
    "import collections\n",
    "import itertools\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb\n",
    "sb.set_style(\"whitegrid\")\n",
    "\n",
    "import sklearn.cross_validation as scv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**nltk**\n",
    "\n",
    "Si vous utilisez la librairie `nltk` pour la première fois, il est nécessaire d'utiliser la commande suivante. Cette commande permet de télécharger de nombreux corpus de texte, mais également des informations grammaticales sur différentes langues. Information notamment nécessaire à l'étape de racinisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# nltk.download(\"all\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Les données\n",
    "\n",
    "Dans le dossier *Cdiscount/data* de ce répértoire vous trouverez les fichiers suivants :\n",
    "\n",
    "* `cdiscount_test.csv.zip`: Fichier d'apprentissage constitué de 1.000.000 de lignes\n",
    "* `cdisount_test`: Fichier test constitué de 50.000 lignes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   ### Read & Split Dataset\n",
    "   \n",
    "   On définit une fonction permettant de lire le fichier d'apprentissage et de créer deux DataFrame Pandas, un pour l'apprentissage, l'autre pour la validation.\n",
    "   La fonction créée un DataFrame en lisant entièrement le fichier. Puis elle scinde ce DataFrame en deux grâce à la fonction dédiée de sklearn. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split_dataset(input_path, nb_line, tauxValid):\n",
    "    data_all = pd.read_csv(input_path,sep=\",\", nrows=nb_line)\n",
    "    data_all = data_all.fillna(\"\")\n",
    "    data_train, data_valid = scv.train_test_split(data_all, test_size = tauxValid)\n",
    "    time_end = time.time()\n",
    "    return data_train, data_valid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bien que déjà réduit par rapport au fichier original du concours, contenant plus de 15M de lignes, le fichier cdiscount_test.csv.zip, contenant 1M de lignes est encore volumineux. \n",
    "Nous allons charger en mémoire qu'une partie de ce fichier grace à l'argument `nb_line` afin d'éviter des temps de calcul trop couteux. \n",
    "Nous allons extraire 5% de ces 1M de lignes commes échantillons de validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = \"data/cdiscount_train.csv.zip\"\n",
    "nb_line=100000  # part totale extraite du fichier initial ici déjà réduit\n",
    "tauxValid = 0.05\n",
    "data_train, data_valid = split_dataset(input_path, nb_line, tauxValid)\n",
    "# Cette ligne permet de visualiser les 5 premières lignes de la DataFrame \n",
    "N_train = data_train.shape[0]\n",
    "N_valid = data_valid.shape[0]\n",
    "print(\"Train set : %d elements, Validation set : %d elements\" %(N_train, N_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La commande suivante permet d'afficher les premières lignes du fichiers. \n",
    "\n",
    "Vous pouvez observer que chaque produit possède 3 niveaux de Catégories, qui correspondent au différents niveaux de l'arborescence que vous retrouverez sur le site.\n",
    "Il y a 44 catégories de niveau 1, 428 de niveau 2 et 3170 de niveau 3. \n",
    "\n",
    "Dans ce TP, nous nous interesserons uniquement à classer les produits dans la catégorie de niveau 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La commande suivante permet d'afficher un exemple de produits pour chaque Catégorie de niveau 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train.groupby(\"Categorie1\").first()[[\"Description\",\"Libelle\",\"Marque\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribution des classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Count occurence of each Categorie\n",
    "data_count = data_train[\"Categorie1\"].value_counts()\n",
    "#Rename index to add percentage\n",
    "new_index = [k+ \": %.2f%%\" %(v*100/N_train) for k,v in data_count.iteritems()]\n",
    "data_count.index = new_index\n",
    "\n",
    "fig=plt.figure(figsize= (10,10))\n",
    "ax = fig.add_subplot(1,1,1)\n",
    "data_count.plot.barh(logx = False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q** Que peut-on dire sur la distribution de ces classes?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sauvegarde des données\n",
    "\n",
    "On sauvegarde dans des csv les fichiers `train` et `validation` afin que ces mêmes fichiers soit ré-utilisés plus tard dans d'autre calepin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_valid.to_csv(\"data/cdiscount_valid.csv\", index=False)\n",
    "data_train.to_csv(\"data/cdiscount_train_subset.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Nettoyage des données\n",
    "\n",
    "Afin de limiter la dimension de l'espace des variables ou *features* (i.e les mots présents dans le document), tout en conservant les informations essentielles, il est nécessaire de nettoyer les données en appliquant plusieurs étapes:\n",
    "\n",
    "* Chaque mot est écrit en minuscule.\n",
    "* Les termes numériques, de ponctuation et autres symboles sont supprimés.\n",
    "* 155 mots-courants, et donc non informatifs, de la langue française sont supprimés (STOPWORDS). Ex: le, la, du, alors, etc...\n",
    "* Chaque mot est \"racinisé\", via la fonction `STEMMER.stem` de la librairie nltk. La racinisation transforme un mot en son radical ou sa racine. Par exemple, les mots: cheval, chevaux, chevalier, chevalerie, chevaucher sont tous remplacés par \"cheva\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exemple \n",
    "\n",
    "Observons dans un premier temps l'effet de ces différentes étapes sur un  exemple. \n",
    "\n",
    "**Ligne Originale**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "description = data_train.Description.values[i]\n",
    "print(\"Original Description : \" + description)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Suppression des posibles balises HTML dans la description**\n",
    "\n",
    "Les descriptions produits étant parfois extraites d'autres sites commerçant, des balises HTML peuvent être incluts dans la description. \n",
    "La librairie 'BeautifulSoup' permet de supprimer ces balises\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup #Nettoyage d'HTML\n",
    "txt = BeautifulSoup(description,\"html.parser\",from_encoding='utf-8').get_text()\n",
    "print(txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conversion du texte en minuscule**\n",
    "\n",
    "Certaines mots peuvent être écrits en majuscule dans les descriptions textes, cela à pour conséquence de dupliquer le nombre de features et une perte d'information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = txt.lower()\n",
    "print(txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remplacement de caractères spéciaux**\n",
    "\n",
    "Certains caractères spéciaux sont supprimés comme par exemple :\n",
    "\n",
    "* `\\u2026`: `…`\n",
    "* `\\u00a0`: `NO-BREAK SPACE`\n",
    "\n",
    "Cette liste est non exhaustive et peut être etayée en fonction du jeu de donées étudié, de l'objectif souhaité ou encore du résultat de l'étude explorative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = txt.replace(u'\\u2026','.')    \n",
    "txt = txt.replace(u'\\u00a0',' ')\n",
    "print(txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Suppression des accents**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = unicodedata.normalize('NFD', txt).encode('ascii', 'ignore').decode(\"utf-8\")\n",
    "print(txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Supprime les caractères qui ne sont ne sont pas des lettres minuscules**\n",
    "\n",
    "Une fois ces premières étapes passées, on supprime tous les caractères qui sont pas des lettres minusculres, c'est à dire les signes de ponctuation, les caractères numériques etc..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = re.sub('[^a-z_]', ' ', txt)\n",
    "print(txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remplace la description par une liste de mots (tokens), supprime les mots de moins de 2 lettres ainsi que les stopwords**\n",
    "\n",
    "On va supprimer maintenant tous les mots considérés comme \"non-informatif\". Par exemple : \"le\", \"la\", \"de\" ...\n",
    "Des listes contenants ces mots sont proposés dans des libraires tels que *nltk* ou encore *lucène*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## listes de mots à supprimer dans la description des produits\n",
    "## Depuis NLTK\n",
    "nltk_stopwords = nltk.corpus.stopwords.words('french') \n",
    "## Depuis Un fichier externe.\n",
    "lucene_stopwords =open(\"data/lucene_stopwords.txt\",\"r\").read().split(\",\") #En local\n",
    "## Union des deux fichiers de stopwords \n",
    "stopwords = list(set(nltk_stopwords).union(set(lucene_stopwords)))\n",
    "\n",
    "stopwords[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On applique également la suppression des accents à cette liste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = [unicodedata.normalize('NFD', sw).encode('ascii', 'ignore').decode(\"utf-8\") for sw in stopwords]\n",
    "stopwords[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enfin on crée des *tokens*, liste de mots dans la description produit, en supprimant les éléments de notre description produit qui sont présent dans la liste de stopword."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = [w for w in txt.split() if (len(w)>2) and (w not in stopwords)]\n",
    "remove_words = [w for w in txt.split() if (len(w)<2) or (w in stopwords)]\n",
    "\n",
    "print(tokens)\n",
    "print(remove_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Racinisation (Stem) chaque tokens**\n",
    "\n",
    "Pour chaque mot de notre liste de token, on va ramener ce mot à sa racine au sens de l'algorithme de Snowball présent dans la librairie **nltk**. \n",
    "\n",
    "Cette liste de mots néttoyé et racinisé va constitué les *features* de cette description produits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Fonction de setmming de stemming permettant la racinisation\n",
    "stemmer=nltk.stem.SnowballStemmer('french')\n",
    "tokens_stem = [stemmer.stem(token) for token in tokens]\n",
    "print(tokens_stem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fonction de nettoyage de texte\n",
    "\n",
    "On définit une fonction `clean-txt` qui prend en entrée un texte de description produit et qui retourne le texte nettoyé en appliquant successivement les étapes présentés précedemment. \n",
    "\n",
    "On définit également une fonction `clean_marque` qui contient signifcativement moins d'étape de nettoyage. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Fonction clean générale\n",
    "def clean_txt(txt):\n",
    "    ### remove html stuff\n",
    "    txt = BeautifulSoup(txt,\"html.parser\",from_encoding='utf-8').get_text()\n",
    "    ### lower case\n",
    "    txt = txt.lower()\n",
    "    ### special escaping character '...'\n",
    "    txt = txt.replace(u'\\u2026','.')\n",
    "    txt = txt.replace(u'\\u00a0',' ')\n",
    "    ### remove accent btw\n",
    "    txt = unicodedata.normalize('NFD', txt).encode('ascii', 'ignore').decode(\"utf-8\")\n",
    "    ###txt = unidecode(txt)\n",
    "    ### remove non alphanumeric char\n",
    "    txt = re.sub('[^a-z_]', ' ', txt)\n",
    "    ### remove french stop words\n",
    "    tokens = [w for w in txt.split() if (len(w)>2) and (w not in stopwords)]\n",
    "    ### french stemming\n",
    "    tokens_stem = [stemmer.stem(token) for token in tokens]\n",
    "    ### tokens = stemmer.stemWords(tokens)\n",
    "    return ' '.join(tokens), \" \".join(tokens_stem)\n",
    "\n",
    "def clean_marque(txt):\n",
    "    txt = re.sub('[^a-zA-Z0-9]', '_', txt).lower()\n",
    "    return txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applique le nettoyage sur toutes les lignes de la DataFrame et créé deux nouvelles Dataframe (avant et sans l'étape de racinisation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# fonction de nettoyage du fichier(stemming et liste de mots à supprimer)\n",
    "def clean_df(input_data, column_names= ['Description', 'Libelle', 'Marque']):\n",
    "\n",
    "    nb_line = input_data.shape[0]\n",
    "    print(\"Start Clean %d lines\" %nb_line)\n",
    "    \n",
    "    # Cleaning start for each columns\n",
    "    time_start = time.time()\n",
    "    clean_list=[]\n",
    "    clean_stem_list=[]\n",
    "    for column_name in column_names:\n",
    "        column = input_data[column_name].values\n",
    "        if column_name == \"Marque\":\n",
    "            array_clean = np.array(list(map(clean_marque,column)))\n",
    "            clean_list.append(array_clean)\n",
    "            clean_stem_list.append(array_clean)\n",
    "        else:\n",
    "            A = np.array(list(map(clean_txt,column)))\n",
    "            array_clean = A[:,0]\n",
    "            array_clean_stem = A[:,1]\n",
    "            clean_list.append(array_clean)\n",
    "            clean_stem_list.append(array_clean_stem)\n",
    "    time_end = time.time()\n",
    "    print(\"Cleaning time: %d secondes\"%(time_end-time_start))\n",
    "    \n",
    "    #Convert list to DataFrame\n",
    "    array_clean = np.array(clean_list).T\n",
    "    data_clean = pd.DataFrame(array_clean, columns = column_names)\n",
    "    \n",
    "    array_clean_stem = np.array(clean_stem_list).T\n",
    "    data_clean_stem = pd.DataFrame(array_clean_stem, columns = column_names)\n",
    "    return data_clean, data_clean_stem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nettoyage des DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take approximately 2 minutes fors 100.000 rows\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "data_valid_clean, data_valid_clean_stem = clean_df(data_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")\n",
    "data_train_clean, data_train_clean_stem = clean_df(data_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Affiche les 5 premières lignes de la DataFrame d'apprentissage après nettoyage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train_clean.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train_clean_stem.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taille du dictionnaire de mots pour le dataset avant et après la racinisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concatenate_text = \" \".join(data_train[\"Description\"].values)\n",
    "list_of_word = concatenate_text.split(\" \")\n",
    "N = len(set(list_of_word))\n",
    "print(N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concatenate_text = \" \".join(data_train_clean[\"Description\"].values)\n",
    "list_of_word = concatenate_text.split(\" \")\n",
    "N = len(set(list_of_word))\n",
    "print(N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concatenate_text = \" \".join(data_train_clean_stem[\"Description\"].values)\n",
    "list_of_word_stem = concatenate_text.split(\" \")\n",
    "N = len(set(list_of_word_stem))\n",
    "print(N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wordcloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les représentations *Wordcloud* permettent des représentations de l'ensemble des mots d'un corpus de documents. Dans cette représentation plus un mot apparait de manière fréquent dans le corpus, plus sa taille sera grande dans la représentation du corpus.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A=WordCloud(background_color=\"black\")\n",
    "A.generate_from_text?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wordcloud de l'ensemble des description à l'état brut."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_descr = \" \".join(data_valid.Description.values)\n",
    "wordcloud_word = WordCloud(background_color=\"black\", collocations=False).generate_from_text(all_descr)\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.imshow(wordcloud_word,cmap=plt.cm.Paired)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wordcloud après racinisation et nettoyage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_descr_clean_stem = \" \".join(data_valid_clean_stem.Description.values)\n",
    "wordcloud_word = WordCloud(background_color=\"black\", collocations=False).generate_from_text(all_descr_clean_stem)\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.imshow(wordcloud_word,cmap=plt.cm.Paired)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vous pouvez observer que les mots \"voir et \"present\" sont les plus représentés. Cela est du au fait que la pluspart des descriptions se terminent par \"Voir la présentation\". C'est deux mots ne sont donc pas informatif car présent dans beaucoup de catégorie différente. C'est une bon exemple de *stopword* propre à un problème spécifique.\n",
    "\n",
    "**Exercice** Ajouter les mots `voir`et `présentation`à la liste des stopwords plus hauts et refaites tourner le nettoyage.\n",
    "\n",
    "**Exercice** Générer les wordcloud par catégorie pour 3 catégories de votre choix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sauvegarde des jeux de données nettoyés dans des fichiers csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_valid_clean.to_csv(\"data/cdiscount_valid_clean.csv\", index=False)\n",
    "data_train_clean.to_csv(\"data/cdiscount_train_clean.csv\", index=False)\n",
    "\n",
    "data_valid_clean_stem.to_csv(\"data/cdiscount_valid_clean_stem.csv\", index=False)\n",
    "data_train_clean_stem.to_csv(\"data/cdiscount_train_clean_stem.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python [conda env:TPInsa]",
   "language": "python",
   "name": "conda-env-TPInsa-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  },
  "toc": {
   "nav_menu": {
    "height": "279px",
    "width": "252px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": true
  },
  "toc-autonumbering": true,
  "toc-showtags": true
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
