{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Ateliers: Technologies de l'intelligence Artificielle](https://github.com/wikistat/AI-Frameworks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<a href=\"http://www.insa-toulouse.fr/\" ><img src=\"http://www.math.univ-toulouse.fr/~besse/Wikistat/Images/logo-insa.jpg\" style=\"float:left; max-width: 120px; display: inline\" alt=\"INSA\"/></a> \n",
    "<a href=\"http://wikistat.fr/\" ><img src=\"http://www.math.univ-toulouse.fr/~besse/Wikistat/Images/wikistat.jpg\" width=400, style=\"max-width: 150px; display: inline\"  alt=\"Wikistat\"/></a>\n",
    "<a href=\"http://www.math.univ-toulouse.fr/\" ><img src=\"http://www.math.univ-toulouse.fr/~besse/Wikistat/Images/logo_imt.jpg\" width=400,  style=\"float:right;  display: inline\" alt=\"IMT\"/> </a>\n",
    "    \n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Traitement Naturel du Langage (NLP) : Catégorisation de Produits Cdiscount\n",
    "\n",
    "Il s'agit d'une version simplifiée du concours proposé par Cdiscount et paru sur le site [datascience.net](https://www.datascience.net/fr/challenge). Les données d'apprentissage sont accessibles sur demande auprès de Cdiscount mais les solutions de l'échantillon test du concours ne sont pas et ne seront pas rendues publiques. Un échantillon test est donc construit pour l'usage de ce tutoriel.  L'objectif est de prévoir la catégorie d'un produit à partir de son descriptif (*text mining*). Seule la catégorie principale (1er niveau, 47 classes) est prédite au lieu des trois niveaux demandés dans le concours. L'objectif est plutôt de comparer les performances des méthodes et technologies en fonction de la taille de la base d'apprentissage ainsi que d'illustrer sur un exemple complexe le prétraitement de données textuelles. \n",
    "\n",
    "Le jeux de données complet (15M produits) permet un test en vrai grandeur du **passage à l'échelle volume** des phases de préparation (*munging*), vectorisation (hashage, TF-IDF) et d'apprentissage en fonction de la technologie utilisée.\n",
    "\n",
    "La synthèse des résultats obtenus est développée par [Besse et al. 2016](https://hal.archives-ouvertes.fr/hal-01350099) (section 5)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partie 2-1 Catégorisation des Produits Cdiscount avec [Scikit-learn](https://spark.apache.org/docs/latest/ml-guide.html) de <a href=\"http://spark.apache.org/\"><img src=\"http://spark.apache.org/images/spark-logo-trademark.png\" style=\"max-width: 100px; display: inline\" alt=\"Python\"/></a> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le principal objectif est de comparer les performances: temps de calcul, qualité des résultats, des principales technologies; ici Python avec la librairie Scikit-Learn. Il s'agit d'un problème de fouille de texte qui enchaîne nécessairement plusieurs étapes et le choix de la meilleure stratégie est fonction de l'étape:\n",
    "- Spark pour la préparation des données: nettoyage, racinisaiton\n",
    "- Python Scikit-learn pour la transformaiton suivante (TF-IDF) et l'apprentissage notamment avec la régresison logistique qui conduit aux meilleurs résultats.\n",
    "\n",
    "\n",
    "L'objectif est ici de comparer les performances des méthodes et technologies en fonction de la taille de la base d'apprentissage. La stratégie de sous ou sur échantillonnage des catégories qui permet d'améliorer la prévision n'a pas été mise en oeuvre.\n",
    "\n",
    "* L'exemple est présenté avec la possibilité de sous-échantillonner afin de réduire les temps de calcul. \n",
    "* L'échantillon réduit peut encore l'être puis, après \"nettoyage\", séparé en 2 parties: apprentissage et test. \n",
    "* Les données textuelles de l'échantillon d'apprentissage sont, \"racinisées\", \"hashées\", \"vectorisées\" avant modélisation.\n",
    "* Les mêmes transformations, notamment (hashage et TF-IDF) évaluées sur l'échantillon d'apprentissage sont appliquées à l'échantillon test.\n",
    "* Un seul modèle est estimé par régression logistique \"multimodal\", plus précisément et implicitement, un modèle par classe.\n",
    "* Différents paramètres: de vectorisation (hashage, TF-IDF), paramètres de la régression logistique (pénalisation L1) pourraient encore être optimisés."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "code_folding": [],
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bguillouet/anaconda/envs/TPInsa/lib/python3.5/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "#Importation des librairies utilisées\n",
    "import unicodedata \n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import nltk\n",
    "import collections\n",
    "import itertools\n",
    "import csv\n",
    "import warnings\n",
    "\n",
    "from sklearn.cross_validation import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Importation des données\n",
    "Définition du répertoir de travail, des noms des différents fichiers utilisés et des variables globales.\n",
    "\n",
    "Dans un premier temps, il vous faut télécharger les fichiers `Categorie_reduit.csv` et `lucene_stopwords.txt` disponible dans le corpus de données de [wikistat](http://wikistat.fr/).\n",
    "\n",
    "Une fois téléchargées, placez ces données dans le repertoire de travail de votre choix et préciser la direction de ce repertoir dans la variable `DATA_DIR`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": [],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Répertoire de travail\n",
    "DATA_DIR = \"\"\n",
    "# Nom des fichiers\n",
    "training_reduit_path = DATA_DIR + \"data/cdiscount_train.csv\"\n",
    "# Variable Globale\n",
    "HEADER_TEST = ['Description','Libelle','Marque']\n",
    "HEADER_TRAIN =['Categorie1','Categorie2','Categorie3','Description','Libelle','Marque']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": [],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Si nécessaire (première exécution) chargement de nltk, librairie pour la suppression \n",
    "## des mots d'arrêt et la racinisation\n",
    "# nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   ### Read & Split Dataset\n",
    "   Fonction permettant de lire le fichier d'apprentissage et de créer deux DataFrame Pandas, un pour l'apprentissage, l'autre pour la validation.\n",
    "   La première méthode créée un DataFrame en lisant entièrement le fichier. Puis elle scinde le DataFrame en deux  grâce à la fonction dédiée de sklearn. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "code_folding": [],
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split Takes 0 s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Categorie1</th>\n",
       "      <th>Categorie2</th>\n",
       "      <th>Categorie3</th>\n",
       "      <th>Description</th>\n",
       "      <th>Libelle</th>\n",
       "      <th>Marque</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5599</th>\n",
       "      <td>PHOTO - OPTIQUE</td>\n",
       "      <td>ACCESSOIRES PHOTO - OPTIQUE</td>\n",
       "      <td>BATTERIE PHOTO - OPTIQUE</td>\n",
       "      <td>Batterie pour PANASONIC DMC-TZ1EG-K - Batterie...</td>\n",
       "      <td>Batterie pour PANASONIC DMC-TZ1EG-K</td>\n",
       "      <td>OTECH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8201</th>\n",
       "      <td>TELEPHONIE - GPS</td>\n",
       "      <td>ACCESSOIRE TELEPHONE</td>\n",
       "      <td>HOUSSE - ETUI - CHAUSSETTE</td>\n",
       "      <td>Housse Folio articulée Classic Noire - Wiko Pe...</td>\n",
       "      <td>Housse Folio articulée Classic Noire - Wiko Pea…</td>\n",
       "      <td>AUCUNE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11414</th>\n",
       "      <td>INSTRUMENTS DE MUSIQUE</td>\n",
       "      <td>OUTIL - PIECE DETACHEE</td>\n",
       "      <td>CORDE</td>\n",
       "      <td>Cordes Thomastik Violon Spirocore Noyau spiral...</td>\n",
       "      <td>Cordes Thomastik Violon Spirocore Noyau spirale…</td>\n",
       "      <td>THOMASTIK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18340</th>\n",
       "      <td>TELEPHONIE - GPS</td>\n",
       "      <td>ACCESSOIRE TELEPHONE</td>\n",
       "      <td>COQUE - BUMPER - FACADE TELEPHONE</td>\n",
       "      <td>Coque  Samsung ACE S5830 Londres Vintage rigid...</td>\n",
       "      <td>Coque  Samsung ACE S5830 Londres Vintage rigide…</td>\n",
       "      <td>AUCUNE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1043</th>\n",
       "      <td>DECO - LINGE - LUMINAIRE</td>\n",
       "      <td>DECORATION MURALE - TABLEAU - CADRE PHOTO - ST...</td>\n",
       "      <td>STICKERS - LETTRES ADHESIVES</td>\n",
       "      <td>Deco Soon - Stickers Bambou - 30 x 23,8 cm - S...</td>\n",
       "      <td>Deco Soon - Stickers Bambou - 30 x 23,8 cm</td>\n",
       "      <td>AUCUNE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Categorie1  \\\n",
       "5599            PHOTO - OPTIQUE   \n",
       "8201           TELEPHONIE - GPS   \n",
       "11414    INSTRUMENTS DE MUSIQUE   \n",
       "18340          TELEPHONIE - GPS   \n",
       "1043   DECO - LINGE - LUMINAIRE   \n",
       "\n",
       "                                              Categorie2  \\\n",
       "5599                         ACCESSOIRES PHOTO - OPTIQUE   \n",
       "8201                                ACCESSOIRE TELEPHONE   \n",
       "11414                             OUTIL - PIECE DETACHEE   \n",
       "18340                               ACCESSOIRE TELEPHONE   \n",
       "1043   DECORATION MURALE - TABLEAU - CADRE PHOTO - ST...   \n",
       "\n",
       "                              Categorie3  \\\n",
       "5599            BATTERIE PHOTO - OPTIQUE   \n",
       "8201          HOUSSE - ETUI - CHAUSSETTE   \n",
       "11414                              CORDE   \n",
       "18340  COQUE - BUMPER - FACADE TELEPHONE   \n",
       "1043        STICKERS - LETTRES ADHESIVES   \n",
       "\n",
       "                                             Description  \\\n",
       "5599   Batterie pour PANASONIC DMC-TZ1EG-K - Batterie...   \n",
       "8201   Housse Folio articulée Classic Noire - Wiko Pe...   \n",
       "11414  Cordes Thomastik Violon Spirocore Noyau spiral...   \n",
       "18340  Coque  Samsung ACE S5830 Londres Vintage rigid...   \n",
       "1043   Deco Soon - Stickers Bambou - 30 x 23,8 cm - S...   \n",
       "\n",
       "                                                Libelle     Marque  \n",
       "5599                Batterie pour PANASONIC DMC-TZ1EG-K      OTECH  \n",
       "8201   Housse Folio articulée Classic Noire - Wiko Pea…     AUCUNE  \n",
       "11414  Cordes Thomastik Violon Spirocore Noyau spirale…  THOMASTIK  \n",
       "18340  Coque  Samsung ACE S5830 Londres Vintage rigide…     AUCUNE  \n",
       "1043         Deco Soon - Stickers Bambou - 30 x 23,8 cm     AUCUNE  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def split_dataset(input_path, nb_line, tauxValid,columns):\n",
    "    time_start = time.time()\n",
    "    data_all = pd.read_csv(input_path,sep=\",\",names=columns,nrows=nb_line)\n",
    "    data_all = data_all.fillna(\"\")\n",
    "    data_train, data_valid = train_test_split(data_all, test_size = tauxValid)\n",
    "    time_end = time.time()\n",
    "    print(\"Split Takes %d s\" %(time_end-time_start))\n",
    "    return data_train, data_valid\n",
    "\n",
    "nb_line=20000  # part totale extraite du fichier initial ici déjà réduit\n",
    "tauxValid=0.10 # part totale extraite du fichier initial ici déjà réduit\n",
    "data_train, data_valid = split_dataset(training_reduit_path, nb_line, tauxValid, HEADER_TRAIN)\n",
    "# Cette ligne permet de visualiser les 5 premières lignes de la DataFrame \n",
    "data_train.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Nettoyage des données\n",
    "Afin de limiter la dimension de l'espace des variables ou *features*, tout en conservant les informations essentielles, il est nécessaire de nettoyer les données en appliquant plusieurs étapes:\n",
    "* Chaque mot est écrit en minuscule.\n",
    "* Les termes numériques, de ponctuation et autres symboles sont supprimés.\n",
    "* 155 mots-courants, et donc non informatifs, de la langue française sont supprimés (STOPWORDS). Ex: le, la, du, alors, etc...\n",
    "* Chaque mot est \"racinisé\", via la fonction `STEMMER.stem` de la librairie nltk. La racinisation transforme un mot en son radical ou sa racine. Par exemple, les mots: cheval, chevaux, chevalier, chevalerie, chevaucher sont tous remplacés par \"cheva\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importation des librairies et fichier pour le nettoyage des données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Librairies \n",
    "from bs4 import BeautifulSoup #Nettoyage d'HTML\n",
    "import re # Regex\n",
    "import nltk # Nettoyage des données\n",
    "\n",
    "## listes de mots à supprimer dans la description des produits\n",
    "## Depuis NLTK\n",
    "nltk_stopwords = nltk.corpus.stopwords.words('french') \n",
    "## Depuis Un fichier externe.\n",
    "lucene_stopwords =open(DATA_DIR+\"data/lucene_stopwords.txt\",\"r\").read().split(\",\") #En local\n",
    "## Union des deux fichiers de stopwords \n",
    "stopwords = list(set(nltk_stopwords).union(set(lucene_stopwords)))\n",
    "\n",
    "## Fonction de setmming de stemming permettant la racinisation\n",
    "stemmer=nltk.stem.SnowballStemmer('french')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fonction de nettoyage de texte\n",
    "Fonction qui prend en intrée un texte et retourne le texte nettoyé en appliquant successivement les étapes suivantes: Nettoyage des données HTML, conversion en texte minuscule, encodage uniforme, suppression des caractéres non alpha numérique (ponctuations), suppression des stopwords, racinisation de chaque mot individuellement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "code_folding": [],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Fonction clean générale\n",
    "def clean_txt(txt):\n",
    "    ### remove html stuff\n",
    "    txt = BeautifulSoup(txt,\"html.parser\",from_encoding='utf-8').get_text()\n",
    "    ### lower case\n",
    "    txt = txt.lower()\n",
    "    ### special escaping character '...'\n",
    "    txt = txt.replace(u'\\u2026','.')\n",
    "    txt = txt.replace(u'\\u00a0',' ')\n",
    "    ### remove accent btw\n",
    "    txt = unicodedata.normalize('NFD', txt).encode('ascii', 'ignore').decode(\"utf-8\")\n",
    "    ###txt = unidecode(txt)\n",
    "    ### remove non alphanumeric char\n",
    "    txt = re.sub('[^a-z_]', ' ', txt)\n",
    "    ### remove french stop words\n",
    "    tokens = [w for w in txt.split() if (len(w)>2) and (w not in stopwords)]\n",
    "    ### french stemming\n",
    "    tokens = [stemmer.stem(token) for token in tokens]\n",
    "    ### tokens = stemmer.stemWords(tokens)\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "def clean_marque(txt):\n",
    "    txt = re.sub('[^a-zA-Z0-9]', '_', txt).lower()\n",
    "    return txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nettoyage des DataFrames\n",
    "Applique le nettoyage sur toutes les lignes de la DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "code_folding": [],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# fonction de nettoyage du fichier(stemming et liste de mots à supprimer)\n",
    "def clean_df(input_data, column_names= ['Description', 'Libelle', 'Marque']):\n",
    "    #Test if columns entry match columns names of input data\n",
    "    column_names_diff= set(column_names).difference(set(input_data.columns))\n",
    "    if column_names_diff:\n",
    "        warnings.warn(\"Column(s) '\"+\", \".join(list(column_names_diff)) +\"' do(es) not match columns of input data\", Warning)\n",
    "    \n",
    "    nb_line = input_data.shape[0]\n",
    "    print(\"Start Clean %d lines\" %nb_line)\n",
    "    \n",
    "    # Cleaning start for each columns\n",
    "    time_start = time.time()\n",
    "    clean_list=[]\n",
    "    for column_name in column_names:\n",
    "        column = input_data[column_name].values\n",
    "        if column_name == \"Marque\":\n",
    "            array_clean = np.array(list(map(clean_marque,column)))\n",
    "        else:\n",
    "            array_clean = np.array(list(map(clean_txt,column)))\n",
    "        clean_list.append(array_clean)\n",
    "    time_end = time.time()\n",
    "    print(\"Cleaning time: %d secondes\"%(time_end-time_start))\n",
    "    \n",
    "    #Convert list to DataFrame\n",
    "    array_clean = np.array(clean_list).T\n",
    "    data_clean = pd.DataFrame(array_clean, columns = column_names)\n",
    "    return data_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Clean 2000 lines\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bguillouet/anaconda/envs/TPInsa/lib/python3.5/site-packages/bs4/__init__.py:146: UserWarning: You provided Unicode markup but also provided a value for from_encoding. Your from_encoding will be ignored.\n",
      "  warnings.warn(\"You provided Unicode markup but also provided a value for from_encoding. Your from_encoding will be ignored.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning time: 2 secondes\n",
      "Start Clean 18000 lines\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bguillouet/anaconda/envs/TPInsa/lib/python3.5/site-packages/bs4/__init__.py:219: UserWarning: \"b'.'\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  ' Beautiful Soup.' % markup)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning time: 17 secondes\n"
     ]
    }
   ],
   "source": [
    "# Take approximately 2 minutes fors 100.000 rows\n",
    "data_valid_clean = clean_df(data_valid)\n",
    "data_train_clean = clean_df(data_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Affiche les 5 premières lignes de la DataFrame d'apprentissage après nettoyage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Description</th>\n",
       "      <th>Libelle</th>\n",
       "      <th>Marque</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>batter panasonic dmc batter appareil photo pan...</td>\n",
       "      <td>batter panasonic dmc</td>\n",
       "      <td>otech</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>houss folio articule classic noir wiko peax to...</td>\n",
       "      <td>houss folio articule classic noir wiko pe</td>\n",
       "      <td>aucune</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cord thomastik violon spirocor noyau spiral je...</td>\n",
       "      <td>cord thomastik violon spirocor noyau spiral</td>\n",
       "      <td>thomastik</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>coqu samsung ace londr vintag rigid mad franc ...</td>\n",
       "      <td>coqu samsung ace londr vintag rigid</td>\n",
       "      <td>aucune</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>deco soon sticker bambou sticker bambou envi a...</td>\n",
       "      <td>deco soon sticker bambou</td>\n",
       "      <td>aucune</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         Description  \\\n",
       "0  batter panasonic dmc batter appareil photo pan...   \n",
       "1  houss folio articule classic noir wiko peax to...   \n",
       "2  cord thomastik violon spirocor noyau spiral je...   \n",
       "3  coqu samsung ace londr vintag rigid mad franc ...   \n",
       "4  deco soon sticker bambou sticker bambou envi a...   \n",
       "\n",
       "                                       Libelle     Marque  \n",
       "0                         batter panasonic dmc      otech  \n",
       "1    houss folio articule classic noir wiko pe     aucune  \n",
       "2  cord thomastik violon spirocor noyau spiral  thomastik  \n",
       "3          coqu samsung ace londr vintag rigid     aucune  \n",
       "4                     deco soon sticker bambou     aucune  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train_clean.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Construction des caractéristiques ou *features* (TF-IDF)¶\n",
    "### Introduction\n",
    "La vectorisation, c'est-à-dire la construction des caractéristiques à partir de la liste des mots se fait en 2 étapes:\n",
    "* **Hashage**. Il permet de réduire l'espace des variables (taille du dictionnaire) en un nombre limité et fixé a priori `n_hash` de caractéristiques. Il repose sur la définition d'une fonction de hashage, $h$ qui à un indice $j$ défini dans l'espace des entiers naturels, renvoie un indice $i=h(j)$ dans dans l'espace réduit (1 à n_hash) des caractéristiques. Ainsi le poids de l'indice $i$, du nouvel espace, est l'association de tous les poids d'indice $j$ tels que $i=h(j)$ de l'espace originale. Ici, les poids sont associés d'après la méthode décrite par Weinberger et al. (2009).\n",
    "\n",
    "N.B. $h$ n'est pas généré aléatoirement. Ainsi pour un même fichier d'apprentissage (ou de test) et pour un même entier n_hash, le résultat de la fonction de hashage est identique\n",
    "\n",
    "* **TF-IDF**. Le TF-IDF permet de faire ressortir l'importance relative de chaque mot $m$ (ou couples de mots consécutifs) dans un texte-produit ou un descriptif $d$, par rapport à la liste entière des produits. La fonction $TF(m,d)$ compte le nombre d'occurences du mot $m$ dans le descriptif $d$. La fonction $IDF(m)$ mesure l'importance du terme dans l'ensemble des documents ou descriptifs en donnant plus de poids aux termes les moins fréquents car considérés comme les plus discriminants (motivation analogue à celle de la métrique du chi2 en anamlyse des correspondance). $IDF(m,l)=\\log\\frac{D}{f(m)}$ où $D$ est le nombre de documents, la taille de l'échantillon d'apprentissage, et $f(m)$ le nombre de documents ou descriptifs contenant le mot $m$. La nouvelle variable ou *features* est $V_m(l)=TF(m,l)\\times IDF(m,l)$.\n",
    "\n",
    "* Comme pour les transformations des variables quantitatives (centrage, réduction), la même transformation c'est-à-dire les mêmes pondérations, est calculée sur l'achantillon d'apprentissage et appliquée à celui de test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fonction de Vectorisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Création d’une matrice indiquant\n",
    "## les fréquences des mots contenus dans chaque description\n",
    "## de nombreux paramètres seraient à tester\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, TfidfTransformer\n",
    "from sklearn.feature_extraction import FeatureHasher\n",
    "\n",
    "\n",
    "def vectorizer_train(df, columns=['Description', 'Libelle', 'Marque'], nb_hash=None, stop_words=None):\n",
    "    \n",
    "    # Hashage\n",
    "    if nb_hash is None:\n",
    "        data_hash = map(lambda x : \" \".join(x), df[columns].values)\n",
    "        feathash = None\n",
    "            # TFIDF\n",
    "        vec = TfidfVectorizer(\n",
    "            min_df = 1,\n",
    "            stop_words = stop_words,\n",
    "            smooth_idf=True,\n",
    "            norm='l2',\n",
    "            sublinear_tf=True,\n",
    "            use_idf=True,\n",
    "            ngram_range=(1,2)) #bi-grams\n",
    "        tfidf = vec.fit_transform(data_hash)\n",
    "    else:\n",
    "        df_text = map(lambda x : collections.Counter(\" \".join(x).split(\" \")), df[columns].values)\n",
    "        feathash = FeatureHasher(nb_hash)\n",
    "        data_hash = feathash.fit_transform(map(collections.Counter,df_text))\n",
    "        \n",
    "        vec =  TfidfTransformer(use_idf=True,\n",
    "                            smooth_idf=True, sublinear_tf=False)\n",
    "        tfidf =  vec.fit_transform(data_hash)\n",
    "\n",
    "    return vec, feathash, tfidf\n",
    "\n",
    "\n",
    "\n",
    "def apply_vectorizer(df, vec, columns =['Description', 'Libelle', 'Marque'], feathash = None ):\n",
    "    \n",
    "    #Hashage\n",
    "    if feathash is None:\n",
    "        data_hash = map(lambda x : \" \".join(x), df[columns].values)\n",
    "    else:\n",
    "        df_text = map(lambda x : collections.Counter(\" \".join(x).split(\" \")), df[columns].values)\n",
    "        data_hash = feathash.transform(df_text)\n",
    "    \n",
    "    # TFIDF\n",
    "    tfidf=vec.transform(data_hash)\n",
    "    return tfidf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vec, feathash, X = vectorizer_train(data_train_clean, nb_hash=60)\n",
    "Y = data_train['Categorie1'].values\n",
    "\n",
    "Xv = apply_vectorizer(data_valid_clean, vec, feathash=feathash)\n",
    "Yv=data_valid['Categorie1'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Modélisation et performances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# training score: 0.533166666667\n"
     ]
    }
   ],
   "source": [
    "# Regression Logistique \n",
    "## estimation\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "cla = LogisticRegression(C=100)\n",
    "cla.fit(X,Y)\n",
    "score=cla.score(X,Y)\n",
    "print('# training score:',score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# validation score: 0.521\n"
     ]
    }
   ],
   "source": [
    "## erreur en validation\n",
    "scoreValidation=cla.score(Xv,Yv)\n",
    "print('# validation score:',scoreValidation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CART Takes 2 s\n",
      "# training score : 0.999555555556\n"
     ]
    }
   ],
   "source": [
    "#Méthode  CART\n",
    "from sklearn import tree\n",
    "clf = tree.DecisionTreeClassifier()\n",
    "time_start = time.time()\n",
    "clf = clf.fit(X, Y)\n",
    "time_end = time.time()\n",
    "print(\"CART Takes %d s\" %(time_end-time_start) )\n",
    "score=clf.score(X,Y)\n",
    "print('# training score :',score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# validation score : 0.545\n"
     ]
    }
   ],
   "source": [
    "scoreValidation=clf.score(Xv,Yv)\n",
    "print('# validation score :',scoreValidation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF Takes 13 s\n",
      "# training score : 0.999555555556\n"
     ]
    }
   ],
   "source": [
    "# Random forest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf = RandomForestClassifier(n_estimators=100,n_jobs=-1,max_features=24)\n",
    "time_start = time.time()\n",
    "rf = rf.fit(X, Y)\n",
    "time_end = time.time()\n",
    "print(\"RF Takes %d s\" %(time_end-time_start) )\n",
    "score=rf.score(X,Y)\n",
    "print('# training score :',score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# validation score : 0.6435\n"
     ]
    }
   ],
   "source": [
    "scoreValidation=rf.score(Xv,Yv)\n",
    "print('# validation score :',scoreValidation)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python [conda env:TPInsa]",
   "language": "python",
   "name": "conda-env-TPInsa-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  },
  "toc": {
   "nav_menu": {
    "height": "279px",
    "width": "252px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
