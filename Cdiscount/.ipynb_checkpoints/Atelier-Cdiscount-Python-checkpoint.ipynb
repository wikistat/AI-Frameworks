{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<a href=\"http://www.insa-toulouse.fr/\" ><img src=\"http://www.math.univ-toulouse.fr/~besse/Wikistat/Images/logo-insa.jpg\" style=\"float:left; max-width: 120px; display: inline\" alt=\"INSA\"/></a> \n",
    "\n",
    "<a href=\"http://wikistat.fr/\" ><img src=\"http://www.math.univ-toulouse.fr/~besse/Wikistat/Images/wikistat.jpg\" style=\"max-width: 250px; display: inline\"  alt=\"Wikistat\"/></a>\n",
    "\n",
    "<a href=\"http://www.math.univ-toulouse.fr/\" ><img src=\"http://www.math.univ-toulouse.fr/~besse/Wikistat/Images/logo_imt.jpg\" style=\"float:right; max-width: 250px; display: inline\" alt=\"IMT\"/> </a>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Ateliers: Technologies des données massives](https://github.com/wikistat/Ateliers-Big-Data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Mining et Catégorisation de Produits en <a href=\"https://www.python.org/\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/f/f8/Python_logo_and_wordmark.svg/390px-Python_logo_and_wordmark.svg.png\" style=\"max-width: 120px; display: inline\" alt=\"R\"/></a> avec <a href=\"http://scikit-learn.org/stable/#\"><img src=\"http://scikit-learn.org/stable/_static/scikit-learn-logo-small.png\" style=\"max-width: 100px; display: inline\" alt=\"R\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Il s'agit d'une version simplifiée du concours proposé par CDiscount et paru sur le site [datascience.net](https://www.datascience.net/fr/challenge). Les données d'apprentissage sont accessibles sur demande auprès de CDiscount. Le solutions de l'échantillon test du concours ne sont pas et ne seront pas rendues publiques. Un échantillon test est donc construit pour l'usage de ce tutoriel.  L'objectif est de prévoir la catégorie d'un produit à partir de son descriptif. Seule la catégorie principale (1er niveau) est prédite au lieu des trois niveaux demandés dans le concours. L'objectif est plutôt de comparer les performances des méthodes et technologies en fonction de la taille de la base d'apprentissage ainsi que d'illustrer sur un exemple complexe le prétraitement de données textuelles. La stratégie de sous ou sur échantillonnage des catégories qui permet d'améliorer la prévision n'a pas été mise en oeuvre.\n",
    "* L'exemple est présenté sur un échantillon réduit d'un million de produits au lieu des 15M initiaux\n",
    "* L'échantillon réduit peut encore l'être puis séparé en 2 parties: apprentissage et validation. \n",
    "* Les données textuelles sont  nettoyées, racinisées, vectorisées avant modélisation.\n",
    "* Trois modélisations sont estimées: logistique, arbre, forêt aléatoire.\n",
    "* Optimiser l'erreur en faisant varier différents paramètres: types et paramètres de vectorisation (TF-IDF), paramètres de la régression logistique (pénalisation L1) et de la forêt aléatoire (nombre d'arbres et nombre de variables aléatoire).\n",
    "\n",
    "Exécuter finalement le code pour différentes tailles (paramètre tauxTot ci-dessous) de l'échantillon d'apprentissage et comparer les qualités de prévision obtenues. \n",
    "\n",
    "Deux échantillons de test ont été mis de côté et seront utilisés dans un prochain calepin (avec pyspark) pour comparer les stratégies.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "code_folding": [],
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Importation des librairies utilisées\n",
    "import unicodedata \n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import nltk\n",
    "import collections\n",
    "import itertools\n",
    "import csv\n",
    "import warnings\n",
    "import urllib2\n",
    "\n",
    "from sklearn.cross_validation import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Importation des données\n",
    "Définition du répertoir de travail, des noms des différents fichiers utilisés et des variables globales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": [],
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Répertoire de travail\n",
    "DATA_DIR_URL = \"https://www.math.univ-toulouse.fr/~besse/Wikistat/data/\"\n",
    "# Nom des fichiers\n",
    "training_reduit_path_url = DATA_DIR_URL + \"Categorie_reduit.csv\"\n",
    "# Variable Globale\n",
    "HEADER_TEST = ['Description','Libelle','Marque']\n",
    "HEADER_TRAIN =['Categorie1','Categorie2','Categorie3','Description','Libelle','Marque']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Si nécessaire (première exécution) chargement de nltk, librairie pour la suppression \n",
    "## des mots d'arrêt et la racinisation\n",
    "## nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   ### Read & Split Dataset\n",
    "   Fonction permettant de lire le fichier d'apprentissage et de créer deux DataFrame Pandas, une pour l'apprentissage, l'autre pour la validation.\n",
    "   La première méthode crée une DataFrame en lisant entièrement le fichier. Puis elle split la DataFrame en deux à partir grâce a la fonction dédié de sklearn. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split Takes 4 s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Categorie1</th>\n",
       "      <th>Categorie2</th>\n",
       "      <th>Categorie3</th>\n",
       "      <th>Description</th>\n",
       "      <th>Libelle</th>\n",
       "      <th>Marque</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>98778</th>\n",
       "      <td>1000001700</td>\n",
       "      <td>1000001777</td>\n",
       "      <td>1000001788</td>\n",
       "      <td>Tringle double fer forgé en kit D20 modèle Sph...</td>\n",
       "      <td>Tringle double fer forgé en kit D20 modèle Sphère</td>\n",
       "      <td>AUCUNE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99558</th>\n",
       "      <td>1000003924</td>\n",
       "      <td>1000003925</td>\n",
       "      <td>1000008094</td>\n",
       "      <td>Dalle Ecran 15.6\" LED  HP COMPAQ Pavilion DV6-...</td>\n",
       "      <td>Dalle Ecran 15.6\" LED  HP COMPAQ Pavilion DV6-...</td>\n",
       "      <td>VISIODIRECT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43001</th>\n",
       "      <td>1000006204</td>\n",
       "      <td>1000007080</td>\n",
       "      <td>1000007118</td>\n",
       "      <td>Adidas Maillot Chelsea FC 2014/15  - Maillot C...</td>\n",
       "      <td>Adidas Maillot Chelsea FC 2… L</td>\n",
       "      <td>ADIDAS ORIGINALS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93928</th>\n",
       "      <td>1000003924</td>\n",
       "      <td>1000003930</td>\n",
       "      <td>1000004085</td>\n",
       "      <td>BATTERIE HP HSTNN101C - Li-Ion 10.8V / 11.1V 4...</td>\n",
       "      <td>BATTERIE HP HSTNN101C</td>\n",
       "      <td>AUCUNE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76251</th>\n",
       "      <td>1000010560</td>\n",
       "      <td>1000010623</td>\n",
       "      <td>1000010653</td>\n",
       "      <td>Coque souple Rose pour GOOGLE NEXUS 4 motif Pl...</td>\n",
       "      <td>Coque souple Rose pour GOOGLE NEXUS 4 motif Pl...</td>\n",
       "      <td>MUZZANO</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Categorie1  Categorie2  Categorie3  \\\n",
       "98778  1000001700  1000001777  1000001788   \n",
       "99558  1000003924  1000003925  1000008094   \n",
       "43001  1000006204  1000007080  1000007118   \n",
       "93928  1000003924  1000003930  1000004085   \n",
       "76251  1000010560  1000010623  1000010653   \n",
       "\n",
       "                                             Description  \\\n",
       "98778  Tringle double fer forgé en kit D20 modèle Sph...   \n",
       "99558  Dalle Ecran 15.6\" LED  HP COMPAQ Pavilion DV6-...   \n",
       "43001  Adidas Maillot Chelsea FC 2014/15  - Maillot C...   \n",
       "93928  BATTERIE HP HSTNN101C - Li-Ion 10.8V / 11.1V 4...   \n",
       "76251  Coque souple Rose pour GOOGLE NEXUS 4 motif Pl...   \n",
       "\n",
       "                                                 Libelle            Marque  \n",
       "98778  Tringle double fer forgé en kit D20 modèle Sphère            AUCUNE  \n",
       "99558  Dalle Ecran 15.6\" LED  HP COMPAQ Pavilion DV6-...       VISIODIRECT  \n",
       "43001                     Adidas Maillot Chelsea FC 2… L  ADIDAS ORIGINALS  \n",
       "93928                              BATTERIE HP HSTNN101C            AUCUNE  \n",
       "76251  Coque souple Rose pour GOOGLE NEXUS 4 motif Pl...           MUZZANO  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def split_dataset(input_path, nb_line, tauxValid,columns):\n",
    "    time_start = time.time()\n",
    "    data_all = pd.read_csv(input_path,sep=\";\",names=columns,nrows=nb_line)\n",
    "    data_all = data_all.fillna(\"\")\n",
    "    data_train, data_valid = train_test_split(data_all, test_size = tauxValid)\n",
    "    time_end = time.time()\n",
    "    print(\"Split Takes %d s\" %(time_end-time_start))\n",
    "    return data_train, data_valid\n",
    "\n",
    "nb_line=50000  # part totale extraite du fichier initial ici déjà réduit\n",
    "tauxValid=0.10 # part totale extraite du fichier initial ici déjà réduit\n",
    "data_train, data_valid = split_dataset(training_reduit_path_url, nb_line, tauxValid, HEADER_TRAIN)\n",
    "# Cette ligne permet de visualiser les 5 premières lignes de la DataFrame \n",
    "data_train.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Nettoyage des données\n",
    "Afin de limiter la dimension de l'espace des variables ou features tout en conservant les informations essentielles, il est nécessaire de nettoyer les données en appliquant plusieurs étapes:\n",
    "* Chaque mot est écrit en minuscule.\n",
    "* Les termes numériques, de ponctuation et autres symboles sont supprimés.\n",
    "* 155 mots-courants, et donc non informatif, de la langue française sont supprimés (STOPWORDS). Ex: le, la, du, alors, etc...\n",
    "* Chaque mot est \"racinisé\", via la fonction STEMMER.stem de la librairie nltk. La racinisation transforme un mot en son radical ou sa racine. Par exemple, les mots: cheval, chevaux, chevalier, chevalerie, chevaucher sont tous remplacés par \"cheva\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importation des librairies et fichier pour le nettoyage des données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Librairies \n",
    "from bs4 import BeautifulSoup #Nettoyage d'HTML\n",
    "import re # Regex\n",
    "import nltk # Nettoyage des données\n",
    "\n",
    "## listes de mots à supprimer dans la description des produits\n",
    "## Depuis NLTK\n",
    "nltk_stopwords = nltk.corpus.stopwords.words('french') \n",
    "## Depuis Un fichier externe.\n",
    "#lucene_stopwords = [unicode(w, \"utf-8\") for w in open(DATA_DIR_LOCAL+\"lucene_stopwords.txt\").read().split(\",\")] #En local\n",
    "lucene_stopwords = [unicode(w, \"utf-8\") for w in urllib2.urlopen(DATA_DIR_URL+\"lucene_stopwords.txt\").read().split(\",\")] #Depuis URL\n",
    "## Union des deux fichiers de stopwords \n",
    "stopwords = list(set(nltk_stopwords).union(set(lucene_stopwords)))\n",
    "\n",
    "## Fonction de setmming de stemming permettant la racinisation\n",
    "stemmer=nltk.stem.SnowballStemmer('french')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fonction de nettoyage de texte\n",
    "Fonction qui prend en intrée un texte et retourne le texte nettoyé en appliquant successivement les étapes suivantes: Nettoyage des données HTML, conversion en texte minuscule, encodage uniforme, suppression des caractéres non alpha numérique (ponctuations), suppression des stopwords, racinisation de chaque mot individuellement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "code_folding": [
     1,
     21
    ],
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Fonction clean générale\n",
    "def clean_txt(txt):\n",
    "    ### remove html stuff\n",
    "    txt = BeautifulSoup(txt,\"html.parser\",from_encoding='utf-8').get_text()\n",
    "    ### lower case\n",
    "    txt = txt.lower()\n",
    "    ### special escaping character '...'\n",
    "    txt = txt.replace(u'\\u2026','.')\n",
    "    txt = txt.replace(u'\\u00a0',' ')\n",
    "    ### remove accent btw\n",
    "    txt = unicodedata.normalize('NFD', txt).encode('ascii', 'ignore')\n",
    "    ###txt = unidecode(txt)\n",
    "    ### remove non alphanumeric char\n",
    "    txt = re.sub('[^a-z_]', ' ', txt)\n",
    "    ### remove french stop words\n",
    "    tokens = [w for w in txt.split() if (len(w)>2) and (w not in stopwords)]\n",
    "    ### french stemming\n",
    "    tokens = [stemmer.stem(token) for token in tokens]\n",
    "    ### tokens = stemmer.stemWords(tokens)\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "def clean_marque(txt):\n",
    "    txt = re.sub('[^a-zA-Z0-9]', '_', txt).lower()\n",
    "    return txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nettoyage des DataFrames\n",
    "Applique le nettoyage sur toutes les lignes de la DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "code_folding": [
     1
    ],
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# fonction de nettoyage du fichier(stemming et liste de mots à supprimer)\n",
    "def clean_df(input_data, column_names= ['Description', 'Libelle', 'Marque']):\n",
    "    #Test if columns entry match columns names of input data\n",
    "    column_names_diff= set(column_names).difference(set(input_data.columns))\n",
    "    if column_names_diff:\n",
    "        warnings.warn(\"Column(s) '\"+\", \".join(list(column_names_diff)) +\"' do(es) not match columns of input data\", Warning)\n",
    "    \n",
    "    nb_line = input_data.shape[0]\n",
    "    print(\"Start Clean %d lines\" %nb_line)\n",
    "    \n",
    "    # Cleaning start for each columns\n",
    "    time_start = time.time()\n",
    "    clean_list=[]\n",
    "    for column_name in column_names:\n",
    "        column = input_data[column_name].values\n",
    "        if column_name == \"Marque\":\n",
    "            array_clean = np.array(map(clean_marque,column))\n",
    "        else:\n",
    "            array_clean = np.array(map(clean_txt,column))\n",
    "        clean_list.append(array_clean)\n",
    "    time_end = time.time()\n",
    "    print(\"Cleaning time: %d secondes\"%(time_end-time_start))\n",
    "    \n",
    "    #Convert list to DataFrame\n",
    "    array_clean = np.array(clean_list).T\n",
    "    data_clean = pd.DataFrame(array_clean, columns = column_names)\n",
    "    return data_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Clean 10000 lines\n",
      "Cleaning time: 15 secondes\n",
      "Start Clean 90000 lines\n",
      "Cleaning time: 142 secondes\n"
     ]
    }
   ],
   "source": [
    "# Take approximately 2 minutes fors 100.000 rows\n",
    "data_valid_clean = clean_df(data_valid)\n",
    "data_train_clean = clean_df(data_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Affiche les 5 premières lignes de la DataFrame d'apprentissage après nettoyage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Description</th>\n",
       "      <th>Libelle</th>\n",
       "      <th>Marque</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tringl doubl fer forg kit model spher kit pret...</td>\n",
       "      <td>tringl doubl fer forg kit model spher</td>\n",
       "      <td>aucune</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>dall ecran led compaq pavilion dall decran led...</td>\n",
       "      <td>dall ecran led compaq pavilion</td>\n",
       "      <td>visiodirect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>adid maillot chels maillot chels color noir tu...</td>\n",
       "      <td>adid maillot chels</td>\n",
       "      <td>adidas_originals</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>batter hstnn ion mah cellul noir compatibl bat...</td>\n",
       "      <td>batter hstnn</td>\n",
       "      <td>aucune</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>coqu soupl ros googl nexus motif plum coqu sou...</td>\n",
       "      <td>coqu soupl ros googl nexus motif plum</td>\n",
       "      <td>muzzano</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         Description  \\\n",
       "0  tringl doubl fer forg kit model spher kit pret...   \n",
       "1  dall ecran led compaq pavilion dall decran led...   \n",
       "2  adid maillot chels maillot chels color noir tu...   \n",
       "3  batter hstnn ion mah cellul noir compatibl bat...   \n",
       "4  coqu soupl ros googl nexus motif plum coqu sou...   \n",
       "\n",
       "                                 Libelle            Marque  \n",
       "0  tringl doubl fer forg kit model spher            aucune  \n",
       "1         dall ecran led compaq pavilion       visiodirect  \n",
       "2                     adid maillot chels  adidas_originals  \n",
       "3                           batter hstnn            aucune  \n",
       "4  coqu soupl ros googl nexus motif plum           muzzano  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train_clean.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Construction des features (TF-IDF)¶\n",
    "### Introduction\n",
    "La vectorisation, c'est à dire la construction des features à partir de la liste des mots se fait en 2 étapes:\n",
    "* Le hashage permet de réduire l'espace des variables (taille du dictionnaire) en un nombre limité et fixé a priori n_hash de features. Il repose sur la définition d'une hash function, $h$ qui à un indice $j$ défini dans l'espace des entiers naturels, renvoie un indice $i=h(j)$ dans dans l'espace réduit (1 à n_hash) des features. Ainsi le poids de l'indice $i$, du nouvel espace, est l'association de tous les poids d'indice $j$ tels que $i=h(j)$ de l'espace originale. Ici, les poids sont associés d'après la méthode décrit par Weinberger et al. (2009).\n",
    "\n",
    "N.B. $h$ n'est pas généré aléatoirement. Ainsi pour un même fichier d'apprentissage (ou de test) et pour un même entier n_hash, le résultat de la fonction de hashage est identique\n",
    "\n",
    "* Le TF-IDF. Le TF-IDF permet de faire ressortir l'importance relative de chaque mot $m$ (ou couples de mots consécutifs) dans un texte-produit ou un descriptif $d$, par rapport à la liste entière des produits. La fonction $TF(m,d)$ compte le nombre d'occurences du mot $m$ dans le descriptif $d$. La fonction $IDF(m)$ mesure l'importance du terme dans l'ensemble des documents ou descriptifs en donnant plus de poids aux termes les moins fréquents car considérés comme les plus discriminants (motivation analogue à celle de la métrique du chi2 en anamlyse des correspondance). $IDF(m,l)=\\log\\frac{D}{f(m)}$ où $D$ est le nombre de documents, la taille de l'échantillon d'apprentissage, et $f(m)$ le nombre de documents ou descriptifs contenant le mot $m$. La nouvelle variable ou features est $V_m(l)=TF(m,l)\\times IDF(m,l)$.\n",
    "\n",
    "* Comme pour les transformations des variables quantitatives (centrage, réduction), la même transformation c'est-à-dire les mêmes pondérations, est calculée sur l'achantillon d'apprentissage et appliquée à celui de test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fonction de Vectorisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Création d’une matrice indiquant\n",
    "## les fréquences\" des mots contenus dans chaque description\n",
    "## de nombreux paramètres seraient à tester\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction import FeatureHasher\n",
    "\n",
    "\n",
    "def vectorizer_train(df, columns=['Description', 'Libelle', 'Marque'], nb_hash=None, stop_words=None):\n",
    "    \n",
    "    # Hashage\n",
    "    if nb_hash is None:\n",
    "        data_hash = map(lambda x : \" \".join(x), df[columns].values)\n",
    "        feathash = None\n",
    "    else:\n",
    "        df_text = map(lambda x : collections.Counter(\" \".join(x).split(\" \")), df[columns].values)\n",
    "        feathash = FeatureHasher(nb_hash)\n",
    "        data_hash = feathash.fit_transform(map(collections.Counter,df_text))\n",
    "\n",
    "    # TFIDF\n",
    "    vec = TfidfVectorizer(\n",
    "        min_df = 1,\n",
    "        stop_words = stop_words,\n",
    "        smooth_idf=True,\n",
    "        norm='l2',\n",
    "        sublinear_tf=True,\n",
    "        use_idf=True,\n",
    "        ngram_range=(1,2)) #bi-grams\n",
    "    tfidf = vec.fit_transform(data_hash)\n",
    "    return vec, feathash, tfidf\n",
    "\n",
    "\n",
    "\n",
    "def apply_vectorizer(df, vec, columns =['Description', 'Libelle', 'Marque'], feathash = None ):\n",
    "    \n",
    "    #Hashage\n",
    "    if feathash is None:\n",
    "        data_hash = map(lambda x : \" \".join(x), df[columns].values)\n",
    "    else:\n",
    "        df_text = map(lambda x : collections.Counter(\" \".join(x).split(\" \")), df[columns].values)\n",
    "        data_hash = feathash.transform(df_text)\n",
    "    \n",
    "    # TFIDF\n",
    "    tfidf=vec.transform(data_hash)\n",
    "    return tfidf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vec, feathash, X = vectorizer_train(data_train_clean)\n",
    "Y = data_train['Categorie1'].values\n",
    "\n",
    "Xv = apply_vectorizer(data_valid_clean, vec)\n",
    "Yv=data_valid['Categorie1'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Modélisation et performances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('# training score:', 0.99928888888888889)\n"
     ]
    }
   ],
   "source": [
    "# Regression Logistique \n",
    "## estimation\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "cla = LogisticRegression(C=100)\n",
    "cla.fit(X,Y)\n",
    "score=cla.score(X,Y)\n",
    "print('# training score:',score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('# validation score:', 0.93200000000000005)\n"
     ]
    }
   ],
   "source": [
    "## erreur en validation\n",
    "scoreValidation=cla.score(Xv,Yv)\n",
    "print('# validation score:',scoreValidation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CART Takes 252 s\n",
      "('# training score :', 0.99934444444444448)\n"
     ]
    }
   ],
   "source": [
    "#Méthode  CART\n",
    "from sklearn import tree\n",
    "clf = tree.DecisionTreeClassifier()\n",
    "time_start = time.time()\n",
    "clf = clf.fit(X, Y)\n",
    "time_end = time.time()\n",
    "print(\"CART Takes %d s\" %(time_end-time_start) )\n",
    "score=clf.score(X,Y)\n",
    "print('# training score :',score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('# validation score :', 0.77929999999999999)\n"
     ]
    }
   ],
   "source": [
    "scoreValidation=clf.score(Xv,Yv)\n",
    "print('# validation score :',scoreValidation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Random forest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf = RandomForestClassifier(n_estimators=100,n_jobs=-1,max_features=24)\n",
    "time_start = time.time()\n",
    "rf = rf.fit(X, Y)\n",
    "time_end = time.time()\n",
    "print(\"RF Takes %d s\" %(time_end-time_start) )\n",
    "score=rf.score(X,Y)\n",
    "print('# training score :',score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "scoreValidation=rf.score(Xv,Yv)\n",
    "print('# validation score :',scoreValidation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  },
  "toc": {
   "nav_menu": {
    "height": "279px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
