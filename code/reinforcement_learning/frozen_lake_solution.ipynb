{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practical Session: Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this practical session, you will implement the [Value iteration and Policy iteration algorithms](https://www.ics.uci.edu/~dechter/publications/r42a-mdp_report.pdf) algorithm and test it in a reinforcement learning toy environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenAI Gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [Gym library](https://www.google.com/search?client=firefox-b-d&q=openai+gym)  is a collection of test problems (or environments) developed by [OpenAI](https://openai.com/) sharing a standard interface.\n",
    "This standard interface allows us to write general reinforcement learning algorithms and test them on several environments without many adaptations.\n",
    "\n",
    "Gym's interface is straightforward. \n",
    "Its core object is an **environment**, usually created with the instruction \n",
    "```python\n",
    "gym.make(\"ENV_NAME\")\n",
    "```\n",
    "A gym environment has three key methods:\n",
    "* `reset()`: this method reset the environment and return an observation of a random initial state \n",
    "* `step(a)`: this method takes action `a` and returns three variables:\n",
    "    * `observation`: the observation of the next state\n",
    "    * `reward`: the reward obtained after transitioning from the previous state to the new one taking action `a`\n",
    "    * `done`: a boolean indicating if an episode is finished\n",
    "    * `info`: a variable used to pass any other kind of information\n",
    "* `render()` this method renders the current state of the environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frozen Lake\n",
    "\n",
    "[Frozen lake](https://gym.openai.com/envs/FrozenLake-v0/) is an elementary \"grid-world\" environment provided in OpenAi Gym.\n",
    "Starting from a non-changing initial position, you control an agent whose objective is to reach a goal located at the exact opposite of the map.\n",
    "Some of the tiles are walkable, some other are holes ,and walking on them leads to the end of the episode. Due to the slipperiness of the frozen lake, some randomness is added in the transitions meaning that the movement direction of the agent is uncertain and only partially depends on the chosen direction.\n",
    "\n",
    "Here is the official FrozenLake description provided by OpenAI:\n",
    "\n",
    "*Winter is here. You and your friends were tossing around a frisbee at the park when you made a wild throw that left the frisbee out in the middle of the lake. The water is mostly frozen, but there are a few holes where the ice has melted. If you step into one of those holes, you'll fall into the freezing water. At this time, there's an international frisbee shortage, so it's absolutely imperative that you navigate across the lake and retrieve the disc. However, the ice is slippery, so you won't always move in the direction you intend.*\n",
    "\n",
    "*The surface is described using a grid like the following:*\n",
    "```\n",
    "SFFF       (S: starting point, safe)\n",
    "FHFH       (F: frozen surface, safe)\n",
    "FFFH       (H: hole, fall to your doom)\n",
    "HFFG       (G: goal, where the frisbee is located)\n",
    "```\n",
    "*The episode ends when you reach the goal or fall in a hole. You receive a reward of 1 if you reach the goal, and zero otherwise.*\n",
    "\n",
    "Let's instantiate a FrozenLake environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "env = gym.make(\"FrozenLake8x8-v0\")\n",
    "env.reset()\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The environment provides informations about its Action and State Spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Action Space {env.action_space}\")\n",
    "print(f\"State Space {env.observation_space}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The environment is composed of 64 discrete states corresponding to the agent's position on the grid. \n",
    "The four possible actions are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_map = {0:u'\\u2190', 1:u'\\u2193', 2:u'\\u2192', 3:u'\\u2191'}\n",
    "for k, v in action_map.items():\n",
    "    print(f\"action {k}: {v}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the ```step``` and ```render``` methods to observe the effect of the actions on the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.step(1)\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function will help you to visualize our agent's trajectories:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "from time import sleep\n",
    "\n",
    "def display_trajectory(frames):\n",
    "    for i, frame in enumerate(frames):\n",
    "        clear_output(wait=True)\n",
    "        print(frame['frame'])\n",
    "        print(f\"Timestep: {i + 1}\")\n",
    "        print(f\"State: {frame['state']}\")\n",
    "        print(f\"Reward: {frame['reward']}\")\n",
    "        sleep(.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code shows you how to run an episode with an agent taking random actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = [] # for animation\n",
    "env.reset()\n",
    "while True:\n",
    "    # draw a random action from the action space\n",
    "    action = env.action_space.sample()\n",
    "    # the step method takes an action as input and returns 4 variables described in the OpenAI section\n",
    "    state, reward, done, info = env.step(action)\n",
    "    frames.append({\n",
    "        'frame': env.render(mode='ansi'),\n",
    "        'state': state,\n",
    "        'reward': reward\n",
    "        })\n",
    "    #if done is True then the episode is over\n",
    "    if done:\n",
    "        break\n",
    "        \n",
    "display_trajectory(frames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Human policy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We just saw a random policy. Let's now check on your policy.    \n",
    "Using the possible actions, try to reach the goal by running multiple times the next cell.  \n",
    "Reminder: action 0: ←\n",
    "action 1: ↓\n",
    "action 2: →\n",
    "action 3: ↑"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.step(1)\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a few probabilities that you achieved the desired goal.\n",
    "This is due to the stochasticity of the environment.\n",
    "This environment provides an entire description of its transition model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = 0\n",
    "env.env.P[state]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here you can see the transitions probabilities for every possible action.\n",
    "For example, taking action 2 has:\n",
    "* 33% chances to lead to state 8\n",
    "* 33% chances to lead to state 1\n",
    "* 33% chances to lead to state 0\n",
    "\n",
    "Each line is composed as follow: (probability, next state, reward, end of episode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now try to solve the FrozenLake problem using the value policy algorithm.\n",
    "![](images/policy_iter.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fill the following code to implement the policy iteration algorithm:  \n",
    "(hint:  ```env.env.P[s][a]``` returns a list  \n",
    "$$[(p_1, s'_1, r_1, done)\\\\  \n",
    "...\\\\\n",
    "(p_n, s'_n, r_n, done)]$$\n",
    "Iterate on this list to compute $\\sum_{s',r}p(s',r|s,a)[r+\\gamma V(s')]$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_sum(env, V, s, a, gamma):\n",
    "    # V is a list containing the estimated value for every state\n",
    "    # len(V) = nb_states\n",
    "    total = 0  # state value for state s\n",
    "    for p, s_prime, r, _ in env.env.P[s][a]:\n",
    "        total += p * (r + gamma * V[s_prime])\n",
    "    return total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "\n",
    "def policy_iteration(env, gamma, theta):\n",
    "    nb_states = env.observation_space.n\n",
    "    nb_actions = env.action_space.n\n",
    "    # 1. Initialization\n",
    "    V = np.zeros(nb_states)\n",
    "    pi = np.zeros(nb_states)\n",
    "    \n",
    "    while True:\n",
    "    \n",
    "        # 2. Policy Evaluation\n",
    "        while True:\n",
    "            delta = 0\n",
    "            for s in range(nb_states):\n",
    "                v = V[s]\n",
    "                V[s] = compute_sum(env, V=V, s=s, a=pi[s], gamma=gamma)\n",
    "                delta = max(delta, abs(v - V[s]))\n",
    "            if delta < theta: break\n",
    "\n",
    "        # 3. Policy Improvement\n",
    "        policy_stable = True\n",
    "        for s in range(nb_states):\n",
    "            old_action = pi[s]\n",
    "            pi[s] = np.argmax([compute_sum(env, V=V, s=s, a=a, gamma=gamma) for a in range(nb_actions)])\n",
    "            if old_action != pi[s]: policy_stable = False\n",
    "        if policy_stable: break\n",
    "    return V, pi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run your algorithm on FrozenLake (you may use $\\gamma=1$ and $\\theta=1e-8$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "V, pi = policy_iteration(env, gamma=1.0, theta=1e-8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at the computed values and policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.color_palette(\"YlOrBr\", as_cmap=True)\n",
    "sns.heatmap(V.reshape([8, -1]), cmap=\"coolwarm\", annot=True)\n",
    "policy = np.array([action_map[x] for x in pi]).reshape([-1, 8])\n",
    "print((policy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see that the policy on state 55(tile (6,7)) and 62(tile (6,6)) appears different from the one we would expect.\n",
    "Using ```env.env.P[s]``` to check the environment's transitions, can you explain this behavior and why state 55 has a low value?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.env.s = 62\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.env.P[62]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Value Iteration:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now try to solve the FrozenLake problem using the value iteration algorithm.\n",
    "![](images/value_iteration.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fill the following code to implement the value iteration algorithm:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration(env, gamma, theta):\n",
    "    nb_states = env.observation_space.n\n",
    "    nb_actions = env.action_space.n\n",
    "    V = np.zeros(nb_states)\n",
    "    \n",
    "    while True:\n",
    "        delta = 0\n",
    "        for s in range(nb_states):\n",
    "            v = V[s]\n",
    "            V[s] = max([compute_sum(env, V=V, s=s, a=a, gamma=gamma) for a in range(nb_actions)])            \n",
    "            delta = max(delta, abs(v - V[s]))\n",
    "    \n",
    "        if delta < theta: break\n",
    "\n",
    "    # Output a deterministic policy\n",
    "    pi = np.zeros(nb_states)\n",
    "    for s in range(nb_states):\n",
    "        pi[s] = np.argmax([compute_sum(env, V=V, s=s, a=a, gamma=gamma) for a in range(nb_actions)])\n",
    "    \n",
    "    return V, pi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run your algorithm on FrozenLake (you may use $\\gamma=1$ and $\\theta=1e-8$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "V, pi = value_iteration(env, gamma=1.0, theta=1e-8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at the computed values and policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.color_palette(\"YlOrBr\", as_cmap=True)\n",
    "sns.heatmap(V.reshape([8, -1]), cmap=\"coolwarm\", annot=True)\n",
    "policy = np.array([action_map[x] for x in pi]).reshape([-1, 8])\n",
    "print((policy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this practical session, we trained agents to solve the FrozenLake environment without ever interacting with it.\n",
    "The Policy iteration and Value iteration algorithms are model-based algorithms. They can only be computed if we know the transitions probabilities used in the environment. This may not always be the case.\n",
    "In the following practical session, you will train an RL agent by interacting with the environment using a model-free algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
