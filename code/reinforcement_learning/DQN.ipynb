{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qtsM5fhPgEuc"
      },
      "source": [
        "# Practical Session DQN\n",
        "\n",
        "In this practical session, you will implement the famous DQN algorithm and test it in various environments.\n",
        "\n",
        "Run the following two cells if you are using Google Colab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V3DsN79PDxHj",
        "outputId": "f81533a1-5f49-459b-8f02-f2bf866e3df7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reading package lists...\n",
            "Building dependency tree...\n",
            "Reading state information...\n",
            "The following additional packages will be installed:\n",
            "  libxxf86dga1\n",
            "Suggested packages:\n",
            "  mesa-utils\n",
            "The following NEW packages will be installed:\n",
            "  libxxf86dga1 x11-utils xvfb\n",
            "0 upgraded, 3 newly installed, 0 to remove and 37 not upgraded.\n",
            "Need to get 994 kB of archives.\n",
            "After this operation, 2,981 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/main amd64 libxxf86dga1 amd64 2:1.1.4-1 [13.7 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic/main amd64 x11-utils amd64 7.7+3build1 [196 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 xvfb amd64 2:1.19.6-1ubuntu4.9 [784 kB]\n",
            "Fetched 994 kB in 0s (10.4 MB/s)\n",
            "Selecting previously unselected package libxxf86dga1:amd64.\r\n",
            "(Reading database ... \r(Reading database ... 5%\r(Reading database ... 10%\r(Reading database ... 15%\r(Reading database ... 20%\r(Reading database ... 25%\r(Reading database ... 30%\r(Reading database ... 35%\r(Reading database ... 40%\r(Reading database ... 45%\r(Reading database ... 50%\r(Reading database ... 55%\r(Reading database ... 60%\r(Reading database ... 65%\r(Reading database ... 70%\r(Reading database ... 75%\r(Reading database ... 80%\r(Reading database ... 85%\r(Reading database ... 90%\r(Reading database ... 95%\r(Reading database ... 100%\r(Reading database ... 155222 files and directories currently installed.)\r\n",
            "Preparing to unpack .../libxxf86dga1_2%3a1.1.4-1_amd64.deb ...\r\n",
            "Unpacking libxxf86dga1:amd64 (2:1.1.4-1) ...\r\n",
            "Selecting previously unselected package x11-utils.\r\n",
            "Preparing to unpack .../x11-utils_7.7+3build1_amd64.deb ...\r\n",
            "Unpacking x11-utils (7.7+3build1) ...\r\n",
            "Selecting previously unselected package xvfb.\r\n",
            "Preparing to unpack .../xvfb_2%3a1.19.6-1ubuntu4.9_amd64.deb ...\r\n",
            "Unpacking xvfb (2:1.19.6-1ubuntu4.9) ...\r\n",
            "Setting up xvfb (2:1.19.6-1ubuntu4.9) ...\r\n",
            "Setting up libxxf86dga1:amd64 (2:1.1.4-1) ...\r\n",
            "Setting up x11-utils (7.7+3build1) ...\r\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\r\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1.3) ...\r\n",
            "/sbin/ldconfig.real: /usr/local/lib/python3.7/dist-packages/ideep4py/lib/libmkldnn.so.0 is not a symbolic link\r\n",
            "\r\n",
            "Requirement already satisfied: gym[box2d]==0.17.* in /usr/local/lib/python3.7/dist-packages (0.17.3)\n",
            "Collecting pyvirtualdisplay==0.2.*\n",
            "  Downloading PyVirtualDisplay-0.2.5-py2.py3-none-any.whl (13 kB)\n",
            "Requirement already satisfied: PyOpenGL==3.1.* in /usr/local/lib/python3.7/dist-packages (3.1.5)\n",
            "Collecting PyOpenGL-accelerate==3.1.*\n",
            "  Downloading PyOpenGL-accelerate-3.1.5.tar.gz (538 kB)\n",
            "Collecting EasyProcess\n",
            "  Downloading EasyProcess-0.3-py2.py3-none-any.whl (7.9 kB)\n",
            "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym[box2d]==0.17.*) (1.3.0)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.7/dist-packages (from gym[box2d]==0.17.*) (1.19.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from gym[box2d]==0.17.*) (1.4.1)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from gym[box2d]==0.17.*) (1.5.0)\n",
            "Collecting box2d-py~=2.3.5\n",
            "  Downloading box2d_py-2.3.8-cp37-cp37m-manylinux1_x86_64.whl (448 kB)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym[box2d]==0.17.*) (0.16.0)\n",
            "Building wheels for collected packages: PyOpenGL-accelerate\n",
            "  Building wheel for PyOpenGL-accelerate (setup.py): started\n",
            "  Building wheel for PyOpenGL-accelerate (setup.py): finished with status 'done'\n",
            "  Created wheel for PyOpenGL-accelerate: filename=PyOpenGL_accelerate-3.1.5-cp37-cp37m-linux_x86_64.whl size=1599536 sha256=7e9d8530e86ff175bfd6d4d2c05d6ae9a69d686517bfa9ce8e943c02a90c5507\n",
            "  Stored in directory: /root/.cache/pip/wheels/1c/f5/6f/169afb3f2d476c5e807f8515b3c9bc9b819c3962316aa804eb\n",
            "Successfully built PyOpenGL-accelerate\n",
            "Installing collected packages: EasyProcess, box2d-py, pyvirtualdisplay, PyOpenGL-accelerate\n",
            "Successfully installed EasyProcess-0.3 PyOpenGL-accelerate-3.1.5 box2d-py-2.3.8 pyvirtualdisplay-0.2.5\n"
          ]
        }
      ],
      "source": [
        "%%bash\n",
        "\n",
        "# install required system dependencies\n",
        "apt-get install -y xvfb x11-utils\n",
        "\n",
        "# install required python dependencies (might need to install additional gym extras depending)\n",
        "pip install gym[box2d]==0.17.* pyvirtualdisplay==0.2.* PyOpenGL==3.1.* PyOpenGL-accelerate==3.1.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "gew6ipsBD1Yo"
      },
      "outputs": [],
      "source": [
        "import pyvirtualdisplay\n",
        "import Box2D\n",
        "\n",
        "_display = pyvirtualdisplay.Display(visible=False,  # use False with Xvfb\n",
        "                                    size=(1400, 900))\n",
        "_ = _display.start()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PnWbiTnogjch"
      },
      "source": [
        "# Lunar Lander\n",
        "[Lunar Lander](https://gym.openai.com/envs/LunarLander-v2/) is a reinforcement learning toy environment provided with the Gym Library.  \n",
        "In this environment, you control the reactors of a Lander that tries to land inside a landing area as smoothly as possible. \n",
        "The official environment description is  \n",
        "*Landing pad is always at coordinates (0,0). Coordinates are the first two numbers in state vector. Reward for moving from the top of the screen to landing pad and zero speed is about 100..140 points. If lander moves away from landing pad it loses reward back. Episode finishes if the lander crashes or comes to rest, receiving additional -100 or +100 points. Each leg ground contact is +10. Firing main engine is -0.3 points each frame. Solved is 200 points. Landing outside landing pad is possible. Fuel is infinite, so an agent can learn to fly and then land on its first attempt. Four discrete actions available: do nothing, fire left orientation engine, fire main engine, fire right orientation engine.*\n",
        "\n",
        "The environment will provide observations to the agent in the form of an 8-dimensional vector composed of:\n",
        "* Position X\n",
        "* Position Y\n",
        "* Velocity X\n",
        "* Velocity Y\n",
        "* Angle\n",
        "* Angular Velocity\n",
        "* Is left leg touching the ground: 0 OR 1\n",
        "* Is right leg touching the ground: 0 OR 1  \n",
        "\n",
        "The agent can choose between four discrete actions:\n",
        "* 0 = Do Nothing\n",
        "* 1 = Fire Left Engine\n",
        "* 2 = Fire Main Engine\n",
        "* 3 = Fire Right Engine  \n",
        "\n",
        "At every transition, the agent will be rewarded by the environment according to the following reward function:\n",
        "\n",
        "* [100, 140] points for Moving to the landing pad and zero speed\n",
        "* Negative reward for moving away from the landing pad\n",
        "* If the lander crashes or comes to rest, it gets -100 or +100\n",
        "* Each leg with ground contact gets +10\n",
        "* Firing the main engine is -0.3 per frame\n",
        "* Firing the side engine is -0.03 per frame\n",
        "* 200 points for solving the environment\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Yaa8cSiXy-w"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import sys\n",
        "from time import time\n",
        "from collections import deque, defaultdict, namedtuple\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import gym\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import seaborn as sns\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import imageio\n",
        "from IPython.display import Image\n",
        "from tqdm.notebook import tqdm\n",
        "from IPython.display import clear_output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yx7AYLSnX7PE"
      },
      "outputs": [],
      "source": [
        "env = gym.make('LunarLander-v2')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GfLhHuhJYA3W"
      },
      "outputs": [],
      "source": [
        "print(env.action_space)\n",
        "print(env.observation_space)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YQpj2vkyivwL"
      },
      "source": [
        "Here is a little code snippet to visualise a random agent playing Lunar Lander"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bJbp4SnIDuEj"
      },
      "outputs": [],
      "source": [
        "images = []\n",
        "obs = env.reset()\n",
        "img = env.render(mode='rgb_array')\n",
        "\n",
        "while True:\n",
        "    # At each step, append an image to list\n",
        "    images.append(img)\n",
        "    action = env.action_space.sample()\n",
        "    obs, _, done ,_ = env.step(action)\n",
        "    img = env.render(mode='rgb_array')\n",
        "    if done:\n",
        "        break\n",
        "    \n",
        "env.close()\n",
        "imageio.mimwrite('./lunar_random.gif',\n",
        "                [np.array(img) for i, img in enumerate(images) if i%2 == 0],\n",
        "                fps=29)\n",
        "\n",
        "Image(open('lunar_random.gif','rb').read())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XWcGoe9ZjzF4"
      },
      "source": [
        "We will now implement and train a DQN agent.\n",
        "Fill the following code to create a neural network class that our agent will use.\n",
        "The network should be small enough not to slow down the training process.  \n",
        "You can use a two hidden layers neural network with 512 and 256 hidden neurons using a ReLU activation function and a final layer with as many neurons as the environment action space using a linear activation function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9lnVAsTZgOnF"
      },
      "outputs": [],
      "source": [
        "class QNetwork(nn.Module):\n",
        "    def __init__(self, state_size, action_size):\n",
        "        super(QNetwork, self).__init__()\n",
        "        self.fc1 = ...\n",
        "        self.fc2 = ...\n",
        "        self.fc3 = ... \n",
        "        \n",
        "    def forward(self, x):\n",
        "        ...\n",
        "        return ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pA_sdNSqj224"
      },
      "source": [
        "The following class implements a replay buffer that will act as our agent memory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EQS7b95VgPlN"
      },
      "outputs": [],
      "source": [
        "class ReplayBuffer:\n",
        "    def __init__(self, buffer_size, batch_size):\n",
        "        \"\"\"\n",
        "        Replay memory allow agent to record experiences and learn from them\n",
        "        \n",
        "        Parametes\n",
        "        ---------\n",
        "        buffer_size (int): maximum size of internal memory\n",
        "        batch_size (int): sample size from experience\n",
        "        \"\"\"\n",
        "        self.batch_size = batch_size\n",
        "        self.memory = deque(maxlen=buffer_size)\n",
        "        self.experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
        "    \n",
        "    def add(self, state, action, reward, next_state, done):\n",
        "        \"\"\"Add experience\"\"\"\n",
        "        experience = self.experience(state, action, reward, next_state, done)\n",
        "        self.memory.append(experience)\n",
        "                \n",
        "    def sample(self):\n",
        "        \"\"\" \n",
        "        Sample randomly and return (state, action, reward, next_state, done) tuple as torch tensors \n",
        "        \"\"\"\n",
        "        experiences = random.sample(self.memory, k=self.batch_size)\n",
        "        \n",
        "        # Convert to torch tensors\n",
        "        states = torch.from_numpy(np.vstack([experience.state for experience in experiences if experience is not None])).float().to(device)\n",
        "        actions = torch.from_numpy(np.vstack([experience.action for experience in experiences if experience is not None])).long().to(device)        \n",
        "        rewards = torch.from_numpy(np.vstack([experience.reward for experience in experiences if experience is not None])).float().to(device)        \n",
        "        next_states = torch.from_numpy(np.vstack([experience.next_state for experience in experiences if experience is not None])).float().to(device)  \n",
        "        # Convert done from boolean to int\n",
        "        dones = torch.from_numpy(np.vstack([experience.done for experience in experiences if experience is not None]).astype(np.uint8)).float().to(device)        \n",
        "        \n",
        "        return (states, actions, rewards, next_states, dones)\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.memory)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2-YTCBEQDuEm"
      },
      "source": [
        "# DQN\n",
        "The fllowing class implements a DQN agent.  \n",
        "Its training procedure is located in the `train` method.  \n",
        "Fill the missing parts of the `act` and `learn` methods according to the DQN algorithm:\n",
        "![](https://github.com/wikistat/AI-Frameworks/blob/website/code/reinforcement_learning/images/dqn.png?raw=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RwxcpmGZgTEs"
      },
      "outputs": [],
      "source": [
        "LR = 1e-3               # Q Network learning rate\n",
        "EPS_DECAY = 0.999    # Epsilon decay rate\n",
        "EPS_MIN = 0.01  #min Epsilon\n",
        "\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "\n",
        "def print_running_mean(training_rewards):\n",
        "    clear_output(wait=True)\n",
        "    plt.figure(figsize=(15,3))\n",
        "    plt.plot(pd.Series(training_rewards).rolling(10).mean())\n",
        "    plt.title(\"Rewards running mean on last 100 episodes\")\n",
        "    plt.show()\n",
        "\n",
        "class DQNAgent:\n",
        "    def __init__(self, env, gamma=0.99, memory_size=100000, batch_size=256, update_rate=4):\n",
        "        self.env =  env\n",
        "        self.gamma = gamma\n",
        "        self.state_size = env.observation_space.shape[0]\n",
        "        self.action_size = env.action_space.n\n",
        "        self.update_rate = 4\n",
        "        self.batch_size = batch_size\n",
        "        self.q_network = QNetwork(self.state_size, self.action_size).to(device)\n",
        "        self.optimizer = optim.Adam(self.q_network.parameters(), lr=0.001, weight_decay=1e-6)\n",
        "        # Initiliase memory \n",
        "        self.memory = ReplayBuffer(memory_size, batch_size)\n",
        "        self.timestep = 0\n",
        "        self.epsilon = 1\n",
        "        \n",
        "                \n",
        "    def train(self, max_steps):\n",
        "        self.timestep = 0\n",
        "        self.epsilon = 1\n",
        "        \n",
        "        state = self.env.reset()\n",
        "        current_reward = 0\n",
        "        episode_rewards = []\n",
        "        for _ in range(max_steps):\n",
        "            action = self.act(state, train=True)\n",
        "            next_state, reward, done, info = self.env.step(action)\n",
        "            current_reward += reward\n",
        "            self.memory.add(state, action, reward, next_state, done)\n",
        "            self.timestep += 1\n",
        "            if (self.timestep % self.update_rate == 0) & (len(self.memory) > self.batch_size):\n",
        "                self.learn()\n",
        "                \n",
        "            self.epsilon = max(self.epsilon * EPS_DECAY, EPS_MIN)\n",
        "            if done:\n",
        "                state = self.env.reset()\n",
        "                episode_rewards.append(current_reward)\n",
        "                current_reward = 0\n",
        "                \n",
        "            else:\n",
        "                state = next_state\n",
        "            if self.timestep % 1000 == 0:\n",
        "                print_running_mean(episode_rewards)\n",
        "                \n",
        "                \n",
        "    def learn(self):\n",
        "        experiences_batch = ... #get a minibatch from the replay buffer\n",
        "        states, actions, rewards, next_states, dones = experiences_batch\n",
        "        # Get the action with max Q value\n",
        "        action_values = self.q_network(next_states).detach() #Do you know why is this detach important?\n",
        "        max_action_values = action_values.max(1)[0].unsqueeze(1)\n",
        "        \n",
        "        # If done just use reward, else update Q_target with discounted action values\n",
        "        Q_target = rewards + (self.gamma * max_action_values * (1 - dones))\n",
        "        Q_expected = self.q_network(states).gather(1, actions)\n",
        "        \n",
        "        loss = ...\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "        \n",
        "        \n",
        "    def act(self, state, train=False):\n",
        "\n",
        "        if (random.uniform(0, 1) < self.epsilon) & train:\n",
        "            return ...\n",
        "        else:\n",
        "            state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
        "            with torch.no_grad():\n",
        "                action_values = ...#the predicted Qvalues\n",
        "            action = np.argmax(action_values.cpu().data.numpy())\n",
        "            return action    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g4TBIZ8jlcIV"
      },
      "source": [
        "Instanciate a DQN agent and train it on lunar lander for at least 100000 steps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-lMIifWGDuEp"
      },
      "outputs": [],
      "source": [
        "dqn_agent = ...\n",
        "..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fna62-OWoLsg"
      },
      "source": [
        "Complete the following code to visualize your agent playing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZH2su8-2DuEp"
      },
      "outputs": [],
      "source": [
        "obs = env.reset()\n",
        "img = env.render(mode='rgb_array')\n",
        "\n",
        "while True:\n",
        "    # At each step, append an image to list\n",
        "    images.append(img)\n",
        "    action = ...\n",
        "    obs, _, done ,_ = env.step(action)\n",
        "    img = env.render(mode='rgb_array')\n",
        "    if done:\n",
        "        break\n",
        "    \n",
        "env.close()\n",
        "imageio.mimwrite('./lunar_dqn.gif',\n",
        "                [np.array(img) for i, img in enumerate(images) if i%2 == 0],\n",
        "                fps=29)\n",
        "\n",
        "Image(open('lunar_dqn.gif','rb').read())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dVUOs7jll5Xs"
      },
      "source": [
        "An important impovement of DQN is the use of a target network.  \n",
        "Modify your DQN agent to use a secondary target network.  \n",
        "![](https://github.com/wikistat/AI-Frameworks/blob/website/code/reinforcement_learning/images/dqn_er.png?raw=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y3b-UoJCDuEr"
      },
      "outputs": [],
      "source": [
        "class DQNTargetAgent(DQNAgent):\n",
        "    def __init__(self, env, gamma=0.99, memory_size=100000, batch_size=256, update_rate=4, target_update_rate=1000):\n",
        "        super().__init__(env, gamma, memory_size, batch_size, update_rate)\n",
        "        self.target_update_rate = target_update_rate\n",
        "        self.target_network = ....to(device)\n",
        "        self.target_network.load_state_dict(...) #initialise the target network weights with the policy network's weights\n",
        "        \n",
        "    def learn(self):\n",
        "        experiences_batch = self.memory.sample()\n",
        "        states, actions, rewards, next_states, dones = experiences_batch\n",
        "        # Get the action with max Q value\n",
        "        action_values = ... #Use the target network\n",
        "        max_action_values = action_values.max(1)[0].unsqueeze(1)\n",
        "        \n",
        "        # If done just use reward, else update Q_target with discounted action values\n",
        "        Q_target = rewards + (self.gamma * max_action_values * (1 - dones))\n",
        "        Q_expected = ...\n",
        "        \n",
        "        loss = ...\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "        if self.timestep % self.target_update_rate == 0: #update the target network weights with the policy network's weights\n",
        "            ...  \n",
        "        "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jgxf30dhvs-R"
      },
      "source": [
        "Train a dqn agent with target network."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ieI4TJEGghuF"
      },
      "outputs": [],
      "source": [
        "dqn_target_agent = ...\n",
        "..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6IHzWpU7of98"
      },
      "source": [
        "Visualize your agent playing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RYzJQmz0DuEr"
      },
      "outputs": [],
      "source": [
        "obs = env.reset()\n",
        "img = env.render(mode='rgb_array')\n",
        "\n",
        "while True:\n",
        "    # At each step, append an image to list\n",
        "    images.append(img)\n",
        "    action = ...\n",
        "    obs, _, done ,_ = env.step(...)\n",
        "    img = env.render(mode='rgb_array')\n",
        "    if done:\n",
        "        break\n",
        "    \n",
        "env.close()\n",
        "imageio.mimwrite('./lunar_target.gif',\n",
        "                [np.array(img) for i, img in enumerate(images) if i%2 == 0],\n",
        "                fps=29)\n",
        "\n",
        "Image(open('lunar_target.gif','rb').read())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fBK075MfokmX"
      },
      "source": [
        "The following code compares all the agents."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bC84Y4FZDuEs"
      },
      "outputs": [],
      "source": [
        "max_episodes = 100\n",
        "\n",
        "df = pd.DataFrame(columns=['rewards', 'agent'])\n",
        "random_fcn = lambda x: np.random.choice(env.action_space.n)\n",
        "dqn_fcn = lambda x: dqn_agent.act(x)\n",
        "target_dqn_fcn = lambda x: dqn_target_agent.act(x)\n",
        "\n",
        "for agent, fcn in zip(['random', 'dqn', 'target_dqn'], [random_fcn, dqn_fcn, target_dqn_fcn]):\n",
        "    scores = []\n",
        "    for i in tqdm(range(max_episodes)):\n",
        "        state = env.reset()\n",
        "        score = 0\n",
        "        while True:\n",
        "            action = fcn(state)\n",
        "            state, reward, done, info = env.step(action)\n",
        "            score += reward\n",
        "            if done:\n",
        "                break\n",
        "\n",
        "        scores.append(score)\n",
        "    for score in scores:\n",
        "        df = df.append({'rewards': score, 'agent': agent}, ignore_index=True)\n",
        "\n",
        "sns.boxplot(x=\"agent\", y=\"rewards\",data=df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nVoMC79Xi81N"
      },
      "source": [
        "# Cartpole\n",
        "\n",
        "CartPole is another [toy control environment avalible in OpeAI Gym](https://gym.openai.com/envs/CartPole-v0/). \n",
        "*A pole is attached by an un-actuated joint to a cart, which moves along a frictionless track. The system is controlled by applying a force of +1 or -1 to the cart. The pendulum starts upright, and the goal is to prevent it from falling over. A reward of +1 is provided for every timestep that the pole remains upright. The episode ends when the pole is more than 15 degrees from vertical, or the cart moves more than 2.4 units from the center.* \n",
        "\n",
        "It is also known as the inverted pendulum problem.  \n",
        "In this environment the objective is to balance a pole as long as possible. It is assumed that at the tip of the pole, there is an object which makes it unstable and very likely to fall over. The agent controls the cart and must move left and right so that the pole can stand as long as possible."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GePTgwYFDuEs"
      },
      "outputs": [],
      "source": [
        "env = gym.make('CartPole-v1')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tVI6Rhv1DuEt"
      },
      "outputs": [],
      "source": [
        "images = []\n",
        "obs = env.reset()\n",
        "img = env.render(mode='rgb_array')\n",
        "\n",
        "while True:\n",
        "    # At each step, append an image to list\n",
        "    images.append(img)\n",
        "    action = env.action_space.sample()\n",
        "    obs, _, done ,_ = env.step(action)\n",
        "    img = env.render(mode='rgb_array')\n",
        "    if done:\n",
        "        break\n",
        "    \n",
        "env.close()\n",
        "imageio.mimwrite('./cartpole_random.gif',\n",
        "                [np.array(img) for i, img in enumerate(images) if i%2 == 0],\n",
        "                fps=29)\n",
        "\n",
        "Image(open('cartpole_random.gif','rb').read())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CDMbTPmrp2LV"
      },
      "source": [
        "Train a dqn agent to solve cartpole."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "shLtAxhki8Da"
      },
      "outputs": [],
      "source": [
        "dqn_agent = ...\n",
        "..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SWQSCBQpDuEu"
      },
      "outputs": [],
      "source": [
        "obs = env.reset()\n",
        "img = env.render(mode='rgb_array')\n",
        "\n",
        "while True:\n",
        "    # At each step, append an image to list\n",
        "    images.append(img)\n",
        "    action = dqn_agent.act(obs)\n",
        "    obs, _, done ,_ = env.step(action)\n",
        "    img = env.render(mode='rgb_array')\n",
        "    if done:\n",
        "        break\n",
        "    \n",
        "env.close()\n",
        "imageio.mimwrite('./cartpole_dqn.gif',\n",
        "                [np.array(img) for i, img in enumerate(images) if i%2 == 0],\n",
        "                fps=29)\n",
        "\n",
        "Image(open('cartpole_dqn.gif','rb').read())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "00SrU1GwqESW"
      },
      "source": [
        "Train a dqn agent to solve cartpole using target network and compare its performances with the simple DQN."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "496w98bDDuEu"
      },
      "outputs": [],
      "source": [
        "dqn_target_agent = ...#you may update the target network every 500 steps\n",
        "..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y4Z65WQtDuEv"
      },
      "outputs": [],
      "source": [
        "obs = env.reset()\n",
        "img = env.render(mode='rgb_array')\n",
        "\n",
        "while True:\n",
        "    # At each step, append an image to list\n",
        "    images.append(img)\n",
        "    action = dqn_target_agent.act(obs)\n",
        "    obs, _, done ,_ = env.step(action)\n",
        "    img = env.render(mode='rgb_array')\n",
        "    if done:\n",
        "        break\n",
        "    \n",
        "env.close()\n",
        "imageio.mimwrite('./cartpole_target.gif',\n",
        "                [np.array(img) for i, img in enumerate(images) if i%2 == 0],\n",
        "                fps=29)\n",
        "\n",
        "Image(open('cartpole_target.gif','rb').read())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QgzubhZHDuEv"
      },
      "outputs": [],
      "source": [
        "max_episodes = 100\n",
        "\n",
        "df = pd.DataFrame(columns=['rewards', 'agent'])\n",
        "random_fcn = lambda x: np.random.choice(env.action_space.n)\n",
        "dqn_fcn = lambda x: dqn_agent.act(x)\n",
        "target_dqn_fcn = lambda x: dqn_target_agent.act(x)\n",
        "\n",
        "for agent, fcn in zip(['random', 'dqn', 'target_dqn'], [random_fcn, dqn_fcn, target_dqn_fcn]):\n",
        "    scores = []\n",
        "    for i in tqdm(range(max_episodes)):\n",
        "        state = env.reset()\n",
        "        score = 0\n",
        "        while True:\n",
        "            action = fcn(state)\n",
        "            state, reward, done, info = env.step(action)\n",
        "            score += reward\n",
        "            if done:\n",
        "                break\n",
        "\n",
        "        scores.append(score)\n",
        "    for score in scores:\n",
        "        df = df.append({'rewards': score, 'agent': agent}, ignore_index=True)\n",
        "\n",
        "sns.boxplot(x=\"agent\", y=\"rewards\",data=df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V7wlteZoGdij"
      },
      "source": [
        "# Stable-baselines3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OUQSNcugqO0k"
      },
      "source": [
        "Stable Baselines3 (SB3) is a set of reliable implementations of reinforcement learning algorithms in PyTorch. It contains efficient implementations of the most famous RL algorithm and is certainly one of the most simple way to prototype an RL agent.  \n",
        "A full description of the librairy is available [here](https://stable-baselines3.readthedocs.io/en/master/)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tDrQonYsIJc-"
      },
      "outputs": [],
      "source": [
        "!pip install stable-baselines3[extra]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a9_1m6EYrBG5"
      },
      "source": [
        "Help yourself with the official documentation and train a dqn agent using Stable baselines3 on Lunar lander."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_mvUTfx3Gg5s"
      },
      "outputs": [],
      "source": [
        "import gym\n",
        "\n",
        "from stable_baselines3 import DQN\n",
        "from stable_baselines3.common.evaluation import evaluate_policy\n",
        "\n",
        "\n",
        "# Create environment\n",
        "env = ...\n",
        "\n",
        "# Instantiate the agent\n",
        "model = DQN(...)\n",
        "# Train the agent\n",
        "...\n",
        "\n",
        "# Evaluate the agent\n",
        "mean_reward, std_reward = evaluate_policy(model, model.get_env(), n_eval_episodes=10)\n",
        "\n",
        "# Enjoy trained agent\n",
        "obs = env.reset()\n",
        "for i in range(1000):\n",
        "    action, _states = model.predict(obs, deterministic=True)\n",
        "    obs, rewards, dones, info = env.step(action)\n",
        "    env.render()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ACI1HpUcweXb"
      },
      "source": [
        "Compare its performance with our implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b6D0qRYrMF3i"
      },
      "outputs": [],
      "source": [
        "scores = []\n",
        "for i in tqdm(range(max_episodes)):\n",
        "    state = env.reset()\n",
        "    score = 0\n",
        "    while True:\n",
        "        action, _states = model.predict(obs, deterministic=True)\n",
        "        state, reward, done, info = env.step(action)\n",
        "        score += reward\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "    scores.append(score)\n",
        "for score in scores:\n",
        "    df = df.append({'rewards': score, 'agent': 'stable_baseline'}, ignore_index=True)\n",
        "\n",
        "sns.boxplot(x=\"agent\", y=\"rewards\",data=df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0vwCiho6MmzR"
      },
      "outputs": [],
      "source": [
        "obs = env.reset()\n",
        "img = env.render(mode='rgb_array')\n",
        "\n",
        "while True:\n",
        "    # At each step, append an image to list\n",
        "    images.append(img)\n",
        "    action, _states = model.predict(obs, deterministic=True)\n",
        "    obs, _, done ,_ = env.step(action)\n",
        "    img = env.render(mode='rgb_array')\n",
        "    if done:\n",
        "        break\n",
        "    \n",
        "env.close()\n",
        "imageio.mimwrite('./stable_baselines.gif',\n",
        "                [np.array(img) for i, img in enumerate(images) if i%2 == 0],\n",
        "                fps=29)\n",
        "\n",
        "Image(open('stable_baselines.gif','rb').read())"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "INSA_DQN.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
