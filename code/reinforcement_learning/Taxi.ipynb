{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practical Session: Q-Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this practical session, you will implement the famous [Q-Learning](https://link.springer.com/content/pdf/10.1007/BF00992698.pdf) algorithm and test it in various environments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Taxi V3\n",
    "\n",
    "The taxi problem was first introduced in [Hierarchical Reinforcement Learning with the MAXQ Value Function Decomposition](https://arxiv.org/abs/cs/9905014).  \n",
    "In this environment, the agent controls a taxi whose job is to pick up a passenger at one location and drop him off in his target location.  \n",
    "Dropping of the passenger to its destination leads to a +20 reward.  \n",
    "To encourage the agent to be as fast as possible, it receives a deceptive reward of -1 at each step.  \n",
    "It will also receive a -10 penalty reward if it takes an illegal 'pick-up' or drop-off' action.  \n",
    "Let's instantiate a Taxi-V3 environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "env = gym.make(\"Taxi-v3\")\n",
    "env.reset()\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The taxi is represented in <span style=\"color:yellow\">yellow</span> when empty and in <span style=\"color:green\">green</span> when full.  \n",
    "The passenger is represented in <span style=\"color:blue\">blue</span> and his destination in <span style=\"color:magenta\">magenta</span>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations**: There are 500 discrete states since there are 25 taxi positions, five possible passenger locations (including the case when the passenger is in the taxi), and four destination locations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"State Space {env.observation_space}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The agent controling the *taxi* has six discrete and deterministic possibles actions:  \n",
    "*  0: *move south*\n",
    "*  1: *move north*\n",
    "*  2: *move east*\n",
    "*  3: *move west*\n",
    "*  4: *pickup passenger*\n",
    "*  5: *drop off passenger*  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Action Space {env.action_space}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a little method to vizualize our taxi's trajectory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "from time import sleep\n",
    "\n",
    "def display_trajectory(frames):\n",
    "    for i, frame in enumerate(frames):\n",
    "        clear_output(wait=True)\n",
    "        print(frame['frame'])\n",
    "        print(f\"Timestep: {i + 1}\")\n",
    "        print(f\"State: {frame['state']}\")\n",
    "        print(f\"Reward: {frame['reward']}\")\n",
    "        sleep(.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code shows how to run an episode with an agent taking random actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = [] # for animation\n",
    "\n",
    "env.reset()\n",
    "while True:\n",
    "    # draw a random action from the action space\n",
    "    action = env.action_space.sample()\n",
    "    # the step method takes an action as input and returns 4 variables described in the OpenAI section\n",
    "    state, reward, done, info = env.step(action)\n",
    "    frames.append({\n",
    "        'frame': env.render(mode='ansi'),\n",
    "        'state': state,\n",
    "        'reward': reward\n",
    "        })\n",
    "    #if done is True then the episode is over\n",
    "    if done == True:\n",
    "        break\n",
    "        \n",
    "display_trajectory(frames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Human policy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We just saw a random policy. Let's now check on your policy.  \n",
    "Using the possible actions, make a little script to take the client to his destination and visualize it with the `display_frames` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = [] \n",
    "actions = [...]\n",
    "for a in actions:\n",
    "    ...\n",
    "\n",
    "display_trajectory(frames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q-learning\n",
    "Let's now try to solve the TaxiV3 problem using the Q-learing algorithm.\n",
    "![](images/q-learning.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fill the `q_learning` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import seaborn as sns\n",
    "\n",
    "def print_running_mean(training_rewards, i):\n",
    "    clear_output(wait=True)\n",
    "    plt.figure(figsize=(15,3))\n",
    "    plt.plot(pd.Series(training_rewards[:i]).rolling(100).mean())\n",
    "    plt.title(\"Rewards running mean on last 100 episodes\")\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "def q_learning(env, alpha, gamma, epsilon, nb_episodes):\n",
    "    nb_states = env.observation_space.n\n",
    "    nb_actions = env.action_space.n\n",
    "    q_table = np.zeros([nb_states, nb_actions])\n",
    "    training_rewards = np.zeros(nb_episodes)\n",
    "    for i in range(nb_episodes):\n",
    "        ...\n",
    "        while True:\n",
    "            ...\n",
    "\n",
    "            training_rewards[i] += reward\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            print_running_mean(training_rewards, i)\n",
    "\n",
    "    return q_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train your q-table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_episodes = 5000\n",
    "alpha = 0.1\n",
    "gamma = 0.99\n",
    "epsilon = 0.1\n",
    "\n",
    "q_table = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us have a look at the learned policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()\n",
    "frames = [] # for animation\n",
    "timesteps = 0\n",
    "while True:\n",
    "    ...\n",
    "        \n",
    "display_trajectory(frames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Frozen lake\n",
    "Now, try your algoithm on frozen lake."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "env = gym.make(\"FrozenLake8x8-v0\")\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_episodes = 20000\n",
    "alpha = 0.1\n",
    "gamma = 0.99\n",
    "epsilon = 0.1\n",
    "\n",
    "q_table = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your agent has probably not reached a satisfying policy.  \n",
    "This is due to the lack of exploration induced by the $\\epsilon$-greedy policy.\n",
    "Fixing $\\epsilon$ to 0.1 is not sufficient in Frozen Lake. The agent does not explore enougth and thus never receives a positive feedback.  \n",
    "A simple solution would be to use a decaying epsilon from 1 to 0.1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decay_rates = [0.9, 0.99, 0.9995, 0.9999]\n",
    "eps_min = 0.1\n",
    "plt.figure(figsize=(10,6))\n",
    "\n",
    "for decay_rate in decay_rates:\n",
    "    eps_list = []\n",
    "    test_eps = 1\n",
    "    for _ in range(nb_episodes):\n",
    "        test_eps = max(test_eps * decay_rate, eps_min)\n",
    "        eps_list.append(test_eps)          \n",
    "    \n",
    "    plt.plot(eps_list, label='decay rate: {}'.format(decay_rate))\n",
    "\n",
    "plt.title('Effect of various decay rates')\n",
    "plt.legend(loc='best')\n",
    "plt.xlabel('# of episodes')\n",
    "plt.ylabel('epsilon')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modify your `q_learning` function to decay the learning rate after each episode (don't forget to clip so it does not go below 0.1).\n",
    "Try your function with different values for `epsilon_decay`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_learning(env, alpha, gamma, epsilon_decay, nb_episodes):\n",
    "    epsilon = 1\n",
    "    min_epsilon = 0.1\n",
    "    nb_states = env.observation_space.n\n",
    "    nb_actions = env.action_space.n\n",
    "    q_table = np.zeros([nb_states, nb_actions])\n",
    "    training_rewards = np.zeros(nb_episodes)\n",
    "    for i in range(nb_episodes):\n",
    "        ...\n",
    "        ...\n",
    "        ...\n",
    "        if i % 100 == 0:\n",
    "            print_running_mean(training_rewards, i)\n",
    "            \n",
    "    return q_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_episodes = 20000\n",
    "alpha = 0.1\n",
    "gamma = 0.99\n",
    "epsilon_decay = 0.9999\n",
    "\n",
    "q_table = q_learning(env, alpha, gamma, epsilon_decay, nb_episodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us have a look at the learned policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()\n",
    "frames = [] # for animation\n",
    "timesteps = 0\n",
    "state = env.reset()\n",
    "while True:\n",
    "    ...\n",
    "        \n",
    "display_trajectory(frames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In these two environments, the State-space was discrete and small enough to learn the entire Q-table.\n",
    "In most cases, this State space is way too large to learn the Q-values for each state.  \n",
    "One solution is to learn an estimator of this Q-function.\n",
    "In the following practical session, you will adapt the Q-learning algorithm to uses neural networks as function approximators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
