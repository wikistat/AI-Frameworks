{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H-yecLFmPzgu"
      },
      "source": [
        "# Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wUdLkQK7pJeA"
      },
      "source": [
        "During this practical session, we will use on the [AG's corpus of news article]():  \n",
        "*AG News (AG’s News Corpus) is a subdataset of [AG's corpus](http://groups.di.unipi.it/~gulli/AG_corpus_of_news_articles.html) of news articles constructed by assembling titles and description fields of articles from the 4 largest classes (“World”, “Sports”, “Business”, “Sci/Tech”) of AG’s Corpus. The AG News contains 30,000 training and 1,900 test samples per class.*  \n",
        "\n",
        "Let's first download the dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OnrTq38nyFOe"
      },
      "outputs": [],
      "source": [
        "!wget https://github.com/mhjabreel/CharCnn_Keras/raw/master/data/ag_news_csv/train.csv 2>&1\n",
        "!wget https://github.com/mhjabreel/CharCnn_Keras/raw/master/data/ag_news_csv/test.csv 2>&1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UdBvst4SPEvR"
      },
      "source": [
        "The following code will load the dataset and add the label names in a new column."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pDcujJdlzUmg"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "traindf = pd.read_csv('train.csv', names=[\"label\", \"title\", \"text\"]).sample(2000)\n",
        "testdf = pd.read_csv('test.csv', names=[\"label\", \"title\", \"text\"]).sample(1000)\n",
        "\n",
        "traindf['label'] = traindf['label'] -1\n",
        "traindf['label_name'] = traindf.label.map({0:\"World\", 1:\"Sports\", 2:\"Business\", 3:\"Sci/Tech\"})\n",
        "testdf['label'] = testdf['label'] -1\n",
        "testdf['label_name'] = testdf.label.map({0:\"World\", 1:\"Sports\", 2:\"Business\", 3:\"Sci/Tech\"})\n",
        "testdf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MkKfKEK_QEKD"
      },
      "source": [
        "Start with a little bit of exploration:  \n",
        "Using the Wordcloud library and follow the example provided by the [documentation](https://amueller.github.io/word_cloud/auto_examples/simple.html#sphx-glr-auto-examples-simple-py) to plot the most common words in the corpus.  \n",
        "Do do so, you should starts by joining all documents within a single one. A simple way consits in using the ```\" \".join()``` function on a list of text..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xxD7rkJ6PVwD"
      },
      "outputs": [],
      "source": [
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "corpus = ...\n",
        "wordcloud = ...\n",
        "...\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nuDM_9hrROGw"
      },
      "source": [
        "By default Wordcloud removes the most common words used in english (and, or, the, a ...). It is still possible to provide a custom list. \n",
        "[NLTK](https://www.nltk.org/) is a powerfull library for natural language processing. It provides a several lists of stop words that can be used to clean text.  \n",
        "Even if it doesn't change the result here let's provide Wordcloud with a custom list of stopwords taken from NLTK."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LDlMwEAN3HoI"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english')) \n",
        "\n",
        "wordcloud = ...\n",
        "..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2N2-5hr1StNl"
      },
      "source": [
        "Now, plot a different wordcloud for every category in the dataset.  \n",
        "Are you capable of predicting the categories given only these wordclouds?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2KKCXmFsBoV1"
      },
      "outputs": [],
      "source": [
        "..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6gzHJWlcH3K1"
      },
      "source": [
        "# Bag-of-Words  \n",
        "We will now train different models to predict the category of these news articles.  \n",
        "We saw, during the course, a first approach called \"*bag of words*\".  \n",
        "BOW methods describe documents using counts or statistics on the words composing the documents. Once the bag-of-words is computed, documents are represented by vectors whose dimensions correspond to words present in the corpus vocabulary.  \n",
        "\n",
        "First, we will vectorize our documents using term frequencies.  \n",
        "Look at the documentation of [scikit-learn's CountVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) to encode the __text column__ of your training set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uHNFlYjJ4j35"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "vectorizer = ...\n",
        "..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qehLcwDuWw4C"
      },
      "source": [
        "The ```vocabulary_``` argument of your vectorizer contains a dictionary with all the tokens and their corresponding index in the bag-of-words.  \n",
        "How many unique tokens compose your bag-of-words?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y4X2Y4BtWgss"
      },
      "outputs": [],
      "source": [
        "..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gw4o9WKOXQ2J"
      },
      "source": [
        "You can also use the ```get_feature_names_out()``` method to get the list of identified tokens:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0XF5UcftWU0e"
      },
      "outputs": [],
      "source": [
        "..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Ft5A6sSXbn2"
      },
      "source": [
        "Now choose a classification method from scikit-learn and train it to classify news article.  \n",
        "Print the classification score of your model on the training set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "po7UX0IWWPez"
      },
      "outputs": [],
      "source": [
        "..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N2sM6gq0YZqq"
      },
      "source": [
        "Now use the ```transform``` method from your vectorizer on the testing set and print the score obtained by your model on the testing set.  \n",
        "Your model is probably overfitting a lot.  \n",
        "Plot a [consusion matrix](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.ConfusionMatrixDisplay.html) to see where your model makes the most mistakes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kttgaiO-YGch"
      },
      "outputs": [],
      "source": [
        "..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DnUlkffjZA9b"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MpnzHmSzcZwh"
      },
      "source": [
        "Play with some of the vectorizer hyper-parameters to see whether you can improve the perfomance of your classifier on the testing set.  \n",
        "Try adding stopwords or changing the ngram_range..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XpzL5IHvbR4J"
      },
      "outputs": [],
      "source": [
        "..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HgfYiPQVc1oP"
      },
      "source": [
        "Once you are satisfied with the performance or do not improve it, plot a t-SNE of your training representations with labels as colors.  \n",
        "In particular, compare the t-SNE representations computed with and without stop words.  \n",
        "What do you observe?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m5xoFzmJ4j3-"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.manifold import TSNE\n",
        "import seaborn as sns \n",
        "\n",
        "..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qrf-AXlAedDn"
      },
      "source": [
        "We will now use a second type of vectorization strategy, a little bit more efficient than pure term frequency: TF-idf.  \n",
        "What is the difference with the previous method?  \n",
        "Use [scikit-lear's ```TfidfVectorizer```](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html#sklearn.feature_extraction.text.TfidfVectorizer) to vectorize the documents in your corpus and train a classification algorithm to classify documents.  \n",
        "Print the score you obtain on the testing set and the corresponding confusion matrix.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f8mGFYzjUBdW"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hu0BqKauf0OO"
      },
      "source": [
        "Plot a t-SNE of the representations obtained using TF-IDF."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IWP7jE9uU2ep"
      },
      "outputs": [],
      "source": [
        "..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CrxLmFtbgofw"
      },
      "source": [
        "Both the ```TfidfVectorizer``` and ```CountVectorizer``` use a default strategy to create a token given a text using whitespaces and punctuations as separators.  \n",
        "It is possible to provide custom __tokenizers__ to these vectorizers.  \n",
        "Here we will use NLTK to build a more powerful tokenizer that will:\n",
        "\n",
        "*   Revmove stop words\n",
        "*   Convert all texts to lowercase\n",
        "*   Ignore punctuations symbols\n",
        "*   Only consider letters\n",
        "*   Perform Stemming on every token\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "34i_bNcpXRPr"
      },
      "outputs": [],
      "source": [
        "from nltk import word_tokenize          \n",
        "from nltk.stem import SnowballStemmer\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "import re\n",
        "\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "# Download stopwords list\n",
        "\n",
        "stop_words = set(stopwords.words('english')) \n",
        "\n",
        "# Interface lemma tokenizer from nltk with sklearn\n",
        "class StemTokenizer:\n",
        "    ignore_tokens = [',', '.', ';', ':', '\"', '``', \"''\", '`']\n",
        "    def __init__(self):\n",
        "        self.stemmer = SnowballStemmer('english')\n",
        "    def __call__(self, doc):\n",
        "        doc = doc.lower()\n",
        "        return [self.stemmer.stem(t) for t in word_tokenize(re.sub(\"[^a-z' ]\", \"\", doc)) if t not in self.ignore_tokens]\n",
        "\n",
        "tokenizer=StemTokenizer()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "shRu942Eh2Xx"
      },
      "source": [
        "Print an example of text from the dataset and the corresponding tokens computed by the tokenizer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fYSLXxsKW5QJ"
      },
      "outputs": [],
      "source": [
        "..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bzIMu7IliF4S"
      },
      "source": [
        "Now provide the tokenizer to the a ```TfidfVectorizer``` and repeat the entire process.  \n",
        "Does it improves the testing performance?  \n",
        "Tips: you should also provided a tokenized version of the stopwords since we apply stemming on all tokens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b87CgcLSW83f"
      },
      "outputs": [],
      "source": [
        "token_stop = tokenizer(' '.join(stop_words))\n",
        "\n",
        "tfidf = TfidfVectorizer(stop_words=token_stop, tokenizer=tokenizer)\n",
        "\n",
        "..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1QZyvBPcuwkU"
      },
      "source": [
        "It is also possible to combine bag-of-words features with other features manually computed.  \n",
        "The following code computes some new features on all documents."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rf9uWiWOj7qE"
      },
      "outputs": [],
      "source": [
        "def count_chars(text):\n",
        "    return len(text)\n",
        "\n",
        "def count_words(text):\n",
        "    return len(text.split())\n",
        "\n",
        "def count_capital_words(string):\n",
        "    return sum(map(str.isupper, string))\n",
        "\n",
        "def count_capital_words(text):\n",
        "  return sum(map(str.isupper,text.split()))\n",
        "\n",
        "def count_punctuations(text):\n",
        "  count = 0\n",
        "  for i in range (0, len (text)):   \n",
        "    if text[i] in ('!', \",\" ,\"\\'\" ,\";\" ,\"\\\"\", \".\", \"-\" ,\"?\"):  \n",
        "        count = count + 1; \n",
        "  return  count\n",
        "\n",
        "def count_sentences(text):\n",
        "    return len(nltk.sent_tokenize(text))\n",
        "\n",
        "def count_unique_words(text):\n",
        "    return len(set(text.split()))\n",
        "\n",
        "for df in [traindf, testdf]:\n",
        "  df['count_chars'] = df.text.apply(lambda s: count_chars(s))\n",
        "  df['count_words'] = df.text.apply(lambda s: count_words(s))\n",
        "  df['count_capital_words'] = df.text.apply(lambda s: count_capital_words(s))\n",
        "  df['count_capital_words'] = df.text.apply(lambda s: count_capital_words(s))\n",
        "  df['count_punctuations'] = df.text.apply(lambda s: count_punctuations(s))\n",
        "  df['count_sentences'] = df.text.apply(lambda s: count_sentences(s))\n",
        "  df['count_unique_words'] = df.text.apply(lambda s: count_unique_words(s))\n",
        "  df['avg_wordlength'] = df['count_chars']/df['count_words']\n",
        "  df['avg_sentlength'] = df['count_words']/df['count_sentences']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qa2oWqxAvDF3"
      },
      "source": [
        "Using a [```ColumnTransformer```](https://scikit-learn.org/stable/modules/generated/sklearn.compose.ColumnTransformer.html#sklearn.compose.ColumnTransformer) it is possible to combine all the features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Y5gtVtLow6l"
      },
      "outputs": [],
      "source": [
        "from sklearn.compose import ColumnTransformer\n",
        "\n",
        "columns_to_keep = ['text', 'count_chars', 'count_words',\n",
        "       'count_capital_words', 'count_punctuations',\n",
        "       'count_unique_words', 'count_sentences', 'avg_wordlength',\n",
        "       'avg_sentlength']\n",
        "\n",
        "column_trans = ColumnTransformer(\n",
        "    [('categories', TfidfVectorizer(stop_words=token_stop, tokenizer=tokenizer), 'text')],\n",
        "    remainder='passthrough', verbose_feature_names_out=False)\n",
        "\n",
        "X_train = column_trans.fit_transform(traindf[columns_to_keep])\n",
        "X_test = column_trans.transform(testdf[columns_to_keep])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iPMxdZ09vR1D"
      },
      "source": [
        "Unfortunatly in our case, these features do not provide any improvement on the testing performance.  \n",
        "In some other tasks like spam detection they can have a stronger influence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lQYdNCfgt_6A"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "rf = RandomForestClassifier()\n",
        "rf.fit(X_train, traindf.label)\n",
        "print(f\"Model score on training data: {rf.score(X_train, traindf.label):.2f}\")\n",
        "print(f\"Model score on test data: {rf.score(X_test, testdf.label)}\")\n",
        "\n",
        "predictions = rf.predict(X_test)\n",
        "cm = confusion_matrix(testdf.label, predictions)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"World\", \"Sports\", \"Business\", \"Sci/Tech\"])\n",
        "disp.plot()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e_CXJTABEkbF"
      },
      "source": [
        "# Word2Vec\n",
        "\n",
        "We will now use a second vectorization technique, seen during the course lectures: word vectorization.  \n",
        "Fisrt, we will use the Gensim library to compute or download pre-computed word embeddings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JPwMk70UFA98"
      },
      "outputs": [],
      "source": [
        "from gensim.test.utils import datapath, get_tmpfile\n",
        "from gensim.models import KeyedVectors\n",
        "from gensim.scripts.glove2word2vec import glove2word2vec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pRWa3c-nFDAF"
      },
      "outputs": [],
      "source": [
        "!wget https://nlp.stanford.edu/data/glove.6B.zip \n",
        "!unzip glove.6B.zip > /dev/null 2>&1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A7qKTMjBFdPM"
      },
      "outputs": [],
      "source": [
        "glove_file = ('glove.6B.100d.txt')\n",
        "word2vec_glove_file = get_tmpfile(\"glove.6B.100d.word2vec.txt\")\n",
        "glove2word2vec(glove_file, word2vec_glove_file)\n",
        "model = KeyedVectors.load_word2vec_format(word2vec_glove_file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Co4oebC2wB-B"
      },
      "source": [
        "The model is a mapping between words and their vector representations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HKvhNtS_HfIF"
      },
      "outputs": [],
      "source": [
        "model['apple']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c-dX8FhdwKkF"
      },
      "source": [
        "It also has usefull methods to explore the  vocabulary's embeddings.  \n",
        "Here are some examples to find the most similar words in the embedding space. \n",
        "Try with some other words and look if the most similar words seem plausibles."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WvPEk6oqFnmn"
      },
      "outputs": [],
      "source": [
        "model.most_similar('zuckerberg')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q_OzpU3dHLTr"
      },
      "outputs": [],
      "source": [
        "model.most_similar('google')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "McVHGcK1HOaZ"
      },
      "outputs": [],
      "source": [
        "model.most_similar('intelligence')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LOIkyKGlFsW2"
      },
      "outputs": [],
      "source": [
        "model.most_similar(negative='network')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n9vKWE8JwvER"
      },
      "source": [
        "An other cool feature of Word2Vec is the possibility to perform analogies.  \n",
        "The most famous example is certainly king - man + woman = queen.  \n",
        "Try to find other working analogies."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xCtYRTZ4F0TS"
      },
      "outputs": [],
      "source": [
        "result = model.most_similar(positive=['woman', 'king'], negative=['man'])\n",
        "print(\"{}: {:.4f}\".format(*result[0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6tCyp6B4INe5"
      },
      "outputs": [],
      "source": [
        "result = model.most_similar(positive=['paris', 'spain'], negative=['france'])\n",
        "print(\"{}: {:.4f}\".format(*result[0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WyB4eJA8IWdy"
      },
      "outputs": [],
      "source": [
        "result = model.most_similar(positive=['clinton', 'republican'], negative=['democrat'])\n",
        "print(\"{}: {:.4f}\".format(*result[0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZrvdGv4PI9T0"
      },
      "outputs": [],
      "source": [
        "result = model.most_similar(positive=['beer', 'france'], negative=['usa'])\n",
        "for i in range(3):\n",
        "  print(\"{}: {:.4f}\".format(*result[i]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aadMnWWqxMzD"
      },
      "source": [
        "The following code plots a PCA or t-SNE representation of a list of words.\n",
        "Use this method with your own list of words to see wether similar words are close to each other in the embedding space."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aofSByqSLwwR"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "def plot_embeddings(model, words, reduction='pca'):       \n",
        "    word_vectors = np.array([model[w] for w in words])\n",
        "    if reduction == PCA:\n",
        "      reductor = PCA(n_components=2)\n",
        "    elif reduction == \"tsne\":\n",
        "      reductor = TSNE(2, perplexity=20)\n",
        "    X = reductor.fit_transform(word_vectors)\n",
        "    plt.figure(figsize=(12,12))\n",
        "    plt.scatter(X[:,0], X[:,1])\n",
        "    for word, x in zip(words, X):\n",
        "        plt.text(x[0]+0.05, x[1]+0.05, word)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "guw84mTgMvaM"
      },
      "outputs": [],
      "source": [
        "word_list = ['coffee', 'tea', 'beer', 'wine', 'brandy', 'rum', 'champagne', 'water',\n",
        "                         'spaghetti', 'borscht', 'hamburger', 'pizza', 'falafel', 'sushi', 'meatballs',\n",
        "                         'dog', 'horse', 'cat', 'monkey', 'parrot', 'koala', 'lizard',\n",
        "                         'frog', 'toad', 'monkey', 'ape', 'kangaroo', 'wombat', 'wolf',\n",
        "                         'france', 'germany', 'hungary', 'luxembourg', 'australia', 'fiji', 'china',\n",
        "                         'homework', 'assignment', 'problem', 'exam', 'test', 'class',\n",
        "                         'school', 'college', 'university', 'institute']\n",
        "\n",
        "plot_embeddings(model, words=word_list, reduction='tsne') "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "znmITEuQykvz"
      },
      "source": [
        "We will now use these pre-computed embeddings to build the document representations.  \n",
        "A simple way to compute a document representation from word embeddings consists in computing the mean or the sum of all the document's word embeddings.  \n",
        "Here, since the documents do not have the same length, it is preferable to use the mean.  \n",
        "Fill in the following code to compute the mean embeddings of all documents.  \n",
        "Since this process is a little bit long, we will use a limited amount of documents during the practical session. Nonetheless, feel free to try with the complete dataset at home.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kesLaOYJs_6M"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "tqdm.pandas()\n",
        "\n",
        "traindf = pd.read_csv('train.csv', names=[\"label\", \"title\", \"text\"]).sample(1000)\n",
        "testdf = pd.read_csv('test.csv', names=[\"label\", \"title\", \"text\"]).sample(200)\n",
        "\n",
        "traindf['label'] = traindf['label'] -1\n",
        "traindf['label_name'] = traindf.label.map({0:\"World\", 1:\"Sports\", 2:\"Business\", 3:\"Sci/Tech\"})\n",
        "testdf['label'] = testdf['label'] -1\n",
        "testdf['label_name'] = testdf.label.map({0:\"World\", 1:\"Sports\", 2:\"Business\", 3:\"Sci/Tech\"})\n",
        "\n",
        "def compute_mean_embeddings(s, model, words_list, dim=100):\n",
        "  s = # convert to lower case\n",
        "  emb_list = [model[w] for w in s if w in words_list]\n",
        "  if emb_list != []:\n",
        "    return # compute the mean\n",
        "  else:\n",
        "    return # return a vector filled with 0 if the list is empty\n",
        "\n",
        "words_list = model.index2entity\n",
        "traindf['mean_embeddings'] = traindf.text.progress_apply(lambda s: compute_mean_embeddings(s, model, words_list))\n",
        "testdf['mean_embeddings'] = testdf.text.progress_apply(lambda s: compute_mean_embeddings(s, model, words_list))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "--gciW2Mz7Ur"
      },
      "source": [
        "The following code extracts the computed embeddings from the dataframe.  \n",
        "Use these to train a model to predict the article category.  \n",
        "Print your testing performance and plot a confusion matrix.  \n",
        "The results may be a little bit disappointing. Any idea why?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BRWcZTfJ1sua"
      },
      "outputs": [],
      "source": [
        "X_train = np.vstack(traindf['mean_embeddings'].values)\n",
        "X_test = np.vstack(testdf['mean_embeddings'].values)\n",
        "\n",
        "..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vNordmaC0leP"
      },
      "source": [
        "Plot a t-SNE of the computed embeddings.  \n",
        "Is it a good representation to classify documents?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tSauNwoyEUEG"
      },
      "outputs": [],
      "source": [
        "..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rSB0EcSB01Pd"
      },
      "source": [
        "We will now try with custom Word2Vec representations.  \n",
        "Gensim allows to train Word2Vec representations in a few lines of codes.  \n",
        "Since our vocabullary is smaller than the one used for the pre-computed Word2Vec, we can now use more samples (the embedding look-up will be cheaper to process)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ThwmUeEYGUax"
      },
      "outputs": [],
      "source": [
        "traindf = pd.read_csv('train.csv', names=[\"label\", \"title\", \"text\"]).sample(10000)\n",
        "testdf = pd.read_csv('test.csv', names=[\"label\", \"title\", \"text\"]).sample(1000)\n",
        "\n",
        "traindf['label'] = traindf['label'] -1\n",
        "traindf['label_name'] = traindf.label.map({0:\"World\", 1:\"Sports\", 2:\"Business\", 3:\"Sci/Tech\"})\n",
        "testdf['label'] = testdf['label'] -1\n",
        "testdf['label_name'] = testdf.label.map({0:\"World\", 1:\"Sports\", 2:\"Business\", 3:\"Sci/Tech\"})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YtfvxMFb1d9J"
      },
      "source": [
        "It might be worth applying our tokenizer to reduce the vocabulary size.  \n",
        "Fill the following code to create a new field with tokenized texts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6NSQamkPgNXG"
      },
      "outputs": [],
      "source": [
        "traindf[\"tokenized\"] = traindf.text.apply(lambda s: tokenizer(s))\n",
        "testdf[\"tokenized\"] = testdf.text.apply(lambda s: tokenizer(s))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1hBi4Rm_2bn_"
      },
      "source": [
        "We will now train our own Word2Vec.  \n",
        "Look at the official [documentation](https://radimrehurek.com/gensim/models/word2vec.html).  \n",
        "What does the ```window``` argument stands for? What type of model are we using (CBOW or Skip-gramm)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YzuN_9Jvgibq"
      },
      "outputs": [],
      "source": [
        "from gensim.models import Word2Vec\n",
        "\n",
        "model=Word2Vec(traindf[\"tokenized\"],size=100,window=5,min_count=2)\n",
        "model.train(traindf[\"tokenized\"], total_examples=model.corpus_count, epochs=200)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0eIrLw4q3fdq"
      },
      "source": [
        "Now train a model on these custom embeddings and evaluate it on the testing set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1GOEv2_kF6Rm"
      },
      "outputs": [],
      "source": [
        "words_list = model.wv.index2entity\n",
        "traindf['mean_embeddings'] = traindf.text.progress_apply(lambda s: compute_mean_embeddings(s, model, words_list))\n",
        "testdf['mean_embeddings'] = testdf.text.progress_apply(lambda s: compute_mean_embeddings(s, model, words_list))\n",
        "\n",
        "X_train = np.vstack(traindf['mean_embeddings'].values)\n",
        "X_test = np.vstack(testdf['mean_embeddings'].values)\n",
        "\n",
        "..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O5b5zVOr31bR"
      },
      "source": [
        "PLot a t-SNE of these new embeddings (use the test set to avoid to many points).  Does it seem better?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cGWVS4en4DoB"
      },
      "outputs": [],
      "source": [
        "..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TQoNNaNgd48L"
      },
      "source": [
        "Word2Vec is an self-supervised learning of words represenations. Thus all words representations are meaningfull and have an equal impact when computing the mean.  This means that category irrelevant words have an equal importance in the document average represenatation than other words more related to the category.  \n",
        "Computing the average of word embeddings learned with self-supervised learning is not very efficient for document classification.  \n",
        "In the following we will see two alternatives using deep neural networks:\n",
        "\n",
        "\n",
        "1.   Replace the mean by a recurrent layer responsible for filtering informative words within the sequence\n",
        "2.   Learn our word embeddings at the same time as we learn the classification function\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VU1PbPOsEf0_"
      },
      "source": [
        "For the rest of this notebook we will need a GPU to speed-up trainings.  \n",
        "Go to __Runtime__ and change your runtime type to GPU.  \n",
        "Then run the following command and retart your runtype."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N2A1VC0YQz5U"
      },
      "outputs": [],
      "source": [
        "!pip3 install torch==1.9.1+cu111\n",
        "!pip3 install torchtext==0.10.1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mdCiptE8Fjt1"
      },
      "source": [
        "Since we changed our runtime we need to re-download the datasets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g-RhU0MBFwJ_"
      },
      "outputs": [],
      "source": [
        "!wget https://github.com/mhjabreel/CharCnn_Keras/raw/master/data/ag_news_csv/train.csv 2>&1\n",
        "!wget https://github.com/mhjabreel/CharCnn_Keras/raw/master/data/ag_news_csv/test.csv 2>&1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HLkmMhsIF_7k"
      },
      "source": [
        "The following code will load the datasets in a format compatible with pytorch.  \n",
        "During the process, texts will be tokenized using the [Spacy tokenizer](https://spacy.io/usage/linguistic-features#how-tokenizer-works), we won't need to do it ourself."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "232pmHKqMTbM"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchtext\n",
        "from torchtext.legacy.data import Field, LabelField, TabularDataset, BucketIterator\n",
        "\n",
        "TEXT = Field(tokenize='spacy',batch_first=True,include_lengths=True)\n",
        "LABEL = LabelField(dtype = torch.long, batch_first=True)\n",
        "fields = [('label', LABEL), ('title', TEXT), ('text',TEXT)]\n",
        "trainset = TabularDataset(path = 'train.csv',format = 'csv',fields = fields, skip_header = True)\n",
        "testset = TabularDataset(path = 'test.csv',format = 'csv',fields = fields, skip_header = True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I6QSUe9VG1Jb"
      },
      "source": [
        "Here is an example of one sample of the processed dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eAhsFt2TqtQQ"
      },
      "outputs": [],
      "source": [
        "print(vars(trainset.examples[0]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "355ri9dEHDls"
      },
      "source": [
        "We will used the same pre-trained Word2Vec than previously.  \n",
        "Once again, since we changed our runtime, we need to download the corresponding files."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FPAZwIWtHXTZ"
      },
      "outputs": [],
      "source": [
        "!wget https://nlp.stanford.edu/data/glove.6B.zip \n",
        "!unzip glove.6B.zip > /dev/null 2>&1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aZASnhucHeIq"
      },
      "source": [
        "The following code loads the pre-computed embeddings and builds the corresponding vocabulary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T7kiOaiGxqy2"
      },
      "outputs": [],
      "source": [
        "glove = torchtext.vocab.Vectors('glove.6B.100d.txt')\n",
        "TEXT.build_vocab(trainset,min_freq=3)\n",
        "TEXT.vocab.set_vectors(glove.stoi, glove.vectors, dim=100)\n",
        "LABEL.build_vocab(trainset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6N9rIezmyGoB"
      },
      "outputs": [],
      "source": [
        "torch.cuda.init()\n",
        "torch.cuda.empty_cache()\n",
        "print('CUDA MEM:',torch.cuda.memory_allocated())\n",
        "\n",
        "print('cuda:', torch.cuda.is_available())\n",
        "print('cude index:',torch.cuda.current_device())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "52xAHmSsHvQ1"
      },
      "source": [
        "We will use a [```BucketIterator```](https://torchtext.readthedocs.io/en/latest/data.html#bucketiterator) to generate mini-batches of token sequences.  \n",
        "BucketIterators generate batches of examples of similar lengths while minimizing the amount of padding needed (padding here corresponds to adding a padding token to the sequence).  \n",
        "The ```sort_key``` parameter is used to sort text sequences in batches. Here we want to use sequences of similar length, so we use a function returning a sequence's length.  \n",
        "This is used with the complementary argument ```sort_with_batch```, which indicates sorting sequences with mini-batches only and not within the entire dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ChxZrtXwrAlw"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')  \n",
        "batch_size = 64\n",
        "\n",
        "train_loader, test_loader = BucketIterator.splits(\n",
        "    (trainset, testset), \n",
        "    batch_size = batch_size,\n",
        "    sort_key = lambda x: len(x.text),\n",
        "    sort_within_batch=True,\n",
        "    device = device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kLlD4FHtKTky"
      },
      "source": [
        "We will now define our network architecture.  \n",
        "It is composed of:\n",
        "\n",
        "*   an embedding layer responsible for the mapping between tokens\n",
        "*   several recurrent LSTM layers\n",
        "*   a final fully connected layer\n",
        "\n",
        "Here we will use pretrained embeddings and freeze their \"weights\".  \n",
        "This is maybe to new for asking you to implement the architecture yourself, but make an effort to understand the following code.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G_GlvHG86IL_"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class TextClassifier(nn.Module):\n",
        "    \n",
        "  def __init__(self, vocab, embedding_dim, hidden_dim=32, nb_lstm_layers=2, dropout=0.2, output_dim=4):\n",
        "      super().__init__()          \n",
        "      \n",
        "      #embedding layer\n",
        "      self.embedding = nn.Embedding.from_pretrained(TEXT.vocab.vectors)\n",
        "      self.embedding.weight.requires_grad = False\n",
        "      self.lstm = nn.LSTM(embedding_dim, \n",
        "                          hidden_dim, \n",
        "                          num_layers=nb_lstm_layers, \n",
        "                          dropout=dropout,\n",
        "                          batch_first=True)\n",
        "      self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "      \n",
        "  def forward(self, text, text_lengths): \n",
        "      embedded = self.embedding(text)    \n",
        "      #packed sequence\n",
        "      packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, text_lengths.cpu(), batch_first=True)\n",
        "      _, (hidden, cell) = self.lstm(packed_embedded)\n",
        "      outputs=self.fc(hidden[1])        \n",
        "      return outputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n37IBgavLT6b"
      },
      "source": [
        "Complete the following code to implement the training and testing routines."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9OrKYD3Drc4C"
      },
      "outputs": [],
      "source": [
        "from tqdm.notebook import tqdm\n",
        "\n",
        "\n",
        "def test(model, dataloader):\n",
        "    model.eval()\n",
        "    test_corrects = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for data in dataloader:\n",
        "            text, text_lengths = data.text\n",
        "            labels = data.label\n",
        "\n",
        "            pred = model(text, text_lengths).squeeze()\n",
        "            _, predicted = pred.max(1)\n",
        "            test_corrects += predicted.eq(labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "    return test_corrects / total\n",
        "\n",
        "\n",
        "def train(model, dataloader, optimizer, criterion, epochs=5):\n",
        "  model.train()  \n",
        "  for epoch in range(epochs): \n",
        "    running_loss = 0.0\n",
        "    running_corrects = 0\n",
        "    total = 0 \n",
        "    t = tqdm(dataloader)\n",
        "    for i, batch in enumerate(t):\n",
        "      text, text_lengths = batch.text\n",
        "      labels = batch.label\n",
        "\n",
        "      pred = model(text, text_lengths).squeeze() #convert to 1D tensor\n",
        "      loss = criterion(pred, labels)\n",
        "      \n",
        "      _, predicted = ...\n",
        "      running_corrects += ...\n",
        "      total += ...\n",
        "      running_loss += loss.item()\n",
        "\n",
        "      ... #zero grad your optimizer\n",
        "      ... # backward the loss  \n",
        "      ... # perform a step\n",
        "            \n",
        "      t.set_description(f\"epoch:{epoch} loss: {(running_loss / (i+1)):.4f} current accuracy:{round(running_corrects / total * 100, 2)}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aiM9Pt6XLc9j"
      },
      "source": [
        "Now, instantiate a model and its corresponding optimizer, choose the right criterion (loss function) to train your model, and evaluate its performance on both the training and testing sets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "siYJI4kDraIy"
      },
      "outputs": [],
      "source": [
        "import torch.optim as optim\n",
        "embedding_size = 100\n",
        "model =  ... # send your model to gpu with .to(device)\n",
        "optimizer = ... # use Adam with defautl parameters\n",
        "criterion = ... # choose the correct criterion\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iNt6lMdfrhWW"
      },
      "outputs": [],
      "source": [
        "train(...)\n",
        "train_acc = ...\n",
        "print(f\"Train accuracy: :{round(train_acc * 100, 2)}%\")\n",
        "test_acc = ...\n",
        "print(f\"Test accuracy: :{round(test_acc * 100, 2)}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qw17QdqKMJ9X"
      },
      "source": [
        "\n",
        "We saw that using the mean of embeddings learned by self-supervised learning is ineffective.  \n",
        "This comes from the fact that all the words present in the corpus are given equal importance.  \n",
        "Another solution could be to learn the embeddings while learning to classify.  \n",
        "We will do this now, still using the mean to compute the final text representation.  \n",
        "To do so, we will use a particular layer in pytorch called [```EmbeddingBag```](https://pytorch.org/docs/stable/generated/torch.nn.EmbeddingBag.html#torch.nn.EmbeddingBag).  \n",
        "This layer computes the mean value of a “bag” of embeddings. Although the text entries here have different lengths, nn.EmbeddingBag module requires no padding here since it only computes the means.  \n",
        "We will now implement a simple network as illustarted bellow computing the mean of embeddings to classify texts.  \n",
        "This time we wont freeze the embeddings since we are aiming to learn theme while learning to classify.  \n",
        "![](https://pytorch.org/tutorials/_images/text_sentiment_ngrams_model.png)  \n",
        "Source (https://pytorch.org/tutorials/beginner/text_sentiment_ngrams_tutorial.html)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8R8YMtEwLMlN"
      },
      "outputs": [],
      "source": [
        "class TextClassifier(nn.Module):\n",
        "\n",
        "  def __init__(self, vocab_size, embedding_dim, output_dim=4):\n",
        "    super().__init__()  \n",
        "    self.embedding = nn.EmbeddingBag(vocab_size, embedding_dim, sparse=True)\n",
        "    self.fc = nn.Linear(embedding_dim, output_dim)\n",
        "\n",
        "  def forward(self, text, _): # we just add a third factice _ argument to make this forward compatible with our previous training method\n",
        "    embedded = self.embedding(text)\n",
        "    return self.fc(embedded)\n",
        "\n",
        "  def get_embeddings(self, text):\n",
        "    return self.embedding(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9kIeN7r1O8NW"
      },
      "source": [
        "We need to build a new vocabulary from our corpus."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MLrcSFHtPHfJ"
      },
      "outputs": [],
      "source": [
        "TEXT.build_vocab(trainset,min_freq=3)\n",
        "vocab_size = len(TEXT.vocab)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jhr7PYkiPJIA"
      },
      "source": [
        "Now, instantiate a model and its corresponding optimizer, choose the right criterion to train your model, and evaluate its performance on both the training and testing sets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YWikrHjdpHxZ"
      },
      "outputs": [],
      "source": [
        "model = ...\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.5)\n",
        "train(...)\n",
        "train_acc = ...\n",
        "print(f\"Train accuracy: :{round(train_acc * 100, 2)}%\")\n",
        "test_acc = ...\n",
        "print(f\"Test accuracy: :{round(test_acc * 100, 2)}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "el_JuiWCPZFs"
      },
      "source": [
        "Is the obtained test perfoamnce better than the one of the previous models using the mean?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "voZcckS2RASc"
      },
      "source": [
        "Now try to do the same but using LSTM layer instead of embeddings averages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "trPySNDeQF5u"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class TextClassifier(nn.Module):\n",
        "    \n",
        "  def __init__(self, vocab_size, embedding_dim, output_dim=4):\n",
        "    super().__init__()  \n",
        "    self.embedding = ...\n",
        "    self.lstm = ...\n",
        "    self.fc = ...\n",
        "      \n",
        "  def forward(self, text, text_lengths): \n",
        "      embedded = self.embedding(text)    \n",
        "      #packed sequence\n",
        "      packed_embedded = ...\n",
        "      _, (hidden, cell) = ...\n",
        "      outputs = ...        \n",
        "      return outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b-10efYAQN0r"
      },
      "outputs": [],
      "source": [
        "model = ...\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.5)\n",
        "train(...)\n",
        "train_acc = test(...)\n",
        "print(f\"Train accuracy: :{round(train_acc * 100, 2)}%\")\n",
        "test_acc = test(...)\n",
        "print(f\"Test accuracy: :{round(test_acc * 100, 2)}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vPKBVOqXyuId"
      },
      "source": [
        "# Transformers ! \n",
        "\n",
        "Transformers models are the current state-of-the-art in natural language processing.  \n",
        "We have not seen them during the video lectures since studying transformers would require an entire session in itself.  \n",
        "Nonetheless, we will see how to use them as \"black-box\" models to finish this practical session.\n",
        "\n",
        "Let $\\mathbf{X} \\in \\mathbb{R}^{B  \\times N\\times F}$ be the input sequence, $\\mathbf{W}^Q, \\mathbf{W}^K, \\mathbf{W}^V \\in \\mathbb{R}^{F \\times H}$ be the query, key and value projection matrix.\n",
        "\n",
        "We define the query, the key and the value as $\\mathbf{Q}=\\mathbf{X} \\mathbf{W}^Q, \\mathbf{K}=\\mathbf{X} \\mathbf{W}^K, \\mathbf{V}=\\mathbf{X} \\mathbf{W}^V \\in \\mathbb{R}^{B \\times N\\times H}$.\n",
        "\n",
        "Self attention compute a score $E =\\frac{\\mathbf{Q K}^T}{\\sqrt{d_k}} \\ \\in \\mathbb{R}^{B \\times N\\times N}$ that measure the similarity between the elements in the input sequence.\n",
        "The normalized attention weights $\\alpha_{i j}=\\frac{\\exp \\left(e_{i j}\\right)}{\\sum_{k=1}^{T_x} \\exp \\left(e_{i k}\\right)}$ are computed using softmax function on $E$ \n",
        "\n",
        "The output of the self attention layer is given by : \n",
        "$$\\mathbf{Y}=\\operatorname{Attention}(\\mathbf{Q}, \\mathbf{K}, \\mathbf{V})=\\operatorname{softmax}\\left(\\frac{\\mathbf{Q} \\mathbf{K}^T}{\\sqrt{d_k}}\\right) \\mathbf{V}$$. The features of each sequence element are therefore weighted according to the context.\n",
        "It is possible to use several attention mechanisms in parallel and aggregate them to obtain the final output : \n",
        "$$MultiHead(\\mathbf{Q}, \\mathbf{K}, \\mathbf{V})= Concat \\left(\\right. head _1, \\ldots, head \\left._h\\right) \\mathbf{W}^O$$\n",
        "\n",
        "where $head_i= Attention \\left(\\mathbf{Q} \\mathbf{W}_i^Q, \\mathbf{K} \\mathbf{W}_i^K, \\mathbf{V} \\mathbf{W}_i^V\\right)$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SN0QnGfjzzxU"
      },
      "source": [
        "[Hugging Face](https://huggingface.co/) provides the most practical [library](https://huggingface.co/docs/transformers/main/en/index) to use transformers and pre-trained models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NCnesjPEyu2W"
      },
      "outputs": [],
      "source": [
        "!pip install transformers[torch] 2>&1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z121asRb0X1i"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "train_df = pd.read_csv('train.csv', names=[\"label\", \"title\", \"text\"]).sample(40000)\n",
        "test_df = pd.read_csv('test.csv', names=[\"label\", \"title\", \"text\"]).sample(2000)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ZMCwC3q0xs3"
      },
      "source": [
        "Separate the train set into a train and validation sets using sklearn's **train_test_split**.\n",
        "\n",
        "Keep 20% of the data for the validation set, and remember to stratify! "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YoGiHjUu0xBx"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train, val, train_labels, val_labels = ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MNAE_TlE1gct"
      },
      "source": [
        "As for computer vision, it is possible to use pre-trained models for transfer learning in NLP. \n",
        "[Bert](https://arxiv.org/abs/1810.04805) is one of NLP's most famous standard transformer. In its largest form, it is composed of 345 million parameters.  \n",
        "In this practical session, we will use a smaller version: Distilbert. [Distilbert](https://arxiv.org/abs/1910.01108) is a smaller model that has been trained to mimic the outputs of the Bert model.   \n",
        "This [distillation](https://arxiv.org/abs/1503.02531) process provides a model achieving very good performance with much fewer parameters.  \n",
        "To use the pre-trained model, we will need to match the toke they were trained on.\n",
        "Here we will use the ```DistilBertTokenizerFast``` to manage the text preprocessing. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AOs65hQB1YgO"
      },
      "outputs": [],
      "source": [
        "from transformers import DistilBertTokenizerFast\n",
        "\n",
        "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aU7TeR1Z4NEe"
      },
      "source": [
        "We will also need to wrap our datasets into Pytorch-compatible datasets.  \n",
        "The following code defines a Torch Dataset to handle our textual data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Js1UQ0-4B14"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class NlpDataset(Dataset):\n",
        "    def __init__(self,data,labels,tokenizer):\n",
        "        self.data = data.to_list()\n",
        "        self.labels = labels.tolist()\n",
        "        self.encodings = tokenizer(self.data, truncation=True, padding=True)\n",
        "\n",
        "    def __getitem__(self,idx):\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        item['labels'] = torch.tensor(self.labels[idx],dtype=torch.long)\n",
        "        return item\n",
        "  \n",
        "    def __len__(self):\n",
        "        return len(self.labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CH9Rbq6042GU"
      },
      "source": [
        "Convert your datasets to torch compatible Datasets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aqDN7Od24l7P"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "train_dataset = ...\n",
        "val_dataset = ...\n",
        "test_dataset = NlpDataset(test_df[\"text\"], test_df[\"label\"]-1, tokenizer)\n",
        "\n",
        "batch_size = 64\n",
        "train_loader = ...\n",
        "val_loader = ...\n",
        "test_loader = DataLoader(test_dataset, batch_size=1) # we need a batch size at 1 for later in the notebook"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A3Vm-fBR5hxx"
      },
      "source": [
        "Look at a sample yielded by your train loader.  \n",
        "What type of object is that? Do you know what it is composed of?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dxtgmcGR4--X"
      },
      "outputs": [],
      "source": [
        "next(iter(train_loader))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W7lankVO5-gP"
      },
      "source": [
        "We will now instanciate our model and wrapp it into a Pytorch module."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xs703lUA5Ido"
      },
      "outputs": [],
      "source": [
        "from transformers import  DistilBertForSequenceClassification\n",
        "import torch.nn as nn \n",
        "\n",
        "class BertClf(nn.Module):\n",
        "\n",
        "    def __init__(self, distilbert):\n",
        "\n",
        "        super(BertClf, self).__init__()\n",
        "\n",
        "        self.distilbert = distilbert\n",
        "        for name, param in distilbert.named_parameters():\n",
        "            if not \"classifier\" in name:\n",
        "                param.requires_grad = False\n",
        "\n",
        "    def forward(self, sent_id, mask):\n",
        "\n",
        "        #pass the inputs to the model  \n",
        "        out = self.distilbert(sent_id, attention_mask=mask)\n",
        "        logits = out.logits\n",
        "        attn = out.attentions\n",
        "        hidden_states = out.hidden_states\n",
        "        \n",
        "\n",
        "        return logits, hidden_states, attn\n",
        "\n",
        "distilbert = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\",\n",
        "                                                                  num_labels=4,\n",
        "                                                                  output_attentions=True,\n",
        "                                                                  output_hidden_states=True)\n",
        "\n",
        "model = BertClf(distilbert)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ev6lzsks6jMF"
      },
      "source": [
        "Now complete the following training and testing loops."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ixM8L7Bd6MRS"
      },
      "outputs": [],
      "source": [
        "from tqdm.notebook import tqdm\n",
        "\n",
        "def train_bert(model, optimizer, dataloader, epochs):\n",
        "  model.train()\n",
        "  for epoch in range(epochs):\n",
        "    running_loss = 0.0\n",
        "    running_corrects = 0\n",
        "    total = 0\n",
        "    t = tqdm(dataloader)\n",
        "    for i, batch in enumerate(t):\n",
        "        input_ids = batch[\"input_ids\"].to(device)\n",
        "        attention_mask = batch[\"attention_mask\"].to(device)\n",
        "        labels = batch[\"labels\"].to(device)\n",
        "        preds, _, _ = model(input_ids,mask=attention_mask)\n",
        "        loss = ...\n",
        "\n",
        "        ... #zero grad your optimizer\n",
        "      ... # backward the loss  \n",
        "      ... # perform a step\n",
        "\n",
        "        _, predicted = ...\n",
        "        running_corrects += predicted.eq(labels).sum().item()\n",
        "        total += labels.size(0)\n",
        "        running_loss += loss.item()\n",
        "\n",
        "        t.set_description(f\"epoch:{epoch} loss: {(running_loss / (i+1)):.4f} current accuracy:{round(running_corrects / total * 100, 2)}%\")\n",
        "\n",
        "def test_bert(model, dataloader):\n",
        "    model.eval()\n",
        "    test_corrects = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "      for batch in tqdm(dataloader):\n",
        "        input_ids = batch[\"input_ids\"].to(device)\n",
        "        attention_mask = batch[\"attention_mask\"].to(device)\n",
        "        labels = batch[\"labels\"].to(device)\n",
        "\n",
        "        preds, _, _ = model(input_ids,mask=attention_mask)\n",
        "        _, predicted = preds.max(1)\n",
        "        test_corrects += predicted.eq(labels).sum().item()\n",
        "        total += labels.size(0)\n",
        "    return test_corrects / total"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3tVU0ptLFYh5"
      },
      "source": [
        "Now train a model for one epoch and print its accuracy on the test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EguvFg6R7FT5"
      },
      "outputs": [],
      "source": [
        "from transformers import DistilBertTokenizerFast,  DistilBertForSequenceClassification, AdamW\n",
        "\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "model = model.to(device)\n",
        "optimizer = AdamW(model.parameters(),lr = 1e-5)\n",
        "criterion  = nn.CrossEntropyLoss()\n",
        "n_epochs = 1\n",
        "\n",
        "train_bert(model, optimizer, train_loader, n_epochs)\n",
        "test_bert(model, test_loader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3QyjUkPlG06p"
      },
      "source": [
        "The following code computes the embeddings generated by the DistilBert model.  \n",
        "Use it to plot a t-SNE of the DistilBert's represntations of the test set.  \n",
        "How are the different classes distributed?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cvg5H2cHEdR8"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def get_embeddings(model, dataloader):\n",
        "    model.eval()\n",
        "    embeddings = []\n",
        "    labels = []\n",
        "    with torch.no_grad():\n",
        "      for batch in tqdm(dataloader):\n",
        "        input_ids = batch[\"input_ids\"].to(device)\n",
        "        attention_mask = batch[\"attention_mask\"].to(device)\n",
        "        labels.append(batch[\"labels\"].item())\n",
        "\n",
        "        _, emb, _ = model(input_ids,mask=attention_mask)\n",
        "        last_layer_cls = emb[-1][:,0,:]\n",
        "        embeddings.append(last_layer_cls.squeeze(0).squeeze(0))\n",
        "    embeddings = np.array([e.cpu().numpy() for e in embeddings])\n",
        "    return embeddings, labels\n",
        "\n",
        "embeddings, labels = get_embeddings(model, test_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eJ_3ECCQEr_-"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.manifold import TSNE\n",
        "import seaborn as sns \n",
        "\n",
        "..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GWeW6G5fHlBa"
      },
      "source": [
        "We can use [ **bertviz** library](https://github.com/jessevig/bertviz) to visualize the relation of the element in the input sequence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aBq4b3jyDURC"
      },
      "outputs": [],
      "source": [
        "!pip install bertviz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c00Fodz1CZcg"
      },
      "outputs": [],
      "source": [
        "from bertviz import model_view,head_view\n",
        "\n",
        "sentence = test_df[\"text\"].iloc[33]\n",
        "tokenized = tokenizer(sentence)\n",
        "print(sentence)\n",
        "print(tokenized)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OuB0jef8DF7S"
      },
      "outputs": [],
      "source": [
        "inputs = torch.tensor(tokenized[\"input_ids\"]).unsqueeze(0).to(device)\n",
        "mask = torch.tensor(tokenized[\"attention_mask\"]).unsqueeze(0).to(device)\n",
        "outputs = model(inputs,mask = mask)\n",
        "attention = outputs[-1] \n",
        "tokens = tokenizer.convert_ids_to_tokens(inputs[0]) \n",
        "model_view(attention, tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NOKalV7HDKEi"
      },
      "outputs": [],
      "source": [
        "head_view(attention, tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vMQkgxPZFNtN"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "provenance": [],
      "toc_visible": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
