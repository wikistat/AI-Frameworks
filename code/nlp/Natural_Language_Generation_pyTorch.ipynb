{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GowzsO3iYq-W"
      },
      "source": [
        "<div style=\"font-size:22pt; line-height:25pt; font-weight:bold; text-align:center;\">Natural Language Generation</div>\n",
        "\n",
        "In this exercice, we will try out the approach of Natural Language Generation, using a Seq2Seq (sequence to sequence) architecture.\n",
        "\n",
        "This notebook is a variant of the 3rd notebook on Natural Language Generation, using pyTorch instead of Keras/TF.\n",
        "It is an adaptation of the excellent pyTorch tutorial on Seq2Seq approaches.\n",
        "\n",
        "Natural Language Generation has often used approaches close to \"fill-in-the-blanks\" templates. \n",
        "Seq2Seq Encoder-Decoder architectures make the issue of Natural Language Generation different, especially in the case of language translation.\n",
        "\n",
        "The approach is the following :\n",
        "1. A LSTM network encodes the input sequence into state vectors, with a predefined dimensionality\n",
        "2. A decoder LSTM predicts the next token of a target sequence based on the beginning o the sequence. The initial state is given by the encoder.\n",
        "\n",
        "The approach used here to train a system that predicts the next token based on the beginning of the sequence is called Teacher Forcing\n",
        "\n",
        "# 1. Parameters of the experiment\n",
        "\n",
        "In this example, we will train a system that translates basic english sentences into french. The data used for this example is a list of French sentences and their translation into english.\n",
        "\n",
        "# 1. Teacher forcing\n",
        "\n",
        "## 1.a Example\n",
        "\n",
        "Say we work with the following sequence :\n",
        "\n",
        "```\n",
        "Rien ne sert de courir il faut partir à point\n",
        "```\n",
        "\n",
        "We want to train a model that predicts the following word of the sequence based on the start of it. First, in order for the first word of the sequence and the end of it to be predicted, we need to add beginning and end tokens to the sequence. We decide to use \\t as the beginning token, and \\n as the end one :\n",
        "\n",
        "```\n",
        "\\t Rien ne sert de courir il faut partir à point \\n\n",
        "```\n",
        "\n",
        "When training the system, we start by inputing the \"\\t\" beginning token:\n",
        "\n",
        "```\n",
        "input: \n",
        "\\t\n",
        "prediction:\n",
        "sert\n",
        "```\n",
        "\n",
        "The untrained model generated \"sert\" where we expected \"Rien\". There are now two options to continue :\n",
        "\n",
        "### Without forcing :\n",
        "\n",
        "We add the previous output, \"sert\", to the input sequence, and continue generating :\n",
        "\n",
        "```\n",
        "input: \n",
        "\\t sert\n",
        "```\n",
        "\n",
        "With this approach, the error will propagate and make the model much slower to learn. \n",
        "\n",
        "### With teacher forcing\n",
        "\n",
        "After computing error, we discard the output \"sert\", and replace it with the word that was actually expected (\"Rien\"). This is called *teacher forcing* :\n",
        "\n",
        "```\n",
        "input: \n",
        "\\t Rien\n",
        "```\n",
        "\n",
        "Using his technique provides much faster training of the model. However, please note that it can also be a source of instability on previously unseen data.\n",
        "\n",
        "# 2. Importing data\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget http://www.manythings.org/anki/fra-eng.zip\n",
        "!unzip fra-eng.zip"
      ],
      "metadata": {
        "id": "YVqAc64BZNko"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h9hTgJsDYq-e"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "hidden_size = 256  # Latent dimensionality of the encoding space.\n",
        "\n",
        "# Path to the data\n",
        "data_path = 'fra.txt'\n",
        "\n",
        "#setting up device for use with pyTorch\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"device used is \",device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lUMFnMTZYq-i"
      },
      "source": [
        "## 2.1 Utils to import and preprocess data\n",
        "\n",
        "We vectorize the data using words as features. \n",
        "\n",
        "This means that we will first define a vocabulary containing all the words in our corpus (simalr to what we did in notebook 1 for BOW and TFIDF calculation). The sequences will then be vectorized as a sequence of ints, corresponding to the id a given word in the dictionnary.\n",
        "\n",
        "This section defines a set of utilities to import data :\n",
        "- A class defining the characteristics of the languages imported (french and english), including the size of the vocabulary and the dictionnary mapping words to indexes.\n",
        "- Methods to import data, including a simple preprocessing that lowercases all the characters and removes special characters, in order to control the size of our dictionnay (see notebook 1). \n",
        "- Methods to select data from our dataset, useful for running iterations of our network.\n",
        "\n",
        "Please note that contrary to what we did for classification, the preprocessing does not include Lemmatization or Stemming. This is logical, considering we need to keep data readable by a human. \n",
        "And also don't forget that we are using two special characters \"1\" for start of sequence and \"0\" for end of sequence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9jmC63ryYq-o"
      },
      "outputs": [],
      "source": [
        "# Start and end sequence tokens\n",
        "Start_sentence_token = 1\n",
        "End_sentence_token = 0\n",
        "\n",
        "\n",
        "#Class defining a language.\n",
        "class Lang:\n",
        "    def __init__(self, name):\n",
        "        self.name = name\n",
        "        self.word2index = {}#Contains the index of each word in the dictionnary\n",
        "        self.word2count = {}#Contains the count of each word\n",
        "        self.index2word = {1: \"SOS\", 0: \"EOS\"} #Reverse lookup table for words (useful for decoding sentences back to a readable form)\n",
        "        self.n_words = 2  # Count SOS and EOS\n",
        "\n",
        "    def addSentence(self, sentence):\n",
        "        for word in sentence.split(' '):\n",
        "            self.addWord(word)\n",
        "\n",
        "    def addWord(self, word):\n",
        "        if word not in self.word2index:\n",
        "            self.word2index[word] = self.n_words\n",
        "            self.word2count[word] = 1\n",
        "            self.index2word[self.n_words] = word\n",
        "            self.n_words += 1\n",
        "        else:\n",
        "            self.word2count[word] += 1\n",
        "            \n",
        "            \n",
        "## UTILS\n",
        "import re, unicodedata\n",
        "\n",
        "# Convert to ASCII (because of french sentences)\n",
        "def unicodeToAscii(s):\n",
        "    return ''.join(\n",
        "        c for c in unicodedata.normalize('NFD', s)\n",
        "        if unicodedata.category(c) != 'Mn'\n",
        "    )\n",
        "\n",
        "# Trim, lowercase sentences and remove special chracters except punctuation\n",
        "def normalizeString(s):\n",
        "    s = unicodeToAscii(s.lower().strip())\n",
        "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
        "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
        "    return s\n",
        "\n",
        "# Filter data to keep only some relevant pairs. In particular, to ensure that the system trains fast enough,\n",
        "# we define a max length for sequences and keep only sentences sentences that start the same.\n",
        "max_length = 10\n",
        "\n",
        "eng_prefixes = (\n",
        "    \"i am \", \"i m \",\n",
        "    \"he is\", \"he s \",\n",
        "    \"she is\", \"she s \",\n",
        "    \"you are\", \"you re \",\n",
        "    \"we are\", \"we re \",\n",
        "    \"they are\", \"they re \"\n",
        ")\n",
        "\n",
        "def filterPair(p):\n",
        "    return len(p[0].split(' ')) < max_length and len(p[1].split(' ')) < max_length and p[0].startswith(eng_prefixes)\n",
        "def filterPairs(pairs):\n",
        "    return [pair for pair in pairs if filterPair(pair)]\n",
        "\n",
        "\n",
        "# Reading, Normalizing data\n",
        "def readLangs(lang1, lang2):\n",
        "    print(\"Reading lines...\")\n",
        "    # Read the file and split into lines\n",
        "    lines = open(data_path, encoding='utf-8').read().strip().split('\\n')\n",
        "\n",
        "    # Split every line into pairs and normalize\n",
        "    pairs = [[normalizeString(s) for s in l.split('\\t')] for l in lines]\n",
        "    \n",
        "    # Create language objects\n",
        "    input_lang = Lang(lang1)\n",
        "    output_lang = Lang(lang2)\n",
        "\n",
        "    return input_lang, output_lang, pairs\n",
        "\n",
        "\n",
        "\n",
        "# Full pipeline for importing data :\n",
        "# Reads the files, and cleans data\n",
        "# Filters pairs of english/french sentences to keep only those that are short enough and start the same\n",
        "def prepareData(lang1, lang2):\n",
        "    input_lang, output_lang, pairs = readLangs(lang1, lang2)\n",
        "    print(\"Read %s sentence pairs\" % len(pairs))\n",
        "    pairs = filterPairs(pairs)\n",
        "    print(\"Filtered to %s sentence pairs\" % len(pairs))\n",
        "    for pair in pairs:\n",
        "        input_lang.addSentence(pair[0])\n",
        "        output_lang.addSentence(pair[1])\n",
        "    print(\"Dictionnary size:\")\n",
        "    print(input_lang.name, input_lang.n_words)\n",
        "    print(output_lang.name, output_lang.n_words)\n",
        "    return input_lang, output_lang, pairs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mliJe1YjYq-s"
      },
      "source": [
        "## 2.2 Import and prepare data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ImXQ0WksYq-u"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "input_lang, output_lang, pairs = prepareData('english', 'francais')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vlWy3S5OYq-x"
      },
      "source": [
        "# 3. Vectorizing the data\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9rUTMj1qYq-y"
      },
      "source": [
        "In the previous section we defined a dictionnary to vectorize data in a BOW fashion. \n",
        "\n",
        "Vectorizing data for use by our Neural Network is performed through the folowing steps :\n",
        "- Step 1 : using the dictionnary, the sequence of words is turned into a sequence of indexes\n",
        "- Step 2 : we append the End of Sentence token. Then, the sequence of indexes is turned into a tensor for use by pyTorch. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6zweWM65Yq-6"
      },
      "outputs": [],
      "source": [
        "# STEP 1\n",
        "def indexesFromSentence(lang, sentence):\n",
        "    return [lang.word2index[word] for word in sentence.split(' ')]\n",
        "\n",
        "# STEP 2\n",
        "def tensorFromSentence(lang, sentence):\n",
        "    indexes = indexesFromSentence(lang, sentence)\n",
        "    indexes.append(End_sentence_token)\n",
        "    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)\n",
        "\n",
        "# Performing the two steps of vectorization on a pair of english/french sequences\n",
        "def tensorsFromPair(pair):\n",
        "    input_tensor = tensorFromSentence(input_lang, pair[0])\n",
        "    target_tensor = tensorFromSentence(output_lang, pair[1])\n",
        "    return (input_tensor, target_tensor)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lgDrf1gYYq-_"
      },
      "source": [
        "# 3.0. Defining the Encoder-Decoder Architecture\n",
        "\n",
        "## 3.1. Encoder\n",
        "\n",
        "Encoding is performed via a LSTM, whose state we will store to condition the decoding. \n",
        "Inputs are first embedded into fixed dimensional space, and then encoded by the lstm. We also keep the hidden states of the lstm, as we need to feed them to the encoder for the next iteration.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LFcRN8xjYq_B"
      },
      "outputs": [],
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super(Encoder, self).__init__()\n",
        "        \n",
        "        self.hidden_size = hidden_size\n",
        "        #Embedding Layer\n",
        "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
        "        #LSTM Layer\n",
        "        self.lstm = nn.LSTM(hidden_size, hidden_size)\n",
        "        \n",
        "        \n",
        "    def forward(self, input, hidden):\n",
        "        \n",
        "        #Embedding the input\n",
        "        embedded = self.embedding(input).view(1, 1, -1)\n",
        "        output = embedded\n",
        "        # We feed the embedded vector as well as the hidden states passed as argument into the lstm\n",
        "        output, hidden = self.lstm(output, hidden)\n",
        "        return output, hidden"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NVPG9t8kYq_F"
      },
      "source": [
        "## 3.2. Decoder\n",
        "\n",
        "The decoder is meant to predict the next token of the target sentence, knowing the current token and the context vectors given by the encoder (hidden vectors).\n",
        "The context vectors ancode the input sequence that was given, and will condition all the prediction of the decoder."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SMgzEF08Yq_I"
      },
      "outputs": [],
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, hidden_size, output_size):\n",
        "        super(Decoder, self).__init__()\n",
        "        \n",
        "        self.hidden_size = hidden_size\n",
        "        #Embedding Laeyr\n",
        "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
        "        #LSTM Layer\n",
        "        self.lstm = nn.LSTM(hidden_size, hidden_size)\n",
        "        #Linear layer mapping to the output size\n",
        "        self.out = nn.Linear(hidden_size, output_size)\n",
        "        self.softmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        \n",
        "        #Embedding the input and applying relu\n",
        "        output = self.embedding(input).view(1, 1, -1)\n",
        "        output = F.relu(output)\n",
        "        \n",
        "        # We feed the embedded vector as well as the context vector passed as argument into the lstm\n",
        "        output, hidden = self.lstm(output, hidden)\n",
        "        # Softmax layer (probabilities of each token)\n",
        "        output = self.softmax(self.out(output[0]))\n",
        "        \n",
        "        return output, hidden\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NAGuZOmtYq_K"
      },
      "source": [
        "# 4. Training the model\n",
        "\n",
        "First, we write a function that defines one step of training the model :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QoI5CKZIYq_L"
      },
      "outputs": [],
      "source": [
        "def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length=max_length):\n",
        "    \n",
        "    #initialize hidden and cell state of the encoder lstm randomly\n",
        "    encoder_hidden = torch.randn(1, 1, hidden_size).to(device)\n",
        "    encoder_cell = torch.randn(1, 1, hidden_size).to(device)\n",
        "\n",
        "    #zero out the gradients\n",
        "    encoder_optimizer.zero_grad()\n",
        "    decoder_optimizer.zero_grad()\n",
        "    \n",
        "    #Get length of input and target sequences\n",
        "    input_length = input_tensor.size(0)\n",
        "    target_length = target_tensor.size(0)\n",
        "\n",
        "    loss = 0\n",
        "    \n",
        "    ## ENCODER\n",
        "    #initialize the output of the encoder to zero\n",
        "    encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
        "\n",
        "    #We pass each input token to the encoder. At each step, we retrive the output and the hidden/cell states,\n",
        "    #forming the context vector. The context vector is fed back to the encoder for the next step.\n",
        "    for ei in range(input_length):\n",
        "        encoder_output, (encoder_hidden,encoder_cell) = encoder(input_tensor[ei], (encoder_hidden,encoder_cell))\n",
        "        encoder_outputs[ei] = encoder_output[0, 0]\n",
        "\n",
        "    ## DECODER\n",
        "    #For the decoder, the input is initialized with a Start of Sequence token\n",
        "    decoder_input = torch.tensor([[Start_sentence_token]], device=device)\n",
        "\n",
        "    #The decoder states are initialized by passing the context vector from the encoder\n",
        "    decoder_hidden = encoder_hidden\n",
        "    decoder_cell = encoder_cell\n",
        "    \n",
        "    #We pass each target token to the decoder. We keep the hidden and cell states, that we will feed back to the\n",
        "    #decoder for the next step. However, the outputs are discarded, and the next input of the decoder is the target\n",
        "    #output (see teacher forcing above)\n",
        "    for di in range(target_length):\n",
        "        decoder_output, (decoder_hidden,decoder_cell) = decoder(decoder_input, (decoder_hidden,decoder_cell))\n",
        "        loss += criterion(decoder_output, target_tensor[di])\n",
        "        decoder_input = target_tensor[di]  # Teacher forcing\n",
        "\n",
        "    #Backward prop\n",
        "    loss.backward()\n",
        "    encoder_optimizer.step()\n",
        "    decoder_optimizer.step()\n",
        "\n",
        "    #Return loss\n",
        "    return loss.item() / target_length"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HcdhF9XuYq_N"
      },
      "source": [
        "## 4.1 Plotting loss\n",
        "\n",
        "We have setup the trainign function to return the current loss of the model. \n",
        "The function below displays the learning curve"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PBfQZK7pYq_O"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.switch_backend('agg')\n",
        "import matplotlib.ticker as ticker\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def showPlot(points):\n",
        "    plt.figure()\n",
        "    fig, ax = plt.subplots()\n",
        "    loc = ticker.MultipleLocator(base=0.2)\n",
        "    ax.yaxis.set_major_locator(loc)\n",
        "    plt.plot(points)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EUscgR77Yq_Q"
      },
      "source": [
        "## 4.2. Running multiple iterations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wWLPu2ZGYq_R"
      },
      "outputs": [],
      "source": [
        "def trainIters(encoder, decoder, n_iters, print_every=1000, plot_every=100, learning_rate=0.01):\n",
        "    \n",
        "    plot_losses = [] #Will hold all losses for plotting\n",
        "    print_loss_total = 0  # Reset every print_every\n",
        "    plot_loss_total = 0  # Reset every plot_every\n",
        "\n",
        "    #Setup optimizers\n",
        "    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
        "    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
        "    \n",
        "    #Prepare n_iter training data to run n-iter steps\n",
        "    training_pairs = [tensorsFromPair(random.choice(pairs)) for i in range(n_iters)]\n",
        "    \n",
        "    criterion = nn.NLLLoss()\n",
        "\n",
        "    \n",
        "    for iter in range(1, n_iters + 1):\n",
        "        training_pair = training_pairs[iter - 1]\n",
        "        \n",
        "        # Retrieve the next tensors for input and target\n",
        "        input_tensor = training_pair[0].to(device)\n",
        "        target_tensor = training_pair[1].to(device)\n",
        "        \n",
        "        #Run one step of training\n",
        "        loss = train(input_tensor, target_tensor, encoder,decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
        "        print_loss_total += loss\n",
        "        plot_loss_total += loss\n",
        "\n",
        "        #Every few steps, we print the current status of training. We also store the loss for plotting\n",
        "        if iter % print_every == 0:\n",
        "            print_loss_avg = print_loss_total / print_every\n",
        "            print_loss_total = 0\n",
        "            print('(iteration %d %d%%) loss = %.4f' % (iter, iter / n_iters * 100, print_loss_avg))\n",
        "\n",
        "        if iter % plot_every == 0:\n",
        "            plot_loss_avg = plot_loss_total / plot_every\n",
        "            plot_losses.append(plot_loss_avg)\n",
        "            plot_loss_total = 0\n",
        "\n",
        "    #Plot learning curve at the end\n",
        "    showPlot(plot_losses)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OY9NYJn6Yq_T"
      },
      "source": [
        "## 4.3. Application"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "Dc2IzJbGYq_V"
      },
      "outputs": [],
      "source": [
        "encoder = Encoder(input_lang.n_words, hidden_size).to(device)\n",
        "decoder = Decoder(hidden_size, output_lang.n_words).to(device)\n",
        "\n",
        "trainIters(encoder, decoder, 75000, print_every=1000,plot_every=100)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_2FVgR_QYq_Y"
      },
      "source": [
        "## 4.4. Inference\n",
        "\n",
        "For inference, the only difference with training is that we will continue to feed back the network's predictions to itself at each step, and stop only when we predict an end of sentence token."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kH6nwd_lYq_Z"
      },
      "outputs": [],
      "source": [
        "def inference(encoder, decoder, sentence, max_length=max_length):\n",
        "    \n",
        "    with torch.no_grad(): #Freeze gradient\n",
        "        \n",
        "        input_tensor = tensorFromSentence(input_lang, sentence)\n",
        "        input_length = input_tensor.size()[0]\n",
        "        \n",
        "        #Initialize the encoder hidden states\n",
        "        encoder_hidden = torch.randn(1, 1, hidden_size).to(device)\n",
        "        encoder_cell = torch.randn(1, 1, hidden_size).to(device)\n",
        "\n",
        "        encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
        "\n",
        "        ##ENCODER\n",
        "        for ei in range(input_length):\n",
        "            encoder_output, (encoder_hidden,encoder_cell) = encoder(input_tensor[ei],\n",
        "                                                     (encoder_hidden,encoder_cell))\n",
        "            encoder_outputs[ei] += encoder_output[0, 0]\n",
        "\n",
        "        #Initialize decoder input with a start of sentence token\n",
        "        decoder_input = torch.tensor([[Start_sentence_token]], device=device) \n",
        "\n",
        "        #Feed the encoder context vectors to the decoder\n",
        "        decoder_hidden = encoder_hidden\n",
        "        decoder_cell=encoder_cell\n",
        "\n",
        "        decoded_words = [] #Will hold the decoded sequence (translation)\n",
        "\n",
        "        for di in range(max_length):\n",
        "            decoder_output, (decoder_hidden,decoder_cell) = decoder(decoder_input, (decoder_hidden,decoder_cell))\n",
        "            \n",
        "            topv, topi = decoder_output.data.topk(1)\n",
        "            if topi.item() == End_sentence_token:\n",
        "                decoded_words.append('<EOS>')\n",
        "                break #Stop if we predict an end of sentence token\n",
        "            else:\n",
        "                decoded_words.append(output_lang.index2word[topi.item()])\n",
        "\n",
        "            decoder_input = topi.squeeze().detach() #Use the previously predicted token as the input for the next step\n",
        "\n",
        "        return decoded_words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dc-8dSccYq_b"
      },
      "source": [
        "We define a util function that will evaluate 10 random sentences from the train set and try to translate them"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-wbt-NUMYq_c"
      },
      "outputs": [],
      "source": [
        "def evaluateRandomly(encoder, decoder, n=10):\n",
        "    for i in range(n):\n",
        "        pair = random.choice(pairs)\n",
        "        print('>', pair[0])\n",
        "        print('=', pair[1])\n",
        "        output_words = inference(encoder, decoder, pair[0])\n",
        "        output_sentence = ' '.join(output_words)\n",
        "        print('<', output_sentence)\n",
        "        print('')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qIqhcygIYq_d"
      },
      "outputs": [],
      "source": [
        "evaluateRandomly(encoder, decoder)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XtYx2boIYq_e"
      },
      "source": [
        "After only a few training steps, we observe that translations are often all identical. It would take more several thousands of training iterations (probably about 100 000) to reach a satisfying result with this system. There are two main reasons that can explain this problem :\n",
        "\n",
        "- First, the **use of teacher forcing**. Because we systematically correct the predictions of the network during training, the system has a tendency to learn to predict sentences that are grammatically correct, but would fail to learn the actual meaning. As an exercise, we could try to modfy the train function to not include teacher forcing. The flexibility of PyTorch also allows us to use teacher forcing sometimes, but not always.\n",
        "- Then, the **architecture of the network**. Here, because the context vectors fed into the decoder are the same for a given input sentence, this means that the encoder has the responsibility to learn representations for the input sequence ***in its entirety***. For longer sentences, this isquite unefficient. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gp2qAB-iYq_y"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "colab": {
      "provenance": []
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}