{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "INSA_Reco_TP.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eVNtZAJ7wZDh"
      },
      "source": [
        "In this practical session, you will implement different strategies to build a recommender system."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SkXGEpy55hXT"
      },
      "source": [
        "# Dataset\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I71b3ARWZId3"
      },
      "source": [
        "We will use The Movie Dataset, available on Kaggle.  \n",
        "It contains metadata for all 45,000 movies listed in the [Full MovieLens Dataset](https://grouplens.org/datasets/movielens/). The dataset consists of movies released on or before July 2017. Data points include cast, crew, plot keywords, budget, revenue, posters, release dates, languages, production companies, countries, TMDB vote counts and vote averages.\n",
        "\n",
        "This dataset also has files containing 26 million ratings from 270,000 users for all 45,000 movies. Ratings are on a scale of 1-5 and have been obtained from the official GroupLens website.  \n",
        "You will need [Kaggle](https://www.kaggle.com/) account to download the data.  You should already have one since the DEFI IA is hosted on Kaggle this year. If you don't, it is time to create your account (and to start participating to the DEFI ;-) )  \n",
        "Once you are logged into Kaggle, go to your account and scroll down to the API section to generate a new token.  \n",
        "![](https://drive.google.com/uc?export=view&id=1YcSTHD_FGrwDKaaLk6T9Gsdte8TKuPCt)  \n",
        "We will now install the kaggle library to download the dataset directly from the notebook.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "czuGtQuDRTIM"
      },
      "source": [
        "!pip install kaggle"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kq0iGNLdca65"
      },
      "source": [
        "Run the next cell to upload your token."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X4O_t6ZDRza5"
      },
      "source": [
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "for fn in uploaded.keys():\n",
        "  print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
        "      name=fn, length=len(uploaded[fn])))\n",
        "  \n",
        "# Then move kaggle.json into the folder where the API expects to find it.\n",
        "!mkdir -p ~/.kaggle/ && mv kaggle.json ~/.kaggle/ && chmod 600 ~/.kaggle/kaggle.json"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RqHM7r0_cmhe"
      },
      "source": [
        "We will start by working with the metadata dataset.\n",
        "It contains information about the movies like their title, description, genres, or even their average IMDB ratings."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WUPskNNGRe6M"
      },
      "source": [
        "!kaggle datasets download \"rounakbanik/the-movies-dataset\" -f movies_metadata.csv\n",
        "!kaggle datasets download \"rounakbanik/the-movies-dataset\" -f ratings.csv\n",
        "!unzip movies_metadata.csv.zip\n",
        "!unzip ratings.csv.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xptfFMZkcxwg"
      },
      "source": [
        "Use pandas to explore the *movies_metadata.csv* dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mzn1PFZbFZ10"
      },
      "source": [
        "import pandas as pd\n",
        "metadata = pd.read_csv('movies_metadata.csv')\n",
        "metadata.dropna(subset=['title'], inplace=True)\n",
        "metadata['id'] = pd.to_numeric(metadata['id'])\n",
        "metadata['genres'] = metadata['genres'].apply(lambda x: ' '.join([i['name'] for i in eval(x)]))\n",
        "metadata.head(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pxrwsdTstfHD"
      },
      "source": [
        "Create a new column called _year_ and use seaborn to plot the number of movies per year."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7X6vms1Tt5T8"
      },
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "metadata['year'] = ...\n",
        "\n",
        "plt.figure(figsize=(20,10))\n",
        "..."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CSWUNjSB5oo-"
      },
      "source": [
        "# Recommendation by popularity"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mzchsBrT_2i1"
      },
      "source": [
        "The metadata dataset contains informations about ratings in the _vote_average_ column.  \n",
        "A classical baseline, or cold start when you implement a recommender system consists in using popular products.  \n",
        "## Best movies by average note  \n",
        "Try to visualize the movies with the best vote average.\n",
        "Do you know these movies?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MMeug2Ns_eWR"
      },
      "source": [
        "..."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eSAhfINJuSQr"
      },
      "source": [
        "You may have guessed that the average score is only reliable when it is averaged on a sufficient number of votes.  \n",
        "Use seaborn ```histplot``` method to plot the histogram of the number of votes.\n",
        "For better readability you may first do this plot for the movies with less than 100 votes and then do another ones for the remaining ones."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R5_OvQG7p_oa"
      },
      "source": [
        "plt.figure(figsize=(20,10))\n",
        "plt.subplot(2,1,1)\n",
        "sns.histplot(...)\n",
        "plt.title('Vote count')\n",
        "plt.subplot(2,1,2)\n",
        "sns.histplot(...)\n",
        "plt.title('Vote count')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IWmLm-0c_7k-"
      },
      "source": [
        "Try to visualize the best movies according to the average vote for movies that have at least 1000 votes.\n",
        "You should now know some of these movies.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oie1uXOt_rx9"
      },
      "source": [
        "..."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B-KcoY_Y__TE"
      },
      "source": [
        "## Best movies by IMDb score  \n",
        "IMDb (an acronym for Internet Movie Database) is an online database of information related to films, television programs, home videos, video games, and streaming content online.  \n",
        "It might be considered as one of the most exhaustive databases on movies.  \n",
        "In addition, IMDb maintains a ranking of movies according to people's votes. To do so, it computes a score based on the average rating and the number of votes. \n",
        "The formula they are using is described [here](https://help.imdb.com/article/imdb/track-movies-tv/ratings-faq/G67Y87TFYYP6TWAV#)  \n",
        "![](https://drive.google.com/uc?export=view&id=12J_uJ86eOimr8Y0LHTGSMmUgkBnZu9cO)   \n",
        "Use this formula to compute the IMDb score for all movies and visualize the ones with the best scores. (You may use a smaller value for m, 500 for example)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6HC4ICpX9rMi"
      },
      "source": [
        "m = 500\n",
        "c = ...\n",
        "\n",
        "def imdb_score(x):\n",
        "    ...\n",
        "    score = ...\n",
        "    return score\n",
        "\n",
        "metadata['imdb_score'] = ...\n",
        "..."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9vEAlQDAu-EM"
      },
      "source": [
        "What were the best movies in your birth year?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QzEhMDWqvPsJ"
      },
      "source": [
        "birth_year = ...\n",
        "..."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k_sgJyVAUPKT"
      },
      "source": [
        "The following code will create a data frame containing one-hot encoding of the movie's genre.  \n",
        "Use it to recommend the best movies according to the genre and the IMDB score (for example the best Horror movies)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QW0S-8DCOlqg"
      },
      "source": [
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "\n",
        "metadata['genres_list'] = metadata['genres'].apply(lambda s: s.split(\" \"))\n",
        "mlb = MultiLabelBinarizer()\n",
        "genre_df = pd.DataFrame(mlb.fit_transform(metadata['genres_list'].fillna('[]')),columns=mlb.classes_, index=metadata.index)\n",
        "genre_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PHol9fYDTLGQ"
      },
      "source": [
        "..."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ouHxUzDPUbwb"
      },
      "source": [
        "# Content based recommender systems"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MXdh1FPrWqOh"
      },
      "source": [
        "### Item description\n",
        "Another way to create a recommender system is to base the recommendations on the content.\n",
        "It is an exciting way to start a recommender system when you do not have many user interactions or new items to recommend.  \n",
        "In many cases, the text description is a good starting point.\n",
        "Use what you learned from the first practical section on text data to compute a TF-IDF matrix with the descriptions of the movies (since colab has limited RAM, use a max of 4000 features. We will also work on a subset of the dataset using only the film that were displayed after 2000). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1v8DbM6rWSPu"
      },
      "source": [
        "metadata['overview'] = metadata['overview'].fillna('')\n",
        "subset = metadata[metadata['release_date'] > \"2000\"].reset_index()\n",
        "subset['overview'].head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iS5jvOmS1BNP"
      },
      "source": [
        "from nltk import word_tokenize          \n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "...\n",
        "...\n",
        "...\n",
        "\n",
        "# Create TF-idf model\n",
        "tfidf = ...\n",
        "\n",
        "#Construct the required TF-IDF matrix by fitting and transforming the data\n",
        "tfidf_matrix = ...\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bAyFzBMhXOe6"
      },
      "source": [
        "Now that you have a representation computed for each movie, you can calculate distances or similarities for movie pairs.\n",
        "Compute the cosine similarity matrix of your TF-IDF Matrix.  \n",
        "You may use scikit-learn 's [cosine_distances](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise.cosine_distances.html) function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bmNMKintC5BI"
      },
      "source": [
        "from sklearn.metrics.pairwise import cosine_distances\n",
        "cosine_sim = ..."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-NqPVADOvt22"
      },
      "source": [
        "We will create a list containig the movies with the correct indexes to help us recommended movies."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QVILAG4AE_lY"
      },
      "source": [
        "titles = subset['title']\n",
        "indices = pd.Series(subset.index, index=subset['title'])\n",
        "titles[370:390]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LZpbREM3Xjp9"
      },
      "source": [
        "Use the following function with your similarity matrix to recommend movies from another movie title."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0G-wd7OeGCAd"
      },
      "source": [
        "def get_reco(title, sim_matrix):\n",
        "  idx = indices[title]\n",
        "  print(f'original: {title}')\n",
        "  recos = sim_matrix[idx].argsort()[1:6]\n",
        "  recos = titles.iloc[recos]\n",
        "\n",
        "  print(recos)\n",
        "\n",
        "title = 'The Dark Knight Rises'#'Rush Hour 2'\n",
        "get_reco(...)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TVyYh0NUYZmD"
      },
      "source": [
        "Delete the similarity matrix to free some meomry in the Colab instance.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8dzEoigUYS_T"
      },
      "source": [
        "del(cosine_sim)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wqFB06l161m7"
      },
      "source": [
        "### Item attributes\n",
        "Sometimes your catalog is also filled with additional information about the items.  \n",
        "These pieces of information are usually hand filled and may contain insightful features for a content-based recommender system.  \n",
        "In our case we will download an associated dataset containing informations about the movie casting and the production crew and an other dataset containing keywords associated to the movies."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SY_wWRSH570B"
      },
      "source": [
        "!kaggle datasets download \"rounakbanik/the-movies-dataset\" -f credits.csv\n",
        "!unzip credits.csv.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2hdI5Dst6JgX"
      },
      "source": [
        "credits = pd.read_csv('credits.csv')\n",
        "credits['cast'] = credits['cast'].apply(lambda x: ' '.join([i['name'].replace(' ', '') for i in eval(x)]))\n",
        "credits['crew'] = credits['crew'].apply(lambda x: ' '.join([i['name'].replace(' ', '') for i in eval(x)]))\n",
        "credits.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uTWzBug47LTR"
      },
      "source": [
        "!kaggle datasets download \"rounakbanik/the-movies-dataset\" -f keywords.csv\n",
        "!unzip keywords.csv.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qVI8CAsK65h6"
      },
      "source": [
        "keywords = pd.read_csv('keywords.csv')\n",
        "keywords['keywords'] = keywords['keywords'].apply(lambda x: ' '.join([i['name'] for i in eval(x)]))\n",
        "keywords.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DWT4RXIMB5hd"
      },
      "source": [
        "We will now create another dataframe containing all the movies attributes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7RySZnFB64W-"
      },
      "source": [
        "attributes_df = pd.merge(keywords, credits, on='id')\n",
        "attributes_df = pd.merge(attributes_df, metadata, on='id')\n",
        "attributes_df = attributes_df.sort_values('vote_count', ascending=False).drop_duplicates(subset='id').reset_index()\n",
        "# We will aslo use a subset to avoid Out of Memory issues\n",
        "attributes_df = attributes_df[attributes_df['release_date'] > \"2000\"].reset_index()\n",
        "attributes_df[['title', 'genres', 'cast', 'crew', 'keywords']].head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iYAz9OipCOeD"
      },
      "source": [
        "Create a new columns called *attributes* where you will concatenate the genre, the cast, the crew and the keywords."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TvX55gbR_IQF"
      },
      "source": [
        "attributes_df['attributes'] = ..."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F08330HQCiKJ"
      },
      "source": [
        "Now repeat the previous feature extraction by TF-IDF on this column and compute a new similarity matrix."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IH57BkaU_NlO"
      },
      "source": [
        "tfidf = TfidfVectorizer(...)\n",
        "tfidf_matrix = ...\n",
        "cosine_sim = ..."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pHXdozhNwg9n"
      },
      "source": [
        "We may need to re-create our tiltle index dataframe."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eWxcE5jvAiGR"
      },
      "source": [
        "titles = attributes_df['title']\n",
        "indices = pd.Series(attributes_df.index, index=attributes_df['title'])\n",
        "titles[370:390]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vrnnbl87C159"
      },
      "source": [
        "Try your new matrix similarity to recommend movies based on these new attributes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q8uC6iVq__r3"
      },
      "source": [
        "title = 'Rush Hour 2'#'Inception'\n",
        "..."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gGv6FOEkwvvy"
      },
      "source": [
        "Let's free some meomry in the Colab instance."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_KMMPluZAM8O"
      },
      "source": [
        "del(cosine_distances)\n",
        "del(tfidf_matrix)\n",
        "del(attributes_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3hpHJS-7Ph9b"
      },
      "source": [
        "### Images"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1cURcQU4Yt1C"
      },
      "source": [
        "An other type of content may be one or several images of the products. \n",
        "It may not necessarily be relevant in the case of movies but let's do it anyway.  \n",
        "We will now work with images and recommend movies according to their posters.  \n",
        "We first need to download another dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tcR_BXDOkebb"
      },
      "source": [
        "!kaggle datasets download \"ghrzarea/movielens-20m-posters-for-machine-learning\"\n",
        "!unzip movielens-20m-posters-for-machine-learning.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WbqtGu8zZVIV"
      },
      "source": [
        "The following code will allow us to load the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "slLm9ty3uBA_"
      },
      "source": [
        "# taken from  andrewjong/pytorch_image_folder_with_file_paths.py (https://gist.github.com/andrewjong/6b02ff237533b3b2c554701fb53d5c4d) \n",
        "\n",
        "import torch\n",
        "from torchvision import datasets\n",
        "\n",
        "class ImageFolderWithPaths(datasets.ImageFolder):\n",
        "    \"\"\"Custom dataset that includes image file paths. Extends\n",
        "    torchvision.datasets.ImageFolder\n",
        "    \"\"\"\n",
        "\n",
        "    # override the __getitem__ method. this is the method that dataloader calls\n",
        "    def __getitem__(self, index):\n",
        "        # this is what ImageFolder normally returns \n",
        "        original_tuple = super(ImageFolderWithPaths, self).__getitem__(index)\n",
        "        # the image file path\n",
        "        path = self.imgs[index][0]\n",
        "        # make a new tuple that includes original and the path\n",
        "        tuple_with_path = (original_tuple + (path,))\n",
        "        return tuple_with_path\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SBfjtzoAaIPf"
      },
      "source": [
        "We will use a pre-trained network to extract the features from the posters.   \n",
        "Similar to what we did with the text descriptions, we will compute similarities between the movies according to these features.  \n",
        "\n",
        "The pre-trained model we will be using was trained with normalized images. Thus, we have to normalize our posters before feeding them to the network.  \n",
        "The following code will instantiate a data loader with normalized images and provide a function to revert the normalization for visualization purposes.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W8RDKcuaozEg"
      },
      "source": [
        "from torch.utils.data import DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "mean = [ 0.485, 0.456, 0.406 ]\n",
        "std = [ 0.229, 0.224, 0.225 ]\n",
        "normalize = transforms.Normalize(mean, std)\n",
        "inv_normalize = transforms.Normalize(\n",
        "   mean= [-m/s for m, s in zip(mean, std)],\n",
        "   std= [1/s for s in std]\n",
        ")\n",
        "\n",
        "transform = transforms.Compose([transforms.Resize((224, 224)),\n",
        "                                transforms.ToTensor(),\n",
        "                                normalize])\n",
        "dataset = ImageFolderWithPaths('MLP-20M', transform)    \n",
        "    \n",
        "dataloader = DataLoader(dataset, batch_size=128, num_workers=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FZ-p-y7waHqN"
      },
      "source": [
        "Here are some exemples of posters:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hEaN1FHVnwtW"
      },
      "source": [
        "from torchvision.utils import make_grid\n",
        "import matplotlib.pyplot as plt\n",
        "x, _, paths = next(iter(dataloader))\n",
        "img_grid = make_grid(x[:16])\n",
        "img_grid = inv_normalize(img_grid)\n",
        "plt.figure(figsize=(20,15))\n",
        "plt.imshow(img_grid.permute(1, 2, 0))\n",
        "plt.axis('off')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6HylAtoxarsu"
      },
      "source": [
        "Instantiate a pre-trained a mobilenet_v3_small model (documentation [here](https://pytorch.org/vision/stable/models.html))"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0AihOUOapv-3"
      },
      "source": [
        "import ...\n",
        "mobilenet = ..."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eLj8Pu3ca4Jb"
      },
      "source": [
        "Have a look to the model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z8Fol_cxa0Ev"
      },
      "source": [
        "print(mobilenet)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_dGZvEizbA3l"
      },
      "source": [
        "We will now crate a subset of this model to extract the features.  \n",
        "Use a Sequential model to get only the features followed by the avgpool layer of mobilnet and finish with a Flatten layer (```torch.nn.Flatten()```)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8TMyJ4p6avz5"
      },
      "source": [
        "model = torch.nn.Sequential(...).cuda()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pPGLFcAbbzE8"
      },
      "source": [
        "If your model is OK, it should predict 576-dimensional vectors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wx6Ue-sHqSM5"
      },
      "source": [
        "import torch\n",
        "x = torch.zeros(100, 3, 224,224).cuda()\n",
        "y = model(x)\n",
        "y.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ezj6kEcFcX4H"
      },
      "source": [
        "We will now create a dataframe with our extracted features and the path to the poster image."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EuVIQdtOI9Y6"
      },
      "source": [
        "import pandas as pd\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "df = pd.DataFrame(columns=[\"features\", \"path\"])\n",
        "for x, _, paths in tqdm(dataloader):\n",
        "  with torch.no_grad():\n",
        "    x = x.cuda()\n",
        "    features = ...\n",
        "  tmp = pd.DataFrame({'features': list(features.cpu().numpy()), 'path': list(paths)})\n",
        "  df = df.append(tmp, ignore_index=True)\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O42UYqDryUhm"
      },
      "source": [
        "We will now extract all the features into a numpy array that will be used to compute the similarity matrix."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PYS2v2Gn_cJl"
      },
      "source": [
        "import numpy as np\n",
        "features = np.vstack(df.features)\n",
        "features.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gOi3Jv0hdWE1"
      },
      "source": [
        "Now compute the cosine similarity between your features."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5E_dnAlL7cUH"
      },
      "source": [
        "from sklearn.metrics.pairwise import cosine_distances\n",
        "cosine_sim = ...\n",
        "cosine_sim.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h5v_SZK4c3QD"
      },
      "source": [
        "The ```plot_image``` function  will display a poster according to it's path.  \n",
        "Fill the ```plot_images``` function to plot a series of posters from a list of paths"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PvuAcBau2dki"
      },
      "source": [
        "import matplotlib.image as mpimg\n",
        "\n",
        "def plot_image(path):\n",
        "  img = mpimg.imread(path)\n",
        "  plt.imshow(img)\n",
        "  plt.axis('off')\n",
        "\n",
        "def plot_images(paths_list):\n",
        "  plt.figure(figsize=(20,20))\n",
        "  n = len(paths_list)\n",
        "  for i, path in enumerate(paths_list):\n",
        "    ...\n",
        "\n",
        "\n",
        "plot_images(['MLP-20M/MLP-20M/1.jpg', 'MLP-20M/MLP-20M/2.jpg', 'MLP-20M/MLP-20M/3.jpg', 'MLP-20M/MLP-20M/4.jpg', 'MLP-20M/MLP-20M/5.jpg'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PwXTSjpBdnti"
      },
      "source": [
        "Fill the following code to implement a function that will plot the top 5 recommendations for a movie according to its index."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OTRaBwlR7_a6"
      },
      "source": [
        "def plot_reco(idx, sim_matrix):\n",
        "  img = plot_image(df['path'][idx])\n",
        "  recos = sim_matrix[idx].argsort()[1:6]\n",
        "  reco_posters = df.iloc[recos]['path'].tolist()\n",
        "  plot_images(reco_posters)\n",
        "\n",
        "idx = 16 #10 #200\n",
        "plot_reco(...)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q2j-aaICyrhv"
      },
      "source": [
        "Try with different movie indexes, you will be surprised by the lack of originality of the marketing staffs ;-)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nOsEu26KqDst"
      },
      "source": [
        "# Collaborative filtering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-cWqD4MV6fi-"
      },
      "source": [
        "### Item-Item\n",
        "\n",
        "Item-item collaborative filtering, is a form of collaborative filtering for recommender systems based on the similarity between items calculated using people's ratings. \n",
        "For sake of simplicity, in this practical session, we will only focus on item-item similarity methods.\n",
        "If you have time, feel free to try an user-item approach. The following [blog post](https://notebook.community/saksham/recommender-systems/Collaborative%20Filtering) may help you to do it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F_Xl2hH6z5eX"
      },
      "source": [
        "We will use another dataset containing the ratings of several users on movies."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TCAmkwCvpX_-"
      },
      "source": [
        "!wget https://raw.githubusercontent.com/wikistat/AI-Frameworks/master/RecomendationSystem/movielens_small/movies.csv\n",
        "!wget https://raw.githubusercontent.com/wikistat/AI-Frameworks/master/RecomendationSystem/movielens_small/ratings.csv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2yik0HALDKnt"
      },
      "source": [
        "ratings = pd.read_csv('ratings.csv')\n",
        "ratings = ratings.rename(columns={'movieId':'id'})\n",
        "ratings['id'] = pd.to_numeric(ratings['id'])\n",
        "ratings = pd.merge(ratings, metadata[['title', 'id']], on='id')[['userId', 'id', 'rating', 'title']]\n",
        "ratings.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WT5X5aV20FQ7"
      },
      "source": [
        "ratings.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E9Ox7f-q0IK2"
      },
      "source": [
        "This dataset is a bit huge and may slow down futur computations. Moreover collaborative filtering kind of suffers from products or user with few ratings.  \n",
        "We will only focus on the 100 movies with the most ratings and the users with the highest number of ratings."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eeoIwtXZEjQi"
      },
      "source": [
        "# filter movies\n",
        "ratings['count'] = ratings.groupby('id').transform('count')['userId']\n",
        "movieId = ratings.drop_duplicates('id').sort_values(\n",
        "    'count', ascending=False).iloc[:100]['id']\n",
        "ratings = ratings[ratings['id'].isin(movieId)].reset_index(drop=True)\n",
        "\n",
        "#filter users\n",
        "ratings['count'] = ratings.groupby('userId').transform('count')['id']\n",
        "userId = ratings.drop_duplicates('userId').sort_values(\n",
        "    'count', ascending=False).iloc[:20001]['userId']\n",
        "ratings = ratings[ratings['userId'].isin(userId)].reset_index(drop=True)\n",
        "\n",
        "ratings.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W3QZ374mZZKx"
      },
      "source": [
        "ratings.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nCQaIr9TGtBB"
      },
      "source": [
        "ratings.title.unique()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Egk9md1V1gUx"
      },
      "source": [
        "Now, we need to build a pivot table with user in lines, movies in columns and ratings as values.  \n",
        "Use pandas [pivot_table](https://pandas.pydata.org/docs/reference/api/pandas.pivot_table.html) method to create this pivot table."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4YcyuTdh2_9y"
      },
      "source": [
        "pivot = ...\n",
        "pivot.head(100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aUgKWd4p2M9X"
      },
      "source": [
        "With this pivot table, it is now easy to compute the similarity between movies.  \n",
        "Indeed each movie can be represented by a vector of the users' ratings.\n",
        "Instead of using a cosine similarity distance as we did earlier in the notebook, we will use the Pearson correlation score since it is already implemented in Pandas.  \n",
        "The pivot table has a method ```corrwith``` that will return the Pairwise correlation score of one entry with all entries of the table."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Y-jHNCu3lyS"
      },
      "source": [
        "movie_vector = pivot[\"The Bourne Supremacy\"]\n",
        "#movie_watched = pivot[\"Solo: A Star Wars Story (2018)\"]\n",
        "similarity = ...\n",
        "similarity.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ont3FZJw38xb"
      },
      "source": [
        "Sort the produced results to get the best recommendations to The Bourne Supremacy. \n",
        "You may also try with different movies."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PysDPuwJ3kjO"
      },
      "source": [
        "..."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AUHPO2SY4TPz"
      },
      "source": [
        "## Matrix factorization\n",
        "Matrix factorization is certainly one of the most efficient way to build a recomender system. I really encourage you to have a look to [this article](https://datajobs.com/data-science-repo/Recommender-Systems-%5BNetflix%5D.pdf) presenting the matrix factorization techniques used in recommender systems.\n",
        "\n",
        "The idea is pretty simple, it consists in factorizing the ratings matrix $R$ into the product of a user embedding matrix $U$ and an item embedding matrix $V$, such that $R \\approx UV^\\top$ with\n",
        "$U = \\begin{bmatrix} u_{1} \\\\ \\hline \\vdots \\\\ \\hline u_{N} \\end{bmatrix}$ and\n",
        "$V = \\begin{bmatrix} v_{1} \\\\ \\hline \\vdots \\\\ \\hline v_{M} \\end{bmatrix}$.\n",
        "\n",
        "Where\n",
        "- $N$ is the number of users,\n",
        "- $M$ is the number of items,\n",
        "- $R_{ij}$ is the rating of the $j$th item by the $i$th user,\n",
        "- each row $U_i$ is a $d$-dimensional vector (embedding) representing user $i$,\n",
        "- each row $V_j$ is a $d$-dimensional vector (embedding) representing item $j$,\n",
        "\n",
        "\n",
        "One these emmbeding matrices are built, predicting the rating of an user $i$ for an item $j$ consists in computing the dot product $\\langle U_i, V_j \\rangle$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YynGarm57S4t"
      },
      "source": [
        "### Using surpise\n",
        "\n",
        "![](https://drive.google.com/uc?export=view&id=1dh2RJ95F0j-rZyuf59G35239B42veAWD) \n",
        "\n",
        "We will begin by using the famous Singular Value Decomposition method.\n",
        "Several libraries implement this algorithm.\n",
        "In this session, we will be using [Surprise](http://surpriselib.com/).\n",
        "Surprise is a recommender system library implemented in Python.  \n",
        "It was actually developed by [Nicolas Hug](http://nicolas-hug.com/about) an INSA Toulouse Alumni!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o_ZZ7J-8YHiF"
      },
      "source": [
        "!pip install scikit-surprise"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V8wLyUJXPMtf"
      },
      "source": [
        "Surprise implements the SVD algorithm.  Help yourself with [the doc](https://surprise.readthedocs.io/en/stable/getting_started.html) to train an SVD model on the rating dataset.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CuPtL9WcPeKu"
      },
      "source": [
        "#Creating a train and a test set\n",
        "testset = ratings.sample(frac=0.1, replace=False)\n",
        "trainset = ratings[~ratings.index.isin(testset.index)]\n",
        "\n",
        "assert set(testset.userId.unique()).issubset(trainset.userId.unique())\n",
        "assert set(testset.id.unique()).issubset(trainset.id.unique())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MiSBnIgdX-7U"
      },
      "source": [
        "from surprise import Reader, Dataset, SVD\n",
        "from surprise.model_selection import cross_validate"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N186i9amYSfA"
      },
      "source": [
        "reader = Reader(rating_scale=(0, 5))\n",
        "data = Dataset.load_from_df(ratings[['userId', 'id', 'rating']].fillna(0), reader)\n",
        "svd = SVD()\n",
        "..."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sYw1LD_rBf3m"
      },
      "source": [
        "Let us look some ratings for one user in the test dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lCcwZ4JEaGUC"
      },
      "source": [
        "testset[testset['userId'] == 24]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t97V9XC2BnsN"
      },
      "source": [
        "What would your model predict for these exemples?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sybGFavmZtT0"
      },
      "source": [
        "uid = 24\n",
        "iid = 3114\n",
        "\n",
        "..."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LWUTZcYbcIUY"
      },
      "source": [
        "Write a code to recommend 5 movies to an user."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RdhSEIsOCLBH"
      },
      "source": [
        "..."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wYbnpl6sQvRR"
      },
      "source": [
        "### Using gradient descent\n",
        "Another way to compute the matrix factorization consists in using gradient descent to minimize $\\text{MSE}(R, UV^\\top)$ where:\n",
        "\n",
        "$$\n",
        "\\begin{align*}\n",
        "\\text{MSE}(A, UV^\\top)\n",
        "&= \\frac{1}{|\\Omega|}\\sum_{(i, j) \\in\\Omega}{( R_{ij} - (UV^\\top)_{ij})^2} \\\\\n",
        "&= \\frac{1}{|\\Omega|}\\sum_{(i, j) \\in\\Omega}{( R_{ij} - \\langle U_i, V_j\\rangle)^2}\n",
        "\\end{align*}\n",
        "$$\n",
        "where $\\Omega$ is the set of observed ratings, and $|\\Omega|$ is the cardinality of $\\Omega$.\n",
        "\n",
        "We will now implement our own matrix factorization algorith using Pytorch.\n",
        "To do so we first need to convert our ratings datasets in Pytorch datasets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ES41z_Q__taJ"
      },
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "user_list = trainset.userId.unique()\n",
        "item_list = trainset.id.unique()\n",
        "user2id = {w: i for i, w in enumerate(user_list)}\n",
        "item2id = {w: i for i, w in enumerate(item_list)}\n",
        "\n",
        "class Ratings_Datset(Dataset):\n",
        "    def __init__(self, df):\n",
        "        self.df = df.reset_index()\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "  \n",
        "    def __getitem__(self, idx):\n",
        "        user = user2id[self.df['userId'][idx]]\n",
        "        user = torch.tensor(user, dtype=torch.long)\n",
        "        item = item2id[self.df['id'][idx]]\n",
        "        item = torch.tensor(item, dtype=torch.long)\n",
        "        rating = torch.tensor(self.df['rating'][idx], dtype=torch.float)\n",
        "        return user, item, rating\n",
        "\n",
        "\n",
        "trainloader = DataLoader(Ratings_Datset(trainset), batch_size=512, shuffle=True ,num_workers=2)\n",
        "testloader = DataLoader(Ratings_Datset(testset), batch_size=64, num_workers=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rIaZBIqpZIZA"
      },
      "source": [
        "These dataloader will provide mini-batches of tuples <user, movie, rating>.\n",
        "We will use a special type of Pytorch layers call [Embedding](https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html).  \n",
        "These layers will create a mapping between an index and a vector representation.  \n",
        "In our case they will provide vector representations of our users and items.  \n",
        "We will train the matrix factorization model to minimize the prediction error between a rating and the dot product of an user embedding with a movie embedding.  \n",
        "![](https://drive.google.com/uc?export=view&id=1wSQbcSN_I28mF74-wnb8_qjAzRH9YDjA) \n",
        "\n",
        "Complete the following code to implement the ```MatrixFactorization``` class in Pytorch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6f1g_NCiFCLC"
      },
      "source": [
        "import torch\n",
        "\n",
        "class MatrixFactorization(torch.nn.Module):\n",
        "    def __init__(self, n_users, n_items, n_factors=20):\n",
        "        super().__init__()\n",
        "        self.user_embeddings = torch.nn.Embedding(...)\n",
        "        self.item_embeddings = torch.nn.Embedding(...)\n",
        "\n",
        "    def forward(self, user, item):\n",
        "        user_emb = ...\n",
        "        item_emb = ...\n",
        "        return torch.mul(user_emb, user_emb).sum(1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zQKw2KyztCOa"
      },
      "source": [
        "Complete the training method that we will use to train the network."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ARev-51UGzDc"
      },
      "source": [
        "from tqdm.notebook import tqdm\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from statistics import mean\n",
        "\n",
        "\n",
        "def train(model, optimizer, trainloader, epochs=30):\n",
        "    criterion = ...\n",
        "    t = tqdm(range(epochs))\n",
        "    for epoch in t:\n",
        "        corrects = 0\n",
        "        total = 0\n",
        "        train_loss = []\n",
        "        for users, items, r in trainloader:\n",
        "            users = users.cuda()\n",
        "            items = items.cuda()\n",
        "            r = r.cuda() / 5 #We normalize the score to ease training\n",
        "            y_hat = ...\n",
        "            loss = criterion(y_hat, r.unsqueeze(1).float())\n",
        "            train_loss.append(loss.item())\n",
        "            total += r.size(0)\n",
        "            ...\n",
        "            ...\n",
        "            ...\n",
        "            t.set_description(f\"loss: {mean(train_loss)}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A6s_pnyStNM-"
      },
      "source": [
        "We now have everything to train our model.\n",
        "Train your model with an Adam optimizer (lr=1e-3) for 5 epochs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SxJCX-TH8qeG"
      },
      "source": [
        "n_user = trainset.userId.nunique()\n",
        "n_items = trainset.id.nunique()\n",
        "model = ...\n",
        "optimizer = ...\n",
        "train(...)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_8SJN4nM8ePO"
      },
      "source": [
        "Complete the following code to evaluate your model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ubxaMOTHuGgi"
      },
      "source": [
        "import math\n",
        "\n",
        "def test(model, testloader, m_eval=False):\n",
        "\n",
        "    \n",
        "    running_mae = 0\n",
        "    with torch.no_grad():\n",
        "        corrects = 0\n",
        "        total = 0\n",
        "        for users, items, y in testloader:\n",
        "            users = ...\n",
        "            items = items.cuda()\n",
        "            y = y.cuda() / 5\n",
        "            y_hat = ...\n",
        "            error = torch.abs(y_hat - y).sum().data\n",
        "            \n",
        "            running_mae += ...\n",
        "            total += y.size(0)\n",
        "    \n",
        "    mae = ...\n",
        "    return mae * 5\n",
        "    \n",
        "\n",
        "test(model, testloader)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8g1WpW_n8p1K"
      },
      "source": [
        "Try to compare the predictions of your model with actual ratings."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tny5-w5kMZuJ"
      },
      "source": [
        "users, movies, r = next(iter(testloader))\n",
        "users = users.cuda()\n",
        "movies = movies.cuda()\n",
        "r = r.cuda()\n",
        "\n",
        "pred = ...\n",
        "print(\"ratings\", r[:10].data)\n",
        "print(\"predictions:\", pred.flatten()[:10].data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LhJKjaZ_x_NQ"
      },
      "source": [
        "We just trained a matrix factorization algorithm using Pytorch.  \n",
        "In this setting, the final prediction was made with the dot product of our embeddings.\n",
        "Actually with a minimal modification of the Class, we could create a full neural network.  \n",
        "If we replace the dot product with a fully-connected network, we would actually have an end-to-end neural network able to predict the ratings of our users.  \n",
        "![](https://drive.google.com/uc?export=view&id=1THBMB-Z3db0Rn0dyYYWhN98AHcYEM-nT)  \n",
        "This approach is called Neural Collaborative Filtering and is presented in this [paper](https://arxiv.org/pdf/1708.05031.pdf).  \n",
        "Try to fill in the following code to create an NCF network.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nQa2cusiWbet"
      },
      "source": [
        "class NCF(nn.Module):\n",
        "        \n",
        "    def __init__(self, n_users, n_items, n_factors=8):\n",
        "        super().__init__()\n",
        "        self.user_embeddings = torch.nn.Embedding(n_users, n_factors)\n",
        "        self.item_embeddings = torch.nn.Embedding(n_items, n_factors)\n",
        "        self.predictor = torch.nn.Sequential(\n",
        "            nn.Linear(in_features=..., out_features=64),\n",
        "            ...,\n",
        "            nn.Linear(in_features=32, out_features=1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "        \n",
        "        \n",
        "    def forward(self, user, item):\n",
        "        \n",
        "\n",
        "        user_emb = ...\n",
        "        item_emb = ...\n",
        "\n",
        "        # Concat the two embedding layers\n",
        "        z = torch.cat([user_emb, item_emb], dim=-1)\n",
        "        y = \n",
        "        return y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5NhrFlrT-zef"
      },
      "source": [
        "Train your NCF network on the train dataset and test it on the test dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-0dFPQrWa3bZ"
      },
      "source": [
        "model = NCF(n_user, n_items).cuda()\n",
        "optimizer = ...\n",
        "train(model, optimizer, trainloader, epochs=5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DlfbVGeG5Hwf"
      },
      "source": [
        "test(model, testloader)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gXWXO0ex7WZI"
      },
      "source": [
        "users, movies, r = next(iter(testloader))\n",
        "users = users.cuda()\n",
        "movies = movies.cuda()\n",
        "r = r.cuda()\n",
        "\n",
        "...\n",
        "print(\"ratings\", r[:10].data)\n",
        "print(\"predictions:\", ...)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dxSeYI6AqmMd"
      },
      "source": [
        "### Implicit feedback with pytorch\n",
        "\n",
        "In this practical session, we only worked with explicit feedbacks (ratings).\n",
        "Sometimes you do not have access to such quantitative feedback and have to deal with implicit feedback.  \n",
        "An implicit feedback is a user's qualitative interaction with an item, such as clicking on an item (positive feedback) or stopping watching a video (negative feedback).\n",
        "If you are interested in neural collaborative filtering in the case of implicit feedback, I recommend you look at this [excellent tutorial](https://sparsh-ai.github.io/rec-tutorials/matrixfactorization%20movielens%20pytorch%20scratch/2021/04/21/rec-algo-ncf-pytorch-pyy0715.html)."
      ]
    }
  ]
}