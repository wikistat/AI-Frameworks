{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Practical session: Interpretability in Machine learning"
      ],
      "metadata": {
        "id": "GUi1aMnq6G6v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Machine learning algorithms often behave as black boxes.  \n",
        "In this practical session, we will see some tools to gain interpretability on our models.\n",
        "In the first part of this session, we will focus on model agnostic methods.  \n",
        "In the second part, we will concentrate on techniques for neural network models.  \n",
        "\n",
        "## Model agnostic Methods\n",
        "\n",
        "Before starting with a simple regression problem, run the code below to install compatible versions of pandas and pandas profiling which we will be using to explore our data."
      ],
      "metadata": {
        "id": "C0wV1cm76Ec6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SD1-P6XvA1yR"
      },
      "outputs": [],
      "source": [
        "!pip install pandas-profiling==2.8.0 > /dev/null 2>&1\n",
        "!pip install pandas==0.25 > /dev/null 2>&1"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Restart the runtime after executing the cell above "
      ],
      "metadata": {
        "id": "1DJL-dl9AHKt"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WCGRXcZMXZqA"
      },
      "source": [
        "### Dataset\n",
        "\n",
        "We will begin with a simple regression problem of predicting Californian houses' house prices according to 8 numerical features.  \n",
        "Scikit-learn provides the dataset, and a full description is available [here](https://scikit-learn.org/stable/datasets/real_world.html#california-housing-dataset).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZqNMicz1GBMV"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "\n",
        "cal_housing = fetch_california_housing()\n",
        "feature_names = cal_housing.feature_names\n",
        "X = pd.DataFrame(cal_housing.data, columns=feature_names)\n",
        "y = cal_housing.target\n",
        "X.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "I encourage you to explore the dataset by yourself using pandas and seaborn; it is always a good exercise.  \n",
        "Nonetheless, today's practical session is not designed to train the best possible models but to learn how to interpret them.  \n",
        "It may be a good opportunity to present you a friendly tool for exploring datasets: [Pandas profiling](https://pandas-profiling.github.io/pandas-profiling/docs/master/rtd/).  \n",
        "Pandas profiling provides an automatic data exploration tool and html reports in a one-liner.  \n",
        "I would recommend usually doing it by yourself in other projects, but it may still be helpful for having a quick overview of what your dataset looks like."
      ],
      "metadata": {
        "id": "PIxE8zIq9GIk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2PXxeEaQNsOG"
      },
      "outputs": [],
      "source": [
        "from pandas_profiling import ProfileReport\n",
        "\n",
        "ProfileReport(X, sort=\"None\")\n",
        "# if this cell returns an error restart the kernel and re-run the previous cells"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now use scikit-learn's ```train_test_split``` method to split your dataset in train and test (10%)"
      ],
      "metadata": {
        "id": "7g4BiV6a9Y_e"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8xEmnWncGDnN"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = ..."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train a linear regression, a random forest and a neural network to predict the houses price.  \n",
        "Use a [pipeline](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.make_pipeline.html) to create a model that performs both the scaling and the training algorithm.  \n",
        "Test all models on your test set to copare their performances.  \n",
        "Feel free to train and test more sophisticated models such as XGBoost, LightGBM..."
      ],
      "metadata": {
        "id": "KhANGvb59x5b"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mcrai3CT0vGy"
      },
      "outputs": [],
      "source": [
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.neural_network import MLPRegressor\n",
        "\n",
        "lr = ...\n",
        "\n",
        "rf = ...\n",
        "\n",
        "\n",
        "mlp = ...\n",
        "\n",
        "lr.fit(X_train, y_train)\n",
        "rf.fit(X_train, y_train)\n",
        "mlp.fit(X_train, y_train)\n",
        "\n",
        " ... #test your models"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Linear models are considered intrinsically interpretable.  \n",
        "Using the ```coef_``` attribute of your model, visualize the importance of each of the features of the linear model.  "
      ],
      "metadata": {
        "id": "IPuKaOB56qb2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# to access the model part of your pipeline: lr[1]\n",
        "\n",
        "..."
      ],
      "metadata": {
        "id": "AtCSl7b25Vyk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "ayA2YdCM5rYv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Features importance\n",
        "\n",
        "We will begin by looking at the features importance.  \n",
        "Scikit-learn implements some native methods to compute the feature importance of tree-based methods.  \n",
        "We will use an external library called [Eli5](https://eli5.readthedocs.io/en/latest/overview.html#features) to compute the feature permutation method, which is model agnostic and can thus be applied to our three models.\n",
        "\n"
      ],
      "metadata": {
        "id": "28qvhcHziwvy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rYX22dwpJ9s5"
      },
      "outputs": [],
      "source": [
        "!pip install eli5 > /dev/null 2>&1"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use the ```PermutationImportance``` to compute the features importance of your models.  (Documentation [here](https://eli5.readthedocs.io/en/latest/blackbox/permutation_importance.html)).  \n",
        "Plot them for each of your model.  \n",
        "Are the feature importance of the linear model similar to the coefficients?\n",
        "Are the features as important for all your models?  \n",
        "Create a dictionnary containing the top 5 features for each of your model (**key**:model name, **value**: dataframe of features importance)"
      ],
      "metadata": {
        "id": "QwtyLPeXABT6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IYqFc4J4WveZ"
      },
      "outputs": [],
      "source": [
        "import eli5\n",
        "from eli5.sklearn import PermutationImportance\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "features_importance_dict = {}\n",
        "for model, name in zip([lr, rf, mlp], ['logistic regression', 'random forest', 'multi layer perceptron']):\n",
        "  plt.figure()\n",
        "  permumtation_import = PermutationImportance(...)\n",
        "  features_importance = {'Feature_name':feature_names, 'Importance':permumtation_import.feature_importances_}  \n",
        "  features_importance = pd.DataFrame(features_importance) # dataframe containing the features names and their importance\n",
        "  features_importance = features_importance.sort_values(...) # sort the dataframe by feature importance\n",
        "  features_importance_dict[name] = ... #add the dataframe to your dictionnary\n",
        "  ax = sns.barplot(x=..., y=..., data=features_importance) #plot the model's features importance\n",
        "  plt.title(name)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You may have noticed that the geographical position seems to be among the most important features for some of your models.  \n",
        "We can use [folium](http://python-visualization.github.io/folium/)\n",
        " to plot these on a map and get a better overview."
      ],
      "metadata": {
        "id": "BSFXcCM_CCuM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import folium\n",
        "\n",
        "latmean = X.Latitude.mean()\n",
        "lonmean = X.Longitude.mean()\n",
        "map = folium.Map(location=[latmean,lonmean], \n",
        "        zoom_start=6,tiles = 'Mapbox bright') \n",
        "\n",
        "def color(value): \n",
        "    if value in range(0,149999): \n",
        "        col = 'green'\n",
        "    elif value in range(150000,249999): \n",
        "        col = 'yellow'\n",
        "    elif value in range(250000,349999): \n",
        "        col = 'orange'\n",
        "    else: \n",
        "        col='red'\n",
        "    return col \n",
        "      \n",
        "map = folium.Map(location=[latmean,lonmean], \n",
        "        zoom_start=6) \n",
        "\n",
        "# Top three smart phone companies by market share in 2016\n",
        "for lat,lan,value in zip(X_test['Latitude'][:300],X_test['Longitude'][:300],y_test[:100]*100000): \n",
        "    folium.Marker(location=[lat,lan],icon= folium.Icon(color=color(value),icon_color='black',icon = 'home')).add_to(map) \n",
        "map"
      ],
      "metadata": {
        "id": "PEuk1hj2XBv-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### PDP and ICE plots\n",
        "\n",
        "We will use the [pdpbox](https://pdpbox.readthedocs.io/en/latest/) library to generate our PDP and ICE plots."
      ],
      "metadata": {
        "id": "rsCwO3iJCJr8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mo1wzI6fE25z"
      },
      "outputs": [],
      "source": [
        "!pip install pdpbox > /dev/null 2>&1"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following code shows you how to produce a PDP plot for the random forest model.  \n",
        "```python\n",
        "from pdpbox import pdp, get_dataset, info_plots\n",
        "\n",
        "pdp_feat = pdp.pdp_isolate(model=rf, dataset=X_test, model_features=feature_names, feature='MedInc')\n",
        "\n",
        "pdp.pdp_plot(pdp_feat, 'MedInc', plot_lines=True, frac_to_plot=0.5)\n",
        "plt.show()\n",
        "```\n",
        "Use it to generate the PDP plots for the three most important features of each of your models.  \n",
        "What is the nature of their relationship with the target?\n"
      ],
      "metadata": {
        "id": "OWXTqLfiCfhx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2n5rPYTHHSvf"
      },
      "outputs": [],
      "source": [
        "from pdpbox import pdp, get_dataset, info_plots\n",
        "model = rf #lr, mlp\n",
        "model_name = 'random forest'#'logistic regression' , 'multi layer perceptron'\n",
        "\n",
        "top_3_features = features_importance_dict[model_name].Feature_name[:3].values\n",
        "for i, feature in enumerate(top_3_features, 1):\n",
        "  pdp_feat = ...\n",
        "  ..."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "It is also possible to visualize the combined effetc of two features:"
      ],
      "metadata": {
        "id": "Ui2NTJPuDWEj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vL5-NZpiKH5n"
      },
      "outputs": [],
      "source": [
        "features_to_plot = ['Latitude', 'Longitude']\n",
        "inter1 = pdp.pdp_interact(model=model, dataset=X_test, model_features=feature_names, features=features_to_plot)\n",
        "\n",
        "pdp.pdp_interact_plot(pdp_interact_out=inter1, feature_names=features_to_plot, plot_type='contour')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Scikit-learns also provides methods to generate such plots, but may offer less flexibility."
      ],
      "metadata": {
        "id": "8ix2ghOXD9fE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oClY7KYiZfLa"
      },
      "outputs": [],
      "source": [
        "from sklearn.inspection import partial_dependence\n",
        "from sklearn.inspection import PartialDependenceDisplay\n",
        "\n",
        "for model, model_name in zip([lr, rf, mlp], ['logistic regression', 'random forest', 'multi layer perceptron']):\n",
        "\n",
        "  top_3_features = features_importance_dict[name].Feature_name[:3].values\n",
        "  display = PartialDependenceDisplay.from_estimator(\n",
        "    model,\n",
        "    X_test,\n",
        "    top_3_features,\n",
        "    kind=\"both\",\n",
        "    subsample=50,\n",
        "    n_jobs=3,\n",
        "    n_cols=3,\n",
        "    grid_resolution=20,\n",
        "    random_state=0,\n",
        "    ice_lines_kw={\"color\": \"tab:blue\", \"alpha\": 0.2, \"linewidth\": 0.5},\n",
        "    pd_line_kw={\"color\": \"tab:orange\", \"linestyle\": \"--\"}\n",
        "    )\n",
        "  display.figure_.suptitle(f\"Partial dependence for {model_name} model\")\n",
        "  display.figure_.subplots_adjust(hspace=0.3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k7B_FP1MLaC1"
      },
      "outputs": [],
      "source": [
        "for model, name in zip([lr, rf, mlp], ['logistic regression', 'random forest', 'multi layer perceptron']):\n",
        "  _, ax = plt.subplots(ncols=3, figsize=(9, 4))\n",
        "  top_2_features = features_importance_dict[name].Feature_name[:3].values\n",
        "  features = [top_2_features[0], top_2_features[1], (top_2_features[0], top_2_features[1])]\n",
        "  display = PartialDependenceDisplay.from_estimator(\n",
        "      model,\n",
        "      X_test,\n",
        "      features,\n",
        "      kind=\"average\",\n",
        "      n_jobs=3,\n",
        "      grid_resolution=20,\n",
        "      ax=ax,\n",
        "  )\n",
        "  display.figure_.suptitle(f\"Partial dependence for {name} model\")\n",
        "  display.figure_.subplots_adjust(wspace=0.4, hspace=0.3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4qcHjlNlqZht"
      },
      "source": [
        "### SHAP"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Previous methods provided global explanations of our models.  \n",
        "We will now focus on local interpretability methods.  \n",
        "We will begin with the SHAP methods based on the estimation of the Shapley values.  \n",
        "The library SHAP implements the SHAP method (and many others).\n",
        "\n",
        "Inspire yourself with the following [documentation](https://shap.readthedocs.io/en/latest/example_notebooks/tabular_examples/model_agnostic/Diabetes%20regression.html) to produce a visualization of the estimated Shapley values of your different models, first for a single example using the ```force_plot``` method and for the entire test, dataset using the ```summary_plot``` method."
      ],
      "metadata": {
        "id": "MqDHTKMk89DV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rL3bFS9fJ74P"
      },
      "outputs": [],
      "source": [
        "!pip install shap > /dev/null 2>&1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "anuzREMVPrtw"
      },
      "outputs": [],
      "source": [
        "import shap\n",
        "shap.initjs() #needed to plot results directly on the notebook\n",
        "\n",
        "idx = 1 # index of the instance we want to explain\n",
        "\n",
        "explainer = ...\n",
        "shap_values = ...\n",
        "... #single exemple plot\n",
        "plt.figure()\n",
        "... #Summary on the dataset. To speed up we just compute the shap values for 20 exemples"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uGiokmqJXJOA"
      },
      "source": [
        "### Lime  \n",
        "\n",
        "We also saw in class another model agnostic local interpretability method.    \n",
        "Many implementations of the LIME method are available in python.  \n",
        "In this practical session, we will use the [implementation provided by the authors](https://github.com/marcotcr/lime).  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9gqZDHzaNQmJ"
      },
      "outputs": [],
      "source": [
        "!pip install lime > /dev/null 2>&1"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "LIME provides eay to understand an friendly looking explanations for your model predictions.  \n",
        "You first need to instanciate an Explainer (in our case a ```LimeTabularExplainer```) and then call the ```explain instance``` method of the explainer to get the explanations.  "
      ],
      "metadata": {
        "id": "KNJcxPiWHxQZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import lime\n",
        "import lime.lime_tabular\n",
        "\n",
        "index = 0\n",
        "\n",
        "explainer = lime.lime_tabular.LimeTabularExplainer(X_test.values, feature_names=feature_names, mode=\"regression\")\n",
        "exp = explainer.explain_instance(X_test.iloc[index], rf.predict, num_features=5, top_labels=1)\n",
        "exp.show_in_notebook(show_table=True, show_all=True)"
      ],
      "metadata": {
        "id": "UxD3boqfDuOL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Classification\n",
        "LIME also works with classification problems.  \n",
        "We will repeat the previous experiment using a different dataset for [breast cancer prediction](https://scikit-learn.org/stable/datasets/toy_dataset.html#breast-cancer-dataset) and a decision trees algorithm."
      ],
      "metadata": {
        "id": "FhF1RRIvFr9W"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BO_5AJ48iMr0"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "\n",
        "breast_cancer = load_breast_cancer()\n",
        "feature_names = breast_cancer.feature_names\n",
        "target_names = breast_cancer.target_names\n",
        "X = pd.DataFrame(breast_cancer.data, columns=feature_names)\n",
        "y = breast_cancer.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=0)\n",
        "X_train.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train a decision tree (with max_depth=5) on this dataset and plot the confusion matrix on the test dataset."
      ],
      "metadata": {
        "id": "sQZIu3brGdp8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import ConfusionMatrixDisplay \n",
        "\n",
        "dt = ...\n",
        "...\n",
        "\n",
        "print(\"Descision Tree score: ...\")\n",
        "\n",
        "..."
      ],
      "metadata": {
        "id": "pM4Q_niCIoYG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Decision trees are also interpretable models.  \n",
        "Scikit-learn provides an efficient way to visualize their structure.  "
      ],
      "metadata": {
        "id": "xYfBIwhWJUj7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import sklearn.tree as tree\n",
        "plt.figure(figsize=(20,20))\n",
        "tree.plot_tree(dt[1])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "XliLIQjTJGYe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explain the predictions of your model on some examples.  \n",
        "For classification tasks, LIME needs the predicted \"probailities\" of the model.  \n",
        "Use the ```predict_proba``` method of your classifier instead of the ```predict``` method when calling the explain instance.    \n",
        "Also, don't forget to remove the ```mode=\"regression\"``` argument when instanciating the ```LimeTabularExplainer```.  \n",
        "Are the explanations consistent with the decision graph?"
      ],
      "metadata": {
        "id": "i6G-XEoAG3a5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x4phdYE0it1h"
      },
      "outputs": [],
      "source": [
        "explainer = lime.lime_tabular.LimeTabularExplainer(...)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "08wSF1wNldmz"
      },
      "outputs": [],
      "source": [
        "index = 0\n",
        "exp = explainer.explain_instance(...)\n",
        "exp.show_in_notebook(show_table=True, show_all=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SJQ6Y29fiE-8"
      },
      "source": [
        "#### Text data\n",
        "The authors also provided nice visualizations for text data.  \n",
        "We will now train a Random forest to classify whether scientific texts are about medicine or space.   \n",
        "We will be using two categories from the [newsgroup dataset](https://scikit-learn.org/0.19/datasets/twenty_newsgroups.html) available in scikit-learn."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jb1uEpxVWNaw"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import fetch_20newsgroups\n",
        "\n",
        "categories = [\n",
        "    'sci.med',\n",
        "    'sci.space'\n",
        "]\n",
        "\n",
        "\n",
        "train_data = fetch_20newsgroups(subset='train', categories=categories)\n",
        "test_data = fetch_20newsgroups(subset='test', categories=categories)\n",
        "\n",
        "class_names = train_data.target_names\n",
        "\n",
        "X_train, y_train  = train_data.data, train_data.target\n",
        "X_test, y_test = test_data.data, test_data.target"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here is an example of text to classify:"
      ],
      "metadata": {
        "id": "e7TcROIKPr-q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train[0]"
      ],
      "metadata": {
        "id": "Fy5fe70LPQxU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's train a random forest to classify our dataset."
      ],
      "metadata": {
        "id": "iF3PCkDNPza6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "model = make_pipeline(\n",
        "        CountVectorizer(max_df= 0.5, ngram_range= (1, 2)),\n",
        "        TfidfTransformer(),\n",
        "        RandomForestClassifier()\n",
        ")\n",
        "\n",
        "model.fit(X_train, y_train)\n",
        "print(f\"Model score: {model.score(X_test, y_test):.2f}\")"
      ],
      "metadata": {
        "id": "PcuQRF56Pzva"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will use a specific ```LimeTextExplainer``` to explain the predictions."
      ],
      "metadata": {
        "id": "tDpK2DSpO67P"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jShOjASkaKqW"
      },
      "outputs": [],
      "source": [
        "from lime.lime_text import LimeTextExplainer\n",
        "explainer = LimeTextExplainer(class_names=class_names)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OioyQTXsbbJC"
      },
      "outputs": [],
      "source": [
        "from lime.lime_text import LimeTextExplainer\n",
        "explainer = LimeTextExplainer(class_names=class_names)\n",
        "\n",
        "index = 11\n",
        "exp = explainer.explain_instance(X_test[index], model.predict_proba, num_features=6)\n",
        "\n",
        "prediction = model.predict_proba([X_test[index]])\n",
        "class_predicted = class_names[prediction.argmax(1)[0]]\n",
        "class_proba = prediction.max(1)[0]\n",
        "true_class = class_names[y_test[index]]\n",
        "print(f'Class predicted: {class_predicted} (p={class_proba})')\n",
        "print(f'True class: {class_names[y_test[index]]}')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here are the top words used by the classifier.  "
      ],
      "metadata": {
        "id": "L-CnzTmiQP5H"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6fBm-MrxewAr"
      },
      "outputs": [],
      "source": [
        "fig = exp.as_pyplot_figure()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "These explanations seem plausible, let's visualize these words in their context:"
      ],
      "metadata": {
        "id": "x5Qa3_eMQu3z"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "chdysnzoe4mz"
      },
      "outputs": [],
      "source": [
        "exp.show_in_notebook(text=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Some of the words are in the newsgroup header!  \n",
        "Would you trust such a classifier?"
      ],
      "metadata": {
        "id": "XJgAQVl2Q1QA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Scikit-learn provides an option to remove all headers and footers.  \n",
        "Train a new model on the datset with removed headers and footers and comare its F1-score with the previous model."
      ],
      "metadata": {
        "id": "M7erm7a2RFoO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "amTysYs5fKUr"
      },
      "outputs": [],
      "source": [
        "train_data = fetch_20newsgroups(subset='train',remove=('headers', 'footers', 'quotes'),\n",
        "                                     categories=categories)\n",
        "X_train, y_train  = train_data.data, train_data.target\n",
        "\n",
        "\n",
        "\n",
        "model = ...\n",
        "\n",
        "...\n",
        "print(f\"Model score: {model.score(X_test, y_test):.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now visualize the explainations computed by lime on the same example with your new model.  \n",
        "Which of the two models would you trust the more?"
      ],
      "metadata": {
        "id": "7__EA0CjRiSh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ygTES-j-gNxM"
      },
      "outputs": [],
      "source": [
        "explainer = ...\n",
        "index = 11\n",
        "exp = ...\n",
        "\n",
        "prediction = ...\n",
        "class_predicted = ...\n",
        "class_proba = ...\n",
        "true_class = ...\n",
        "..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4-nwQYZxplnx"
      },
      "source": [
        "#### Image Data\n",
        "\n",
        "Finally, LIME also provides friendly-looking visualizations on images.  \n",
        "We will use a subset of Imagenet to test these visualizations.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ysRk_M-A9Qds"
      },
      "outputs": [],
      "source": [
        "!wget https://s3.amazonaws.com/fast-ai-imageclas/imagenette2.tgz\n",
        "!tar zxvf imagenette2.tgz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lthQ-VxA_su7"
      },
      "outputs": [],
      "source": [
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torch\n",
        "import os\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "means, stds = (0.485, 0.456, 0.406), (0.229, 0.224, 0.225)\n",
        "train_transform = transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(means, stds),\n",
        "    ])\n",
        "\n",
        "test_transform = transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(means, stds),\n",
        "    ])\n",
        "\n",
        "def get_imagenette2_loaders(root_path='./imagenette2', **kwargs):\n",
        "\n",
        "    trainset = torchvision.datasets.ImageFolder(os.path.join(root_path, \"train\"), transform=train_transform)\n",
        "    trainloader = torch.utils.data.DataLoader(trainset, **kwargs)\n",
        "    testset = torchvision.datasets.ImageFolder(os.path.join(root_path, \"val\"), transform=test_transform)\n",
        "    testloader = torch.utils.data.DataLoader(testset, **kwargs)\n",
        "    return trainloader, testloader\n",
        "\n",
        "trainloader, testloader = get_imagenette2_loaders( batch_size=64, shuffle=True, num_workers=2)\n",
        "\n",
        "labels = ['tench', 'English springer', 'cassette player', 'chain saw', 'church', 'French horn', 'garbage truck', 'gas pump', 'golf ball', 'parachute']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lmgiKjbPI7jS"
      },
      "outputs": [],
      "source": [
        "from torchvision.utils import make_grid\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "inv_normalize = transforms.Normalize(\n",
        "   mean= [-m/s for m, s in zip(means, stds)],\n",
        "   std= [1/s for s in stds]\n",
        ")\n",
        "\n",
        "x, _ = next(iter(trainloader))\n",
        "img_grid = make_grid(x[:16])\n",
        "img_grid = inv_normalize(img_grid)\n",
        "plt.figure(figsize=(20,15))\n",
        "plt.imshow(img_grid.permute(1, 2, 0))\n",
        "plt.axis('off')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will train a neural network to classify these images.  \n",
        "However, training neural networks on high-definition images may be long and difficult.  \n",
        "We will use a pre-trained VGG11 and replace the fully connected part of the network to match the ten classes (this is called transfer learning).  \n",
        "Complete the following code to instantiate a model predicting among ten classes with pre-trained features. "
      ],
      "metadata": {
        "id": "-zhmmKQgXEtv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U8wgIMYcBJP4"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch\n",
        "\n",
        "model = torchvision.models.vgg11(pretrained=True)\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for param in model.features:\n",
        "    param.requires_grad = False #we freeze the feature extraction part of the network\n",
        "\n",
        "model.classifier = nn.Sequential(\n",
        "            ...\n",
        "        )\n",
        "\n",
        "model = model.cuda()"
      ],
      "metadata": {
        "id": "3teRHooM9hhx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fill the following code to implement the training loop."
      ],
      "metadata": {
        "id": "geTOAMq2XIKC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KIpkmp0WCuzR"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "criterion_classifier = nn.CrossEntropyLoss(reduction='mean')\n",
        "\n",
        "def train(model, optimizer, trainloader, epochs=30):\n",
        "    t = tqdm(range(epochs))\n",
        "    for epoch in t:\n",
        "        corrects = 0\n",
        "        total = 0\n",
        "        for x, y in trainloader:\n",
        "            loss = 0\n",
        "            x = x.cuda()\n",
        "            y = y.cuda()\n",
        "            y_hat = ...\n",
        "\n",
        "            loss += criterion_classifier(...)\n",
        "            _, predicted = y_hat.max(1)\n",
        "            corrects += predicted.eq(y).sum().item()\n",
        "            total += y.size(0)\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            t.set_description(f'epoch:{epoch} current accuracy:{round(corrects / total * 100, 2)}%')\n",
        "    return (corrects / total)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train your model. One or two epochs should be enough since we are using transfer learning."
      ],
      "metadata": {
        "id": "wzs1TAmVXXwB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NYtyT44D7-Go"
      },
      "outputs": [],
      "source": [
        "learning_rate = 5e-3\n",
        "epochs = 1\n",
        "optimizer = torch.optim.Adam(model.classifier.parameters(), lr=learning_rate)\n",
        "..."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test your network to validate it has enough classification abilities."
      ],
      "metadata": {
        "id": "vOV8mtfYXoGM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Hsn0o04zF3n"
      },
      "outputs": [],
      "source": [
        "def test(model, dataloader):\n",
        "    test_corrects = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for x, y in dataloader:\n",
        "            x = x.cuda()\n",
        "            y = y.cuda()\n",
        "            y_hat = model(x).argmax(1)\n",
        "            test_corrects += y_hat.eq(y).sum().item()\n",
        "            total += y.size(0)\n",
        "    return test_corrects / total\n",
        "\n",
        "model.eval()\n",
        "test_acc = test(model, testloader)\n",
        "print(f'Test accuracy: {test_acc:.2f} %')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using a single example, we will now use lime to visualize the important parts of the image for our model prediction."
      ],
      "metadata": {
        "id": "Ba3sRtcSYMbf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "idx = 0\n",
        "\n",
        "img = inv_normalize(x[idx])\n",
        "np_img = np.transpose(img.cpu().detach().numpy(), (1,2,0))*255\n",
        "np_img = np_img.astype(np.uint8)\n",
        "plt.imshow(np_img)\n",
        "plt.axis('off')"
      ],
      "metadata": {
        "id": "wJm063ZpHrUh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's first verify our model prediction:"
      ],
      "metadata": {
        "id": "aEkocJ9kYi72"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input = x[idx].unsqueeze(0).cuda()\n",
        "output = model(input)\n",
        "_, prediction = torch.topk(output, 1)\n",
        "print(f\"Model's prediction: {labels[prediction.item()]}\")"
      ],
      "metadata": {
        "id": "3gPOMxM3xIs-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "LIME provide a ```LimeImageExplainer``` to deal with images.\n",
        "However, the  ```LimeImageExplainer``` requires a callable function that will directly produce predictions for a list of images (the perturbed from the original images) in the form of a numpy array.  \n",
        "Our pytorch model works with pytorch ```Tensor``` mini-batches and outputs ```Tensor``` objects.  \n",
        "We thus need to wrap our model and the associated pre-processing into a single callable function to use the ```LimeImageExplainer```."
      ],
      "metadata": {
        "id": "1mnNQ7jlZ0YD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "lime_transform = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(means, stds),\n",
        "    ])\n",
        "\n",
        "def batch_predict(images):\n",
        "  with torch.no_grad():\n",
        "    model.eval()\n",
        "    batch = torch.stack(tuple(lime_transform(i) for i in images), dim=0)\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    batch = batch.to(device)\n",
        "    logits = model(batch)\n",
        "  return logits.detach().cpu().numpy()"
      ],
      "metadata": {
        "id": "CbSTIkPyDM5p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We now can use the Lime explainer to visualize which parts of the images were the most important."
      ],
      "metadata": {
        "id": "BviJvjTWZ4Lf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from lime import lime_image\n",
        "from skimage.segmentation import mark_boundaries\n",
        "\n",
        "explainer = lime_image.LimeImageExplainer()\n",
        "explanation = explainer.explain_instance(np_img, \n",
        "                                         batch_predict, # classification function\n",
        "                                         top_labels=5, \n",
        "                                         hide_color=0, \n",
        "                                         num_samples=1000)\n",
        "\n",
        "temp, mask = explanation.get_image_and_mask(explanation.top_labels[0], positive_only=False, num_features=10, hide_rest=False)\n",
        "img_boundry = mark_boundaries(temp/255.0, mask)\n",
        "plt.imshow(img_boundry)"
      ],
      "metadata": {
        "id": "Bi525ZiGE-JW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model specific methods\n",
        "We just saw many methods for model agnostic interpretability.  \n",
        "The second part of this session is dedicated to model-specific methods.  \n",
        "In particular, we will implement methods specific to neural networks."
      ],
      "metadata": {
        "id": "EqMXh4lZaK4e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Vanilla gradient back-propagation\n",
        "\n",
        "We will now implement three methods to generate saliency maps on our images.  \n",
        "One of the simplest ways to generate saliency maps is certainly to backpropagate the gradients of the predicted output directly to the image input.  This method, called vanilla gradient backpropagation, is presented in this [article](Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps).\n",
        "\n",
        "Let's first visualize an image for which we will generate a saliency map according to our model's prediction."
      ],
      "metadata": {
        "id": "s7qRGlB_u85-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np \n",
        "\n",
        "x, _ = next(iter(trainloader))\n",
        "idx = 0\n",
        "\n",
        "img = x[idx]\n",
        "np_img = np.transpose(inv_normalize(img).cpu().detach().numpy(), (1,2,0))\n",
        "plt.imshow(np_img)\n",
        "plt.axis('off')"
      ],
      "metadata": {
        "id": "zLMiENqtu--D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "By default, input tensors do not require to generate gradients in Pytorch.  \n",
        "Thus, we first need to set the image to catch the gradient during the backward pass."
      ],
      "metadata": {
        "id": "0LinUlN7v4RR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "img = img.unsqueeze(0).cuda() # we need to set the input on GPU before the requires_grad operation!\n",
        "img.requires_grad_();"
      ],
      "metadata": {
        "id": "Vri3nLsAvUJf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will now compute the model's prediction for this image and backpropagate from this prediction to the image."
      ],
      "metadata": {
        "id": "wEMUhj3hwhWX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "output = model(img)\n",
        "output_idx = output.argmax()\n",
        "output_max = output[0, output_idx]\n",
        "\n",
        "output_max.backward()"
      ],
      "metadata": {
        "id": "J3sok1YRvn-q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can now generate a saliency map were important pixels will correspond to important gradients!"
      ],
      "metadata": {
        "id": "EkUxlktjw9Xs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "saliency, _ = torch.max(img.grad.data.abs(), dim=1) \n",
        "saliency = saliency.squeeze(0)\n",
        "\n",
        "\n",
        "plt.figure(figsize=(15,10))\n",
        "plt.subplot(1,2,1)\n",
        "plt.imshow(np_img)\n",
        "plt.axis('off')\n",
        "plt.subplot(1,2,2)\n",
        "plt.imshow(saliency.cpu(), cmap='hot')\n",
        "plt.axis('off')\n"
      ],
      "metadata": {
        "id": "eflE0Kv6vymQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Try with different images."
      ],
      "metadata": {
        "id": "xGdX8kjIxX6g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "..."
      ],
      "metadata": {
        "id": "I-hQThOr91CJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Smooth grad\n",
        "A simple way to generate smoother visualizations called [Smooth-grad](https://arxiv.org/pdf/1706.03825.pdf) consists of averaging saliency maps from augmented versions of the original image.  \n",
        "Complete the following function to generate the gradient of an image according to the model's prediction."
      ],
      "metadata": {
        "id": "1ciHu8Mq8KtO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_vanilla_grad(img, model):\n",
        "  ...#retain gradients\n",
        "  ...\n",
        "\n",
        "  return img.grad"
      ],
      "metadata": {
        "id": "y_VTS4MJ1BlL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will now generate perturbated versions of the image by adding a gaussian noise to the original image.  \n",
        "For every image generated we will compute the corresponding gardients and average them to generate the final saliency map."
      ],
      "metadata": {
        "id": "XhyNzQs8y1yG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "stdev_spread=0.15\n",
        "n_samples=100\n",
        "stdev = stdev_spread * (img.max() - img.min())\n",
        "total_gradients = torch.zeros_like(img, device='cuda')\n",
        "\n",
        "for i in range(n_samples):\n",
        "    noise = np.random.normal(0, stdev.item(), img.shape).astype(np.float32)\n",
        "    noisy_img = img + torch.tensor(noise, device='cuda', requires_grad=True)\n",
        "    grad= get_vanilla_grad(noisy_img, model)\n",
        "    total_gradients += grad * grad #using the square of the gradients generates smoother visualizations\n",
        "    #total_gradients += grad\n",
        "total_gradients /= n_samples\n",
        "\n",
        "saliency, _ = torch.max(total_gradients.abs(), dim=1) \n",
        "saliency = saliency.squeeze(0)\n",
        "\n",
        "\n",
        "plt.figure(figsize=(15,10))\n",
        "plt.subplot(1,2,1)\n",
        "plt.imshow(np_img)\n",
        "plt.axis('off')\n",
        "plt.subplot(1,2,2)\n",
        "plt.imshow(saliency.cpu(), cmap='hot')\n",
        "plt.axis('off')"
      ],
      "metadata": {
        "id": "oEwQ7GmQ1jyi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Try it with other images."
      ],
      "metadata": {
        "id": "zQ0nTGsBzp-Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "..."
      ],
      "metadata": {
        "id": "eiwiMQHk-Fs6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Grad-CAM\n",
        "Instead of propagating the gradients to the inputs, the [Grad-CAM](https://arxiv.org/abs/1610.02391) method generates a saliency map by multiplying the outputs of the final feature map with an average of its gradients.  It thus generates coarse saliency maps, sometimes more relevant than pixels.  \n",
        "We will create a 'hook' to keep both the activations and the gardients of a network layer.  \n",
        "This operation is a bit tricky, just keep it mind that it is a way to keep activations and gradients in a single object during the forward and the backward pass."
      ],
      "metadata": {
        "id": "bMW4AGjJARWJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class HookFeatures():\n",
        "    def __init__(self, module):\n",
        "        self.feature_hook = module.register_forward_hook(self.feature_hook_fn)\n",
        "    def feature_hook_fn(self, module, input, output):\n",
        "        self.features = output.clone().detach()\n",
        "        self.gradient_hook = output.register_hook(self.gradient_hook_fn)\n",
        "    def gradient_hook_fn(self, grad):\n",
        "        self.gradients = grad\n",
        "    def close(self):\n",
        "        self.feature_hook.remove()\n",
        "        self.gradient_hook.remove()"
      ],
      "metadata": {
        "id": "_sHWPVIoAUDk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will 'hook' the activations and gradients of the last convolutional layer."
      ],
      "metadata": {
        "id": "-wJA1Ppa2Sf9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(model)\n",
        "hook = HookFeatures(model.features[19])"
      ],
      "metadata": {
        "id": "m-g3YLoE2PVi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Similar to what we did before, we will backpropagate the gradients of the predicted output on the feature map this time and get both the activations and the gradients thanks to our hook on the last convolutional layer."
      ],
      "metadata": {
        "id": "CmeLAfyk2hP-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "output = model(img)\n",
        "output_idx = output.argmax()\n",
        "output_max = output[0, output_idx]\n",
        "output_max.backward()\n",
        "\n",
        "gradients = hook.gradients\n",
        "activations = hook.features\n",
        "pooled_gradients = torch.mean(gradients, dim=[0, 2, 3]) # we take the average gradient of every chanels\n",
        "for i in range(activations.shape[1]):\n",
        "    activations[:, i, :, :] *= pooled_gradients[i] # we multiply every chanels of the feature map with their corresponding averaged gradients"
      ],
      "metadata": {
        "id": "JDTxSM9L6Xd6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can now take the average of all channels of the gradient weighted feature map to generate a heat map, keeping only the positive values to get the positive influences only.  \n",
        "We also need to reshape the generated heat map to math the original input size."
      ],
      "metadata": {
        "id": "4RHfl02a3q0m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "\n",
        "heatmap = torch.mean(activations, dim=1).squeeze()\n",
        "heatmap = np.maximum(heatmap.detach().cpu(), 0)\n",
        "heatmap /= torch.max(heatmap)\n",
        "heatmap = cv2.resize(np.float32(heatmap), (img.shape[2], img.shape[3]))\n",
        "heatmap = np.uint8(255 * heatmap)\n",
        "heatmap = cv2.applyColorMap(heatmap, cv2.COLORMAP_RAINBOW) / 255\n",
        "superposed_img = (heatmap) * 0.4 + np_img \n",
        "plt.figure(figsize=(8,8))\n",
        "plt.imshow(np.clip(superposed_img,0,1))\n",
        "plt.axis('off')"
      ],
      "metadata": {
        "id": "4GLSW8qVHR2q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We must remove our hook, it won't be used in the rest of the session."
      ],
      "metadata": {
        "id": "ayXfEGQy4P4Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hook.close()"
      ],
      "metadata": {
        "id": "LV9IkpD7nOyl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Captum  \n",
        "\n",
        "[Captum](https://captum.ai/)  is a library developed by Facebook to generate explanations on Pytorch models.  \n",
        "It implements various other saliency maps methods that we did not cover during our class. \n",
        "You should look at the doc and find out [other possible methods](https://captum.ai/docs/algorithms) to gain interpretability in your Pytorch models.  \n",
        "We will quickly try some of them so you know how to use them if you need to."
      ],
      "metadata": {
        "id": "ko1tBuUhE8aB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vkCYNgf-J6qN"
      },
      "outputs": [],
      "source": [
        "!pip install captum"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "12e56myGJ3kQ"
      },
      "outputs": [],
      "source": [
        "from captum.attr import IntegratedGradients\n",
        "from captum.attr import GradientShap\n",
        "from captum.attr import Occlusion\n",
        "from captum.attr import NoiseTunnel\n",
        "from captum.attr import GuidedGradCam\n",
        "from captum.attr import DeepLift\n",
        "from captum.attr import visualization as viz"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_heatmap(attributions, img):\n",
        "  _ = viz.visualize_image_attr_multiple(np.transpose(attributions.squeeze().cpu().detach().numpy(), (1,2,0)),\n",
        "                              np.transpose(inv_normalize(img).squeeze().cpu().detach().numpy(), (1,2,0)),\n",
        "                              methods=[\"original_image\", \"heat_map\"],\n",
        "                              signs=['all', 'positive'],\n",
        "                              cmap='hot',\n",
        "                              show_colorbar=True)\n",
        "\n",
        "# Integradted gradients (https://arxiv.org/abs/1703.01365) \n",
        "integrated_gradients = IntegratedGradients(model)\n",
        "attributions = integrated_gradients.attribute(img, target=output_idx, n_steps=200, internal_batch_size=1)\n",
        "\n",
        "plot_heatmap(attributions, img)"
      ],
      "metadata": {
        "id": "Xw7EzoAq51sN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I9PussT6q3Gg"
      },
      "outputs": [],
      "source": [
        "#Noise tunnel (SmoothGrad, VarGrad: https://arxiv.org/abs/1810.03307)\n",
        "noise_tunnel = NoiseTunnel(integrated_gradients)\n",
        "attributions = noise_tunnel.attribute(img, nt_samples_batch_size=1, nt_samples=10, nt_type='smoothgrad_sq', target=output_idx)\n",
        "\n",
        "plot_heatmap(attributions, img)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Occlusion (https://arxiv.org/abs/1311.2901)\n",
        "occlusion = Occlusion(model)\n",
        "attributions = occlusion.attribute(img,\n",
        "                                  strides = (3, 8, 8),\n",
        "                                  target=output_idx,\n",
        "                                  sliding_window_shapes=(3,15, 15),\n",
        "                                  baselines=0)\n",
        "\n",
        "plot_heatmap(attributions, img)"
      ],
      "metadata": {
        "id": "PdBAjCeySQcz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#DeepLift (https://arxiv.org/pdf/1704.02685.pdf)\n",
        "dl = DeepLift(model)\n",
        "attributions = dl.attribute(img, target=output_idx, baselines=img * 0)\n",
        "\n",
        "plot_heatmap(attributions, img)"
      ],
      "metadata": {
        "id": "YW34crn9Vo5i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Guided Grad-CAM (https://arxiv.org/abs/1610.02391)\n",
        "guided_gc = GuidedGradCam(model, model.features[19])\n",
        "attribution = guided_gc.attribute(img, target=output_idx)\n",
        "plot_heatmap(attributions, img)"
      ],
      "metadata": {
        "id": "p3_D4BBbWz3z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TzdhApmtnwlh"
      },
      "source": [
        "## Feature visualization\n",
        "Previous methods, generating saliency maps were model-specific local-interpretability methods.  \n",
        "We will now implement a global interpretability called feature visualization.  \n",
        "I really encourage you to read the [distill publication](https://distill.pub/2017/feature-visualization/) presenting the methods.  The obtained visualizations are superb!\n",
        "The principle of the method is straightforward. It consists of optimizing a random image to maximize the output of a neural network unit.  \n",
        "A unit can be a neuron, a channel of a feature map, or an entire feature map.  \n",
        "In this practical session, we will focus on channels.  \n",
        "Let's first have a look at our network architecture."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XfJNJIXbGlWC"
      },
      "outputs": [],
      "source": [
        "model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once again, we will create a hook to keep the activation of intermediate layers.  \n",
        "However, we won't need to keep the gradients this time."
      ],
      "metadata": {
        "id": "1WuhEYp4Kn7P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FeaturesHook():\n",
        "    def __init__(self, module):\n",
        "        self.hook = module.register_forward_hook(self.hook_fn)\n",
        "    def hook_fn(self, module, input, output):\n",
        "        self.features = output\n",
        "    def close(self):\n",
        "        self.hook.remove()"
      ],
      "metadata": {
        "id": "GDthf9CzKloB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following code will compute the feature visualization for one channel of one layer.  \n",
        "We begin by initializing a random image and creating a hook on the desired layer.  \n",
        "Then we will do a forward pass of our image through the network and compute a loss to maximize the hooked layer's desired channel.  \n",
        "We will repeat this operation several epochs. "
      ],
      "metadata": {
        "id": "NrzkFp14Mh_S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def visualize_feature(model, layer_idx, channel_idx):\n",
        "  img = torch.rand((1,3,224,224), requires_grad=True, device=\"cuda\") #initialize a random image\n",
        "  optimizer = torch.optim.Adam([img], lr=0.1, weight_decay=1e-6)\n",
        "  features_hook =  FeaturesHook(model.features[layer_idx]) # hook the desired layer\n",
        "  for n in range(20):  \n",
        "        optimizer.zero_grad()\n",
        "        model(img) #forward pass\n",
        "        features_map = features_hook.features\n",
        "        loss = -features_map[0, channel_idx].mean() # maximize channel's output\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "  features_hook.close()\n",
        "  img = img.squeeze(0)\n",
        "  img = inv_normalize(img).cpu().detach().numpy()\n",
        "  img = np.transpose(img, (1,2,0))\n",
        "  img = np.clip(img, 0, 1)\n",
        "  plt.imshow(img)\n",
        "  plt.axis('off')\n",
        "  plt.tight_layout()\n",
        "\n",
        "#visualize layer one channel 2 \n",
        "visualize_feature(model, 1, 2)"
      ],
      "metadata": {
        "id": "BOCsazzOK6CX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Complete the following code to generate feature visaulizations of various filters of layer 1."
      ],
      "metadata": {
        "id": "PmmxVqDsNi5H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "layer = 1\n",
        "\n",
        "plt.figure(figsize=(20,25))\n",
        "for i, filter in enumerate(range(0, 10), 1):\n",
        "  plt.subplot(5,5,i)\n",
        "  ..."
      ],
      "metadata": {
        "id": "5NW8q0mpNBuX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Try to vizsualize deeper layers (5, 9, 17 for instance)"
      ],
      "metadata": {
        "id": "9dHU7TxWN262"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "layer = 5\n",
        "\n",
        "plt.figure(figsize=(20,25))\n",
        "for i, filter in enumerate(range(0, 10), 1):\n",
        "  plt.subplot(5,5,i)\n",
        "  ..."
      ],
      "metadata": {
        "id": "jrdFP7NwNPau"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "layer = 9\n",
        "\n",
        "plt.figure(figsize=(20,25))\n",
        "for i, filter in enumerate(range(0, 10), 1):\n",
        "  plt.subplot(5,5,i)\n",
        "  ..."
      ],
      "metadata": {
        "id": "HuyJPOiiNUe5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "layer = 17\n",
        "\n",
        "plt.figure(figsize=(20,25))\n",
        "for i, filter in enumerate(range(0, 10), 1):\n",
        "  plt.subplot(5,5,i)\n",
        "  ..."
      ],
      "metadata": {
        "id": "DevgoMqbNX0X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Obtaining nice visualizations as in the original publication requires some additional tricks.  \n",
        "Feel free to read the publication and try it on other more sophisticated networks."
      ],
      "metadata": {
        "id": "RUVrhAxMOPiR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "KUtYqHLOiHvm"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "TP_interpretability.ipynb",
      "toc_visible": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}