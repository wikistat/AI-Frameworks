{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"index.html","text":"Data Science Artificial Intelligence Frameworks This course is the continuation of the Machine Learning and High Dimensional & Deep Learning courses. While these courses focused on machine learning and deep learning theory and methods, this course aims to provide you with the tools to apply these methods in concrete situations. During this course you will: Learn to use cloud computing to train your models efficiently and monitor their trainings Learn about code versioning and containers to share your code and produce reproducible results Learn to process text data for natural language processing tasks Learn to train autonomous agents to play video games using Reinforcement Learning Learn how to implement a recommender system Learn how to interpret your machine learning models NB : Some contents from previous years are still available on the repository (like Spark ) but are not treated anymore. Knowledge requirements Python Tutorial Elementary statistic tools Data Exploration and Clustering . Machine Learning High Dimensional & Deep Learning","title":"Home"},{"location":"index.html#artificial-intelligence-frameworks","text":"This course is the continuation of the Machine Learning and High Dimensional & Deep Learning courses. While these courses focused on machine learning and deep learning theory and methods, this course aims to provide you with the tools to apply these methods in concrete situations. During this course you will: Learn to use cloud computing to train your models efficiently and monitor their trainings Learn about code versioning and containers to share your code and produce reproducible results Learn to process text data for natural language processing tasks Learn to train autonomous agents to play video games using Reinforcement Learning Learn how to implement a recommender system Learn how to interpret your machine learning models NB : Some contents from previous years are still available on the repository (like Spark ) but are not treated anymore.","title":"Artificial Intelligence Frameworks"},{"location":"index.html#knowledge-requirements","text":"Python Tutorial Elementary statistic tools Data Exploration and Clustering . Machine Learning High Dimensional & Deep Learning","title":"Knowledge requirements"},{"location":"dev.html","text":"Development for Data Scientist: Pytorch and Python Script Course Course notebook: Notebook Practical session For this session, you will have to write a script to train a small neural network on the MNIST dataset using Pytorch. During training, you will use tensorboard to: monitor your network across epochs manage your experiments and hyper-parameters provide some visualizations. Practical session repository: If you haven't already done so, create an account on Github . Then fork this repository and clone it on your computer. The network class: Using the figure above, fill in the following code, in the mnist_net.py file, to create the network class: The method __init__() should instantiate all the layers that will be used by the network. The method forward() describes the forward graph of your network. All the pooling operations and activation functions are realized in this method. Do not forget to change the shape of your input before the first linear layer using torch.flatten(...) or x.view(...) . import torch import torch.nn as nn import torch.nn.functional as F class MNISTNet(nn.Module): def __init__(self): super(MNNISTNet, self).__init__() self.conv1 = nn.Conv2d(...) self.conv2 = nn.Conv2d(...) self.pool = nn.MaxPool2d(...) self.fc1 = nn.Linear(...) self.fc2 = nn.Linear(...) self.fc3 = nn.Linear(...) def forward(self, x): x = F.relu(self.conv1(x)) # First convolution followed by x = self.pool(x) # a relu activation and a max pooling# x = ... ... x = self.fc3(x) return x The training script The previous file contained our model class. We will now complete the training script train_mnist.py . This file will be used as a python script to train a neural network on the MNIST Dataset. The train() and test() methods are already implemented. import argparse from statistics import mean import torch import torchvision import torchvision.transforms as transforms import torch.nn as nn import torch.nn.functional as F import torch.optim as optim from tqdm import tqdm from models import MNISTNet # setting device on GPU if available, else CPU device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') def train(net, optimizer, loader, epochs=10): criterion = nn.CrossEntropyLoss() for epoch in range(epochs): running_loss = [] t = tqdm(loader) for x, y in t: x, y = x.to(device), y.to(device) outputs = net(x) loss = criterion(outputs, y) running_loss.append(loss.item()) optimizer.zero_grad() loss.backward() optimizer.step() t.set_description(f'training loss: {mean(running_loss)}') def test(model, dataloader): test_corrects = 0 total = 0 with torch.no_grad(): for x, y in dataloader: x = x.to(device) y = y.to(device) y_hat = model(x).argmax(1) test_corrects += y_hat.eq(y).sum().item() total += y.size(0) return test_corrects / total We will now implement the main method that will be called every time the python script is executed. We would like to give the user the possibility to adjust some parameters of the learning process, such as: The batch size The learning rate The number of training epochs To do so we will use Python argparse module . This module is used to write user-friendly command-line interfaces. Adding an argument to a python script using argaparse is pretty straightforward. First you need to import the argparse module and instanciate a parser within the main method: import argparse if __name__=='__main__': parser = argparse.ArgumentParser() Then, just add a new argument to the parser precising the argument's name, its type, and optionaly a default value and an helping message. parser.add_argument('--exp_name', type=str, default = 'MNIST', help='experiment name') Finaly, you can use the arguments in the script by using an args variable. args = parser.parse_args() print(args.exp_name) Complete the main method to parse the four possible arguments provided when executing the script: if __name__=='__main__': parser = argparse.ArgumentParser() parser.add_argument('--exp_name', type=str, default = 'MNIST', help='experiment name') parser.add_argument(...) parser.add_argument(...) parser.add_argument(...) args = parser.parse_args() exp_name = args.exp_name epochs = ... batch_size = ... lr = ... The following code instantiates two data loaders : one loading data from the training set, the other one from the test set. # transforms transform = transforms.Compose( [transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))]) # datasets trainset = torchvision.datasets.MNIST('./data', download=True, train=True, transform=transform) testset = torchvision.datasets.MNIST('./data', download=True, train=False, transform=transform) # dataloaders trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=2) testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=2) Instantiate a MNISTNet and a SGD optimizer using the learning rate provided in the script arguments and use the train method to train your network and the test method to compute the test accuracy. net = ... # setting net on device(GPU if available, else CPU) net = net.to(device) optimizer = optim.SGD(...) train(...) test_acc = test(...) print(f'Test accuracy:{test_acc}') Finally, save your model using the torch.save method. torch.save(net.state_dict(), 'mnist_net.pth') You should now be able to run your python script using the following command in your terminal: python train_mnist.py --epochs=5 --lr=1e-3 --batch_size=64 Monitoring and experiment management Training our model on MNIST is pretty fast. Nonetheless, in most cases, training a network may be very long. For such cases, it is essential to log partial results during training to ensure that everything is behaving as expected. A very famous tool to monitor your experiments in deep learning is Tensorboard. The main object used by Tensorboard is a SummaryWriter . Add the following import: from torch.utils.tensorboard import SummaryWriter and modify the train method to take an additional argument named writer . Use its add_scalar method to log the training loss for every epoch. def train(net, optimizer, loader, writer, epochs=10): criterion = nn.CrossEntropyLoss() for epoch in range(epochs): running_loss = [] t = tqdm(loader) for x, y in t: x, y = x.to(device), y.to(device) outputs = net(x) loss = criterion(outputs, y) running_loss.append(loss.item()) optimizer.zero_grad() loss.backward() optimizer.step() t.set_description(f'training loss: {mean(running_loss)}') writer.add_scalar('training loss', mean(running_loss), epoch) In the main method instantiate a SummaryWriter with writer = SummaryWriter(f'runs/MNIST') and add it as argument to the train method. Re-run your script and check your tensorboard logs using in a separate terminal: tensorboard --logdir runs You can use tensorboard to log many different things such as your network computational graph, images, samples from your dataset, embeddings, or even use it for experiment management. Add a new method to the MNISTNet class to get the embeddings computed after the last convolutional layer. def get_features(self, x): x = self.pool(F.relu(self.conv1(x))) x = self.pool(F.relu(self.conv2(x))) x = x.view(-1, 16 * 4 * 4) return x Now these following code to the end of your main function to log the embeddings and the computational graph in tensorboard. #add embeddings to tensorboard perm = torch.randperm(len(trainset.data)) images, labels = trainset.data[perm][:256], trainset.targets[perm][:256] images = images.unsqueeze(1).float().to(device) with torch.no_grad(): embeddings = net.get_features(images) writer.add_embedding(embeddings, metadata=labels, label_img=images, global_step=1) # save networks computational graph in tensorboard writer.add_graph(net, images) # save a dataset sample in tensorboard img_grid = torchvision.utils.make_grid(images[:64]) writer.add_image('mnist_images', img_grid) Re-run your script and restart tensorboard. Visualize the network computational graph by clicking on Graph . You should see something similar to this: Click on the inactive button and choose projector to look at the embeddings computed by your network Deploying your model with Gradio We will now create a simple application to guess the numbers drawn by a user from our saved model. We will use the Gradio library to quickly prototype machine learning applications and demonstrations with user friendly web interfaces. Install the library: pip install gradio Creating an application with Gradio is done through the use of its Interface class The core Interface class is initialized with three required parameters: fn: the function to wrap a user interface around inputs: which component(s) to use for the input, e.g. \"text\" or \"image\" or \"audio\" outputs: which component(s) to use for the output, e.g. \"text\" or \"image\" \"label\" Gradio includes more than 20 different components , most of which can be used as inputs or outputs. In this example, we will use a sketchpad (which is an instance of the Image component )component for the input and a Label component for the output. gr.Interface(fn=recognize_digit, inputs=\"sketchpad\", outputs=gr.outputs.Label(num_top_classes=3), live=True, description=\"Draw a number on the sketchpad to see the model's prediction.\", ).launch(debug=True, share=True); Complete the mnist_app.py file so that the weights path is provided by the user and run your application with the following command: python mnist_app.py --weights_path [path_to_the weights] Is your model accurate with your drawings? Do you know why it is less accurate than on MNIST? Git Commit all the modifications you have made to the repository as well as the weights and push them to your remote repository.","title":"Python Scripts"},{"location":"dev.html#development-for-data-scientist","text":"","title":"Development for Data Scientist:"},{"location":"dev.html#pytorch-and-python-script","text":"","title":"Pytorch and Python Script"},{"location":"dev.html#course","text":"","title":"Course"},{"location":"dev.html#course-notebook","text":"Notebook","title":"Course notebook:"},{"location":"dev.html#practical-session","text":"For this session, you will have to write a script to train a small neural network on the MNIST dataset using Pytorch. During training, you will use tensorboard to: monitor your network across epochs manage your experiments and hyper-parameters provide some visualizations.","title":"Practical session"},{"location":"dev.html#practical-session-repository","text":"If you haven't already done so, create an account on Github . Then fork this repository and clone it on your computer.","title":"Practical session repository:"},{"location":"dev.html#the-network-class","text":"Using the figure above, fill in the following code, in the mnist_net.py file, to create the network class: The method __init__() should instantiate all the layers that will be used by the network. The method forward() describes the forward graph of your network. All the pooling operations and activation functions are realized in this method. Do not forget to change the shape of your input before the first linear layer using torch.flatten(...) or x.view(...) . import torch import torch.nn as nn import torch.nn.functional as F class MNISTNet(nn.Module): def __init__(self): super(MNNISTNet, self).__init__() self.conv1 = nn.Conv2d(...) self.conv2 = nn.Conv2d(...) self.pool = nn.MaxPool2d(...) self.fc1 = nn.Linear(...) self.fc2 = nn.Linear(...) self.fc3 = nn.Linear(...) def forward(self, x): x = F.relu(self.conv1(x)) # First convolution followed by x = self.pool(x) # a relu activation and a max pooling# x = ... ... x = self.fc3(x) return x","title":"The network class:"},{"location":"dev.html#the-training-script","text":"The previous file contained our model class. We will now complete the training script train_mnist.py . This file will be used as a python script to train a neural network on the MNIST Dataset. The train() and test() methods are already implemented. import argparse from statistics import mean import torch import torchvision import torchvision.transforms as transforms import torch.nn as nn import torch.nn.functional as F import torch.optim as optim from tqdm import tqdm from models import MNISTNet # setting device on GPU if available, else CPU device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') def train(net, optimizer, loader, epochs=10): criterion = nn.CrossEntropyLoss() for epoch in range(epochs): running_loss = [] t = tqdm(loader) for x, y in t: x, y = x.to(device), y.to(device) outputs = net(x) loss = criterion(outputs, y) running_loss.append(loss.item()) optimizer.zero_grad() loss.backward() optimizer.step() t.set_description(f'training loss: {mean(running_loss)}') def test(model, dataloader): test_corrects = 0 total = 0 with torch.no_grad(): for x, y in dataloader: x = x.to(device) y = y.to(device) y_hat = model(x).argmax(1) test_corrects += y_hat.eq(y).sum().item() total += y.size(0) return test_corrects / total We will now implement the main method that will be called every time the python script is executed. We would like to give the user the possibility to adjust some parameters of the learning process, such as: The batch size The learning rate The number of training epochs To do so we will use Python argparse module . This module is used to write user-friendly command-line interfaces. Adding an argument to a python script using argaparse is pretty straightforward. First you need to import the argparse module and instanciate a parser within the main method: import argparse if __name__=='__main__': parser = argparse.ArgumentParser() Then, just add a new argument to the parser precising the argument's name, its type, and optionaly a default value and an helping message. parser.add_argument('--exp_name', type=str, default = 'MNIST', help='experiment name') Finaly, you can use the arguments in the script by using an args variable. args = parser.parse_args() print(args.exp_name) Complete the main method to parse the four possible arguments provided when executing the script: if __name__=='__main__': parser = argparse.ArgumentParser() parser.add_argument('--exp_name', type=str, default = 'MNIST', help='experiment name') parser.add_argument(...) parser.add_argument(...) parser.add_argument(...) args = parser.parse_args() exp_name = args.exp_name epochs = ... batch_size = ... lr = ... The following code instantiates two data loaders : one loading data from the training set, the other one from the test set. # transforms transform = transforms.Compose( [transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))]) # datasets trainset = torchvision.datasets.MNIST('./data', download=True, train=True, transform=transform) testset = torchvision.datasets.MNIST('./data', download=True, train=False, transform=transform) # dataloaders trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=2) testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=2) Instantiate a MNISTNet and a SGD optimizer using the learning rate provided in the script arguments and use the train method to train your network and the test method to compute the test accuracy. net = ... # setting net on device(GPU if available, else CPU) net = net.to(device) optimizer = optim.SGD(...) train(...) test_acc = test(...) print(f'Test accuracy:{test_acc}') Finally, save your model using the torch.save method. torch.save(net.state_dict(), 'mnist_net.pth') You should now be able to run your python script using the following command in your terminal: python train_mnist.py --epochs=5 --lr=1e-3 --batch_size=64","title":"The training script"},{"location":"dev.html#monitoring-and-experiment-management","text":"Training our model on MNIST is pretty fast. Nonetheless, in most cases, training a network may be very long. For such cases, it is essential to log partial results during training to ensure that everything is behaving as expected. A very famous tool to monitor your experiments in deep learning is Tensorboard. The main object used by Tensorboard is a SummaryWriter . Add the following import: from torch.utils.tensorboard import SummaryWriter and modify the train method to take an additional argument named writer . Use its add_scalar method to log the training loss for every epoch. def train(net, optimizer, loader, writer, epochs=10): criterion = nn.CrossEntropyLoss() for epoch in range(epochs): running_loss = [] t = tqdm(loader) for x, y in t: x, y = x.to(device), y.to(device) outputs = net(x) loss = criterion(outputs, y) running_loss.append(loss.item()) optimizer.zero_grad() loss.backward() optimizer.step() t.set_description(f'training loss: {mean(running_loss)}') writer.add_scalar('training loss', mean(running_loss), epoch) In the main method instantiate a SummaryWriter with writer = SummaryWriter(f'runs/MNIST') and add it as argument to the train method. Re-run your script and check your tensorboard logs using in a separate terminal: tensorboard --logdir runs You can use tensorboard to log many different things such as your network computational graph, images, samples from your dataset, embeddings, or even use it for experiment management. Add a new method to the MNISTNet class to get the embeddings computed after the last convolutional layer. def get_features(self, x): x = self.pool(F.relu(self.conv1(x))) x = self.pool(F.relu(self.conv2(x))) x = x.view(-1, 16 * 4 * 4) return x Now these following code to the end of your main function to log the embeddings and the computational graph in tensorboard. #add embeddings to tensorboard perm = torch.randperm(len(trainset.data)) images, labels = trainset.data[perm][:256], trainset.targets[perm][:256] images = images.unsqueeze(1).float().to(device) with torch.no_grad(): embeddings = net.get_features(images) writer.add_embedding(embeddings, metadata=labels, label_img=images, global_step=1) # save networks computational graph in tensorboard writer.add_graph(net, images) # save a dataset sample in tensorboard img_grid = torchvision.utils.make_grid(images[:64]) writer.add_image('mnist_images', img_grid) Re-run your script and restart tensorboard. Visualize the network computational graph by clicking on Graph . You should see something similar to this: Click on the inactive button and choose projector to look at the embeddings computed by your network","title":"Monitoring and experiment management"},{"location":"dev.html#deploying-your-model-with-gradio","text":"We will now create a simple application to guess the numbers drawn by a user from our saved model. We will use the Gradio library to quickly prototype machine learning applications and demonstrations with user friendly web interfaces. Install the library: pip install gradio Creating an application with Gradio is done through the use of its Interface class The core Interface class is initialized with three required parameters: fn: the function to wrap a user interface around inputs: which component(s) to use for the input, e.g. \"text\" or \"image\" or \"audio\" outputs: which component(s) to use for the output, e.g. \"text\" or \"image\" \"label\" Gradio includes more than 20 different components , most of which can be used as inputs or outputs. In this example, we will use a sketchpad (which is an instance of the Image component )component for the input and a Label component for the output. gr.Interface(fn=recognize_digit, inputs=\"sketchpad\", outputs=gr.outputs.Label(num_top_classes=3), live=True, description=\"Draw a number on the sketchpad to see the model's prediction.\", ).launch(debug=True, share=True); Complete the mnist_app.py file so that the weights path is provided by the user and run your application with the following command: python mnist_app.py --weights_path [path_to_the weights] Is your model accurate with your drawings? Do you know why it is less accurate than on MNIST?","title":"Deploying your model with Gradio"},{"location":"dev.html#git","text":"Commit all the modifications you have made to the repository as well as the weights and push them to your remote repository.","title":"Git"},{"location":"docker.html","text":"Development for Data Scientist: Docker Course (Video by Brendan Guillouet) Slides Practical Session In this practical session, you will now run your code through a Docker container. Using docker in data science projects has two advantages: Improving the reproducibility of the results Facilitating the portability and deployment In this session, we will try to package the code from our Gradio applications, allowing us to predict digits labels and to colorize images into a Docker image. We will then use this image to instantiate a container that could be hosted on any physical device to run the app. We will first create the Dockerfile corresponding to our environment. On your local machine, create a new file named Dockerfile containing the following code: # Base image from pytorch FROM pytorch/pytorch # Set up for your local zone an UTC information ENV TZ=Europe/Paris RUN ln -snf /usr/share/zoneinfo/$TZ /etc/localtime && echo $TZ > /etc/timezone # Additional librairies RUN pip install gradio tensorboard RUN pip install markupsafe==2.0.1 Take a moment to analyze this dockerfile. As you can see, it is built upon an existing image from Pytorch. Starting from existing images allows for fast prototyping. You may find existing images on DockerHub . The Pytorch image we will be using is available here . If docker is not already installed in your machine, follow this guide to install it. You may now build your first image using the following command: sudo docker build -t [your_image_name] [path_to_your_dockerfile] The image should take a few minutes to build. Once it is done, use the following command to list the available images on your device: sudo docker image ls How many images can you see? What do they refer to? Now that our images are built, we can use them to instantiate containers. Since a container is an instance of an image, we can instantiate several containers using a single image. We will run our first container using the interactive mode. Run the following command to run your fist container: docker run -it --name [your_container_name] [your_image_name] You should now have access to an interactive terminal within your container. On this terminal, open a Python console and check that Pytorch is installed. import torch print(torch.__version__) Quit the Python console and quit your container using ctrl+d . You can list all your running containers using the following command: sudo docker container ls Your container is closed and does not appear. To list all the existing containers, add the -a to the previous command. sudo docker container ls -a Start your containers using: sudo docker start [container_id_or_name] Check that it is now listed as started. You can have access to its interactive mode using the attach command: sudo docker attach [container_id_or_name] You can delete a container using the rm command: sudo docker rm [container_id_or_name] We will now see how to share data between the container and the machine it is running on. First create a folder containing the files: colorize_app.py mnist_app.py mnist.pth unet.pth Create a new container, this time mounting a shared volume with the following command: docker run -it --name [container_name] -v ~/[absolute_path_to_your_folder_to_share]:/workspace/[folder_name_in_the_container] [image_name] for instance: docker run -it --name my_container_name -v ~/workspace/colorize:/workspace/colorize_container my_image_name Try to run one of your Gradio applications using the interactive mode. cd [folder_name] python colorize_app.py Leave the container and look at your folder on your local machine. What can you see? Now try to run your applications on your cloud instance. Send the Dockerfile and the folder containing your applications to your cloud instance. On the cloud instance, build your image and run your container and your app in background mode. sudo docker exec -t my_container_name python ./colorize_container/colorize_app.py --weights_path ./colorize_container/unet.pth That's it! You have deployed a machine learning application on a cloud machine it is now accessible from everywhere. Send the url to one of your classmate and ask him/her to test your app. This is it for this session. Please remember to shutdown your cloud machine with the command: sudo shutdown -h now Do not hesitate to play a little more with Docker. For instance try to train the MNIST classifier directly in your container and to collect the tensorboard logs and the resulting weights on your local machine.","title":"Introduction to Docker"},{"location":"docker.html#development-for-data-scientist","text":"","title":"Development for Data Scientist:"},{"location":"docker.html#docker","text":"","title":"Docker"},{"location":"docker.html#course-video-by-brendan-guillouet","text":"Slides","title":"Course (Video by Brendan Guillouet)"},{"location":"docker.html#practical-session","text":"In this practical session, you will now run your code through a Docker container. Using docker in data science projects has two advantages: Improving the reproducibility of the results Facilitating the portability and deployment In this session, we will try to package the code from our Gradio applications, allowing us to predict digits labels and to colorize images into a Docker image. We will then use this image to instantiate a container that could be hosted on any physical device to run the app. We will first create the Dockerfile corresponding to our environment. On your local machine, create a new file named Dockerfile containing the following code: # Base image from pytorch FROM pytorch/pytorch # Set up for your local zone an UTC information ENV TZ=Europe/Paris RUN ln -snf /usr/share/zoneinfo/$TZ /etc/localtime && echo $TZ > /etc/timezone # Additional librairies RUN pip install gradio tensorboard RUN pip install markupsafe==2.0.1 Take a moment to analyze this dockerfile. As you can see, it is built upon an existing image from Pytorch. Starting from existing images allows for fast prototyping. You may find existing images on DockerHub . The Pytorch image we will be using is available here . If docker is not already installed in your machine, follow this guide to install it. You may now build your first image using the following command: sudo docker build -t [your_image_name] [path_to_your_dockerfile] The image should take a few minutes to build. Once it is done, use the following command to list the available images on your device: sudo docker image ls How many images can you see? What do they refer to? Now that our images are built, we can use them to instantiate containers. Since a container is an instance of an image, we can instantiate several containers using a single image. We will run our first container using the interactive mode. Run the following command to run your fist container: docker run -it --name [your_container_name] [your_image_name] You should now have access to an interactive terminal within your container. On this terminal, open a Python console and check that Pytorch is installed. import torch print(torch.__version__) Quit the Python console and quit your container using ctrl+d . You can list all your running containers using the following command: sudo docker container ls Your container is closed and does not appear. To list all the existing containers, add the -a to the previous command. sudo docker container ls -a Start your containers using: sudo docker start [container_id_or_name] Check that it is now listed as started. You can have access to its interactive mode using the attach command: sudo docker attach [container_id_or_name] You can delete a container using the rm command: sudo docker rm [container_id_or_name] We will now see how to share data between the container and the machine it is running on. First create a folder containing the files: colorize_app.py mnist_app.py mnist.pth unet.pth Create a new container, this time mounting a shared volume with the following command: docker run -it --name [container_name] -v ~/[absolute_path_to_your_folder_to_share]:/workspace/[folder_name_in_the_container] [image_name] for instance: docker run -it --name my_container_name -v ~/workspace/colorize:/workspace/colorize_container my_image_name Try to run one of your Gradio applications using the interactive mode. cd [folder_name] python colorize_app.py Leave the container and look at your folder on your local machine. What can you see? Now try to run your applications on your cloud instance. Send the Dockerfile and the folder containing your applications to your cloud instance. On the cloud instance, build your image and run your container and your app in background mode. sudo docker exec -t my_container_name python ./colorize_container/colorize_app.py --weights_path ./colorize_container/unet.pth That's it! You have deployed a machine learning application on a cloud machine it is now accessible from everywhere. Send the url to one of your classmate and ask him/her to test your app. This is it for this session. Please remember to shutdown your cloud machine with the command: sudo shutdown -h now Do not hesitate to play a little more with Docker. For instance try to train the MNIST classifier directly in your container and to collect the tensorboard logs and the resulting weights on your local machine.","title":"Practical Session"},{"location":"docker_2021.html","text":"Development for Data Scientist: Docker Course Slides Practical Session In this practical session, you will now run your code through a Docker container. Using docker in data science projects has two advantages: * Improving the reproducibility of the results * Facilitating the portability and deployment In this session, we will try to package the code from the previous session, allowing us to train a neural network to colorize images into a Docker image and use this image to instantiate a container on a GCloud instance to run the code. We will first create the Dockerfile corresponding to our environment. On your local machine, create a new file named Dockerfile containing the following code: # Base image from pytorch FROM pytorch/pytorch:1.7.1-cuda11.0-cudnn8-runtime # Set up for your local zone an UTC information ENV TZ=Europe/Paris RUN ln -snf /usr/share/zoneinfo/$TZ /etc/localtime && echo $TZ > /etc/timezone # Additional librairies RUN pip install tqdm tensorboard Take a moment to analyze this dockerfile. As you can see, it is built upon an existing image from Pytorch. Starting from existing images allows for fast prototyping. You may find existing images on DockerHub . The Pytorch image we will be using is available here Fire up your GCloud instance and send your dockerfile using gcloud compute scp [your_file_path] [your_instance_name]:Workspace/ --zone [your_instance_zone] Connect to your instance: gcloud compute ssh --zone [your_instance_zone] [your_instance_name] If docker is not already installed in your machine, follow this guide to install it. You will also need the NVIDIA Container Toolkit to be installed to allow docker to communicate with the instance GPU. If you created your GCloud instance following the previous session's instructions, it should be OK. To verify that it is OK you may run the following command: sudo docker run --rm --gpus all nvidia/cuda:11.0-base nvidia-smi If the command output is in the form of : +-----------------------------------------------------------------------------+ | NVIDIA-SMI 450.51.06 Driver Version: 450.51.06 CUDA Version: 11.0 | |-------------------------------+----------------------+----------------------+ | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. | | | | MIG M. | |===============================+======================+======================| | 0 Tesla T4 On | 00000000:00:1E.0 Off | 0 | | N/A 34C P8 9W / 70W | 0MiB / 15109MiB | 0% Default | | | | N/A | +-------------------------------+----------------------+----------------------+ +-----------------------------------------------------------------------------+ | Processes: | | GPU GI CI PID Type Process name GPU Memory | | ID ID Usage | |=============================================================================| | No running processes found | +-----------------------------------------------------------------------------+ then everything is OK. Otherwise, you may need to install the NVIDIA Container Toolkit manually, following this guide . You may now build your first image using the following command: sudo docker build -t [your_image_name] -f [path_to_your_image] [build_context_folder] The image should take a few minutes to build. Once this is done, use the following command to list the available images on your GCloud instance: sudo docker image ls How many images can you see? What do they refer to? Now that our images are built, we can now use them to instantiate containers. Since a container is an instance of an image, we can instantiate several containers using a single image. We will run our first container using the interactive mode. Run the following command to run your fist container: docker run -it --name [your_container_name] [your_image_name] You should now have access to an interactive terminal from your container. On this terminal, open a Python console and check that Pytorch is installed and has access to your instance GPU. import torch print(torch.cuda.is_available()) Quit the Python console and quit your container using ctrl+d . You can list all your running containers using the following command: sudo docker container ls Your container is closed and does not appear. To list all the existing containers, add the -a to the previous command. sudo docker container ls -a Start your containers using: sudo docker start [container_id_or_name] Check that it is now listed as started. You can have access to its interactive mode using the attach command: sudo docker attach [container_id_or_name] You can delete a container using the rm command: sudo docker rm [container_id_or_name] We will now see how to share data between the container and the machine it is running on. First create a folder containing the files: * download_landscapes.sh * unet.py * colorize.py * data_utils.py Create a new container using this time mounting a shared volume using the following command: docker run -it --name [container_name] -v ~/[your_folder_path]:/workspace/[folder_name] [image_name] Go to the shared folder and run the download_landscapes.sh script. Leave the container and look at your folder in the local. What can you see? If you want to run your job using the interactive mode, you need to give access at your container to your host resources. Start a new container using the following command to get access to the GPU and CPU resources and run the colorize.py script. docker run -it --gpus all --ipc=host --name [container_name] -v ~/[your_folder_path]:/workspace/[folder_name] [image_name] Now try to send all the results and weights to your local machine and maybe to look at the tensorboard logs.","title":"Development for Data Scientist:"},{"location":"docker_2021.html#development-for-data-scientist","text":"","title":"Development for Data Scientist:"},{"location":"docker_2021.html#docker","text":"","title":"Docker"},{"location":"docker_2021.html#course","text":"Slides","title":"Course"},{"location":"docker_2021.html#practical-session","text":"In this practical session, you will now run your code through a Docker container. Using docker in data science projects has two advantages: * Improving the reproducibility of the results * Facilitating the portability and deployment In this session, we will try to package the code from the previous session, allowing us to train a neural network to colorize images into a Docker image and use this image to instantiate a container on a GCloud instance to run the code. We will first create the Dockerfile corresponding to our environment. On your local machine, create a new file named Dockerfile containing the following code: # Base image from pytorch FROM pytorch/pytorch:1.7.1-cuda11.0-cudnn8-runtime # Set up for your local zone an UTC information ENV TZ=Europe/Paris RUN ln -snf /usr/share/zoneinfo/$TZ /etc/localtime && echo $TZ > /etc/timezone # Additional librairies RUN pip install tqdm tensorboard Take a moment to analyze this dockerfile. As you can see, it is built upon an existing image from Pytorch. Starting from existing images allows for fast prototyping. You may find existing images on DockerHub . The Pytorch image we will be using is available here Fire up your GCloud instance and send your dockerfile using gcloud compute scp [your_file_path] [your_instance_name]:Workspace/ --zone [your_instance_zone] Connect to your instance: gcloud compute ssh --zone [your_instance_zone] [your_instance_name] If docker is not already installed in your machine, follow this guide to install it. You will also need the NVIDIA Container Toolkit to be installed to allow docker to communicate with the instance GPU. If you created your GCloud instance following the previous session's instructions, it should be OK. To verify that it is OK you may run the following command: sudo docker run --rm --gpus all nvidia/cuda:11.0-base nvidia-smi If the command output is in the form of : +-----------------------------------------------------------------------------+ | NVIDIA-SMI 450.51.06 Driver Version: 450.51.06 CUDA Version: 11.0 | |-------------------------------+----------------------+----------------------+ | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. | | | | MIG M. | |===============================+======================+======================| | 0 Tesla T4 On | 00000000:00:1E.0 Off | 0 | | N/A 34C P8 9W / 70W | 0MiB / 15109MiB | 0% Default | | | | N/A | +-------------------------------+----------------------+----------------------+ +-----------------------------------------------------------------------------+ | Processes: | | GPU GI CI PID Type Process name GPU Memory | | ID ID Usage | |=============================================================================| | No running processes found | +-----------------------------------------------------------------------------+ then everything is OK. Otherwise, you may need to install the NVIDIA Container Toolkit manually, following this guide . You may now build your first image using the following command: sudo docker build -t [your_image_name] -f [path_to_your_image] [build_context_folder] The image should take a few minutes to build. Once this is done, use the following command to list the available images on your GCloud instance: sudo docker image ls How many images can you see? What do they refer to? Now that our images are built, we can now use them to instantiate containers. Since a container is an instance of an image, we can instantiate several containers using a single image. We will run our first container using the interactive mode. Run the following command to run your fist container: docker run -it --name [your_container_name] [your_image_name] You should now have access to an interactive terminal from your container. On this terminal, open a Python console and check that Pytorch is installed and has access to your instance GPU. import torch print(torch.cuda.is_available()) Quit the Python console and quit your container using ctrl+d . You can list all your running containers using the following command: sudo docker container ls Your container is closed and does not appear. To list all the existing containers, add the -a to the previous command. sudo docker container ls -a Start your containers using: sudo docker start [container_id_or_name] Check that it is now listed as started. You can have access to its interactive mode using the attach command: sudo docker attach [container_id_or_name] You can delete a container using the rm command: sudo docker rm [container_id_or_name] We will now see how to share data between the container and the machine it is running on. First create a folder containing the files: * download_landscapes.sh * unet.py * colorize.py * data_utils.py Create a new container using this time mounting a shared volume using the following command: docker run -it --name [container_name] -v ~/[your_folder_path]:/workspace/[folder_name] [image_name] Go to the shared folder and run the download_landscapes.sh script. Leave the container and look at your folder in the local. What can you see? If you want to run your job using the interactive mode, you need to give access at your container to your host resources. Start a new container using the following command to get access to the GPU and CPU resources and run the colorize.py script. docker run -it --gpus all --ipc=host --name [container_name] -v ~/[your_folder_path]:/workspace/[folder_name] [image_name] Now try to send all the results and weights to your local machine and maybe to look at the tensorboard logs.","title":"Practical Session"},{"location":"evaluation.html","text":"Evaluation The evaluation is associated to the DEFI-IA Objective You will be evaluated on your capacity of acting like a Data Scientist , i.e. Handle a new dataset and explore it. Find a solution to address the defi's problem with a high score (above baseline). Explain the choosen algorithm. Write a complete pipeline to easily reproduce the results. Justify the choice of the algorithms and the environment (CPU/GPU, Cloud etc..). Share it and make your results easily reproducible (Git - docker, conda environment.). Notations Project - ( 60% ): a Git repository. The git should contain a clear markdown Readme, which describes ( 33% ) Which result you achieved? In which computation time? On which engine? What do I have to install to be able to reproduce the code? Which command do I have to run to reproduce the results? The code has to be easily reproducible. ( 33% ) Packages required has to be well described. (a requirements.txt files is the best) Conda command or docker command can be furnish The code should be clear and easily readable. ( 33% ) Final results can be run in a script and not a notebook. Only final code can be found in this script. Rapport - ( 40% ) 10 pages maximum: Quality of the presentation. 25% In-Deep explanation of the chosen algorithm. 25% Choice of the tools-infrastructure used. 25% Results you obtained. 25% Other details Group of 4 to 5 people (DEFI IA's team).","title":"Evaluation"},{"location":"evaluation.html#evaluation","text":"The evaluation is associated to the DEFI-IA","title":"Evaluation"},{"location":"evaluation.html#objective","text":"You will be evaluated on your capacity of acting like a Data Scientist , i.e. Handle a new dataset and explore it. Find a solution to address the defi's problem with a high score (above baseline). Explain the choosen algorithm. Write a complete pipeline to easily reproduce the results. Justify the choice of the algorithms and the environment (CPU/GPU, Cloud etc..). Share it and make your results easily reproducible (Git - docker, conda environment.).","title":"Objective"},{"location":"evaluation.html#notations","text":"Project - ( 60% ): a Git repository. The git should contain a clear markdown Readme, which describes ( 33% ) Which result you achieved? In which computation time? On which engine? What do I have to install to be able to reproduce the code? Which command do I have to run to reproduce the results? The code has to be easily reproducible. ( 33% ) Packages required has to be well described. (a requirements.txt files is the best) Conda command or docker command can be furnish The code should be clear and easily readable. ( 33% ) Final results can be run in a script and not a notebook. Only final code can be found in this script. Rapport - ( 40% ) 10 pages maximum: Quality of the presentation. 25% In-Deep explanation of the chosen algorithm. 25% Choice of the tools-infrastructure used. 25% Results you obtained. 25%","title":"Notations"},{"location":"evaluation.html#other-details","text":"Group of 4 to 5 people (DEFI IA's team).","title":"Other details"},{"location":"gcloud.html","text":"Development for Data Scientist: Introduction to Google Cloud Computing Practical Session In this session, you will train a neural network to colorize black and white images using virtual machines on Google Cloud . You will have to: Write your Python scripts on your local machine Push your code to your Github repository Clone your repository to your Colab instance Run your code on Colab Monitor your code running on the virtual machine Get your results and send them to your local machine Practical session repository: If you haven't already done so, fork this repository and clone it on your computer. Data We will be working with the Landscapes dataset composed of 4000 images in seven categories of landscapes (city, road, mountain, lake, ocean, field, and forest). Instead of using it to train a classifier, we will use it to train a neural network to colorize black and white images. Run the download_landscapes.sh script to download and extract the dataset. ./download_landscapes.sh The file data_utils.py contains some useful functions to load the dataset. Development in cloud computing environments Cloud providers charge by the hour, so cloud computing can quickly get expensive. A good practice consists of doing most of the code development on your local hardware before sending it to your cloud instances. That is what you are going to do in this practical session. You will run one small iteration of your code on your local machine to test your code and then send it to your virtual machine. The network architecture We will use a particular category of neural networks to perform the colorization operation: Unets . Initially designed for Biomedical Image Segmentation, Unets offer state-of-the-art performances in many segmentation tasks. These performances are mainly due to the skip connections used in UNets architectures. Indeed, Unets are a particular form of Auto-Encoders using skip connections between corresponding layers of the encoder and the decoder. The network architecture is defined in the unet.py file and need to be completed. Help yourself with the above image to implement a Unet network using the template located in the unet.py file: import torch import torch.nn as nn import torch.nn.functional as F def double_conv(in_channels, out_channels): # returns a block compsed of two Convolution layers with ReLU activation function return nn.Sequential( nn.Conv2d(in_channels, out_channels, 3, padding=1), nn.ReLU(), nn.Conv2d(out_channels, out_channels, 3, padding=1), nn.ReLU() ) class DownSampleBlock(nn.Module): def __init__(self, in_channels, out_channels): super().__init__() self.conv_block = ... self.maxpool = ... def forward(self, x): x_skip = ... out = ... return out , x_skip class UpSampleBlock(nn.Module): def __init__(self, in_channels, out_channels): super().__init__() self.conv_block = ... self.upsample = ... # use nn.Upsample def forward(self, x, x_skip): x = self.upsample(x) x = torch.cat([x, x_skip], dim=1) # concatenates x and x_skip x = self.conv_block(x) return x class UNet(nn.Module): def __init__(self): super().__init__() self.downsample_block_1 = ... self.downsample_block_2 = ... self.downsample_block_3 = ... self.middle_conv_block = double_conv(128, 256) self.upsample_block_3 = ... self.upsample_block_2 = ... self.upsample_block_1 = ... self.last_conv = nn.Conv2d(32, 3, 1) def forward(self, x): x, x_skip1 = ... x, x_skip2 = ... x, x_skip3 = ... x = self.middle_conv_block(x) x = #use upsampleblock_3 and x_skip3 x = #use upsampleblock_2 and x_skip2 x = #use upsampleblock_1 and x_skip1 out = self.last_conv(x) return out if __name__=='__main__': x = torch.rand(16,1,224,224) net = UNet() y = net(x) assert y.shape == (16,3,224,224) print('Shapes OK') Check that your network is producing correct outputs by running your file with: python unet.py Training script You will now implement the training procedure. Training a network to colorize images is a supervised regression problem. Consider \\(x\\) a grayscaled image and \\(y\\) its corresponding colored image. Training a parametrized network \\(f_\\theta\\) to predict colorized images \\(\u0177\\) amounts to minimizing the distance between the prediction \\(\u0177\\) and the actual \\(y\\) . That is to say minimizing \\(MSE(y, f_\\theta(x))\\) . Fill the colorize.py file to train a UNet to colorize images (you can inspire yourself from the one in the MNIST example. However, be careful in your criterion choice): import argparse # to parse script arguments from statistics import mean # to compute the mean of a list from tqdm import tqdm #used to generate progress bar during training import torch import torch.optim as optim from torch.utils.tensorboard import SummaryWriter from torchvision.utils import make_grid #to generate image grids, will be used in tensorboard from data_utils import get_colorized_dataset_loader # dataloarder from unet import UNet # setting device on GPU if available, else CPU device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') def train(net, optimizer, loader, epochs=5, writer=None): criterion = ... for epoch in range(epochs): running_loss = [] t = tqdm(loader) for x, y in t: # x: black and white image, y: colored image ... ... ... ... ... ... ... ... if writer is not None: #Logging loss in tensorboard writer.add_scalar('training loss', mean(running_loss), epoch) # Logging a sample of inputs in tensorboard input_grid = make_grid(x[:16].detach().cpu()) writer.add_image('Input', input_grid, epoch) # Logging a sample of predicted outputs in tensorboard colorized_grid = make_grid(outputs[:16].detach().cpu()) writer.add_image('Predicted', colorized_grid, epoch) # Logging a sample of ground truth in tensorboard original_grid = make_grid(y[:16].detach().cpu()) writer.add_image('Ground truth', original_grid, epoch) return mean(running_loss) if __name__=='__main__': parser = argparse.ArgumentParser() parser.add_argument('--exp_name', type=str, default = 'Colorize', help='experiment name') parser.add_argument('--data_path', ...) parser.add_argument('--batch_size'...) parser.add_argument('--epochs'...) parser.add_argument('--lr'...) exp_name = ... args = ... data_path = ... batch_size = ... epochs = ... lr = ... unet = UNet().to(device) loader = get_colorized_dataset_loader(path=data_path, batch_size=batch_size, shuffle=True, num_workers=0) optimizer = optim.Adam(unet.parameters(), lr=lr) writer = SummaryWriter(f'runs/{exp_name}') train(unet, optimizer, loader, epochs=epochs, writer=writer) x, y = next(iter(loader)) with torch.no_grad(): all_embeddings = [] all_labels = [] for x, y in loader: x , y = x.to(device), y.to(device) embeddings = unet.get_features(x).view(-1, 128*28*28) all_embeddings.append(embeddings) all_labels.append(y) if len(all_embeddings)>6: break embeddings = torch.cat(all_embeddings) labels = torch.cat(all_labels) writer.add_embedding(embeddings, label_img=labels, global_step=1) writer.add_graph(unet, x.to(device)) # Save model weights torch.save(unet.state_dict(), 'unet.pth') Try to run your code on your local machine for one or two minibatches to check that everything is working. Training on GCloud You now have everything to run your code on GCloud. Fire up your GCloud instance. On a terminal, connect to your instance using the following command (replace the zone and instance name with yours): gcloud compute ssh --zone \"europe-west1-d\" \"your_instance_name\" Create a folder Workspace on your virtual machine: mkdir Workspace You can copy a file from your local machine to the virtual machine using the following command on your local terminal: gcloud compute scp [your_file_path] your_instance_name:Workspace/ --zone \"europe-west1-d\" Conversly, you can copy a from your virtual machine to your local machine the following command on your local terminal: gcloud compute scp bsf.pth your_instance_name:Workspace/ --zone \"europe-west1-d\" Add the --recurse argument to your command if you want to copy a folder. Copy the folder containing all your code into your virtual machine's Workspace folder. Also, copy the download_landscapes.sh file into your VM and execute it. You should now be able to run your python script and thus learn to colorize images. Run your script for one entire epoch to check that everything is working fine. We will now run our script for a few more epochs, but before that, we will create an ssh tunnel between our local machine and the virtual machine. Run the following command on your local machine with the correct name for your virtual machine (use your correct zone). gcloud compute ssh --ssh-flag=\"-L 8898:localhost:8898\" --zone \"us-central1-b\" \"example_instance_name\" This command connects you to your virtual machine and forwards its port 8898 to your local machine's port 8898 (you can change the port value if needed). Thanks to that, you will get access to the tensorboard interface running on the virtual machine through your local machine. Now on this terminal window, run the following command: tensorboard --logdir runs --port 8898 On another terminal, connect to your virtual machine and run your script with a few more epochs (like 10 for instance). On your web browser, go to the following adress: http://localhost:8898/ You should be able to access tensorboard. Check on your network graph. You should see the U shape of your Unet. You can now visualize the progression of your network while it is training in the images tab. Once training is over, download your model's weights on your local machine. We will use them to create the web app. Web app with Gradio Complete the colorize_app.py file to create a web app that colorizes black and white images and run your application with the following command: python colorize_app.py --weights_path [path_to_the weights] You can test your app with random balck and white images from the net. For exemple one of these . Do you have any idea why the colors are so dull? Bonus synchronize with Rsync An easy way to synchronize your code with your VM is to use rsync Install rsync on your virtual machine : sudo apt-get install rsync Add the public key you\u2019re going to use to connect to the VM to the VM\u2019s ~/.ssh/authorized_keys On the virtual machine: touch ~/.ssh/authorized_keys nano ~/.ssh/authorized_keys Copy the content of your public key (It is usually located in ~/.ssh/id_rsa.pub on your local machine) in the opened file. Press ctrl+x then y then enter to save. Now to synchronize a folder, find the IP of your virtual machine in the GCloud interface: To synchronize your folder on your local machine (for instance, named test_gcloud ) with a distant folder on the virtual machine (located, for example, in Workspace/test_gcloud): rsync -r [VM_IP_ADRESS]:Workspace/test_gcloud/ test_gcloud/ To synchronize a distant folder on the virtual machine with a folder on your local machine : rsync -r test_gcloud/ [VM_IP_ADRESS]:Workspace/test_gcloud/ You can find more information on the rsync command here (in french) You can stop your VM using the GCloud interface or just by running the following command: sudo shutdown -h now","title":"Introduction to GCloud"},{"location":"gcloud.html#development-for-data-scientist","text":"","title":"Development for Data Scientist:"},{"location":"gcloud.html#introduction-to-google-cloud-computing","text":"","title":"Introduction to Google Cloud Computing"},{"location":"gcloud.html#practical-session","text":"In this session, you will train a neural network to colorize black and white images using virtual machines on Google Cloud . You will have to: Write your Python scripts on your local machine Push your code to your Github repository Clone your repository to your Colab instance Run your code on Colab Monitor your code running on the virtual machine Get your results and send them to your local machine","title":"Practical Session"},{"location":"gcloud.html#practical-session-repository","text":"If you haven't already done so, fork this repository and clone it on your computer.","title":"Practical session repository:"},{"location":"gcloud.html#data","text":"We will be working with the Landscapes dataset composed of 4000 images in seven categories of landscapes (city, road, mountain, lake, ocean, field, and forest). Instead of using it to train a classifier, we will use it to train a neural network to colorize black and white images. Run the download_landscapes.sh script to download and extract the dataset. ./download_landscapes.sh The file data_utils.py contains some useful functions to load the dataset.","title":"Data"},{"location":"gcloud.html#development-in-cloud-computing-environments","text":"Cloud providers charge by the hour, so cloud computing can quickly get expensive. A good practice consists of doing most of the code development on your local hardware before sending it to your cloud instances. That is what you are going to do in this practical session. You will run one small iteration of your code on your local machine to test your code and then send it to your virtual machine.","title":"Development in cloud computing environments"},{"location":"gcloud.html#the-network-architecture","text":"We will use a particular category of neural networks to perform the colorization operation: Unets . Initially designed for Biomedical Image Segmentation, Unets offer state-of-the-art performances in many segmentation tasks. These performances are mainly due to the skip connections used in UNets architectures. Indeed, Unets are a particular form of Auto-Encoders using skip connections between corresponding layers of the encoder and the decoder. The network architecture is defined in the unet.py file and need to be completed. Help yourself with the above image to implement a Unet network using the template located in the unet.py file: import torch import torch.nn as nn import torch.nn.functional as F def double_conv(in_channels, out_channels): # returns a block compsed of two Convolution layers with ReLU activation function return nn.Sequential( nn.Conv2d(in_channels, out_channels, 3, padding=1), nn.ReLU(), nn.Conv2d(out_channels, out_channels, 3, padding=1), nn.ReLU() ) class DownSampleBlock(nn.Module): def __init__(self, in_channels, out_channels): super().__init__() self.conv_block = ... self.maxpool = ... def forward(self, x): x_skip = ... out = ... return out , x_skip class UpSampleBlock(nn.Module): def __init__(self, in_channels, out_channels): super().__init__() self.conv_block = ... self.upsample = ... # use nn.Upsample def forward(self, x, x_skip): x = self.upsample(x) x = torch.cat([x, x_skip], dim=1) # concatenates x and x_skip x = self.conv_block(x) return x class UNet(nn.Module): def __init__(self): super().__init__() self.downsample_block_1 = ... self.downsample_block_2 = ... self.downsample_block_3 = ... self.middle_conv_block = double_conv(128, 256) self.upsample_block_3 = ... self.upsample_block_2 = ... self.upsample_block_1 = ... self.last_conv = nn.Conv2d(32, 3, 1) def forward(self, x): x, x_skip1 = ... x, x_skip2 = ... x, x_skip3 = ... x = self.middle_conv_block(x) x = #use upsampleblock_3 and x_skip3 x = #use upsampleblock_2 and x_skip2 x = #use upsampleblock_1 and x_skip1 out = self.last_conv(x) return out if __name__=='__main__': x = torch.rand(16,1,224,224) net = UNet() y = net(x) assert y.shape == (16,3,224,224) print('Shapes OK') Check that your network is producing correct outputs by running your file with: python unet.py","title":"The network architecture"},{"location":"gcloud.html#training-script","text":"You will now implement the training procedure. Training a network to colorize images is a supervised regression problem. Consider \\(x\\) a grayscaled image and \\(y\\) its corresponding colored image. Training a parametrized network \\(f_\\theta\\) to predict colorized images \\(\u0177\\) amounts to minimizing the distance between the prediction \\(\u0177\\) and the actual \\(y\\) . That is to say minimizing \\(MSE(y, f_\\theta(x))\\) . Fill the colorize.py file to train a UNet to colorize images (you can inspire yourself from the one in the MNIST example. However, be careful in your criterion choice): import argparse # to parse script arguments from statistics import mean # to compute the mean of a list from tqdm import tqdm #used to generate progress bar during training import torch import torch.optim as optim from torch.utils.tensorboard import SummaryWriter from torchvision.utils import make_grid #to generate image grids, will be used in tensorboard from data_utils import get_colorized_dataset_loader # dataloarder from unet import UNet # setting device on GPU if available, else CPU device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') def train(net, optimizer, loader, epochs=5, writer=None): criterion = ... for epoch in range(epochs): running_loss = [] t = tqdm(loader) for x, y in t: # x: black and white image, y: colored image ... ... ... ... ... ... ... ... if writer is not None: #Logging loss in tensorboard writer.add_scalar('training loss', mean(running_loss), epoch) # Logging a sample of inputs in tensorboard input_grid = make_grid(x[:16].detach().cpu()) writer.add_image('Input', input_grid, epoch) # Logging a sample of predicted outputs in tensorboard colorized_grid = make_grid(outputs[:16].detach().cpu()) writer.add_image('Predicted', colorized_grid, epoch) # Logging a sample of ground truth in tensorboard original_grid = make_grid(y[:16].detach().cpu()) writer.add_image('Ground truth', original_grid, epoch) return mean(running_loss) if __name__=='__main__': parser = argparse.ArgumentParser() parser.add_argument('--exp_name', type=str, default = 'Colorize', help='experiment name') parser.add_argument('--data_path', ...) parser.add_argument('--batch_size'...) parser.add_argument('--epochs'...) parser.add_argument('--lr'...) exp_name = ... args = ... data_path = ... batch_size = ... epochs = ... lr = ... unet = UNet().to(device) loader = get_colorized_dataset_loader(path=data_path, batch_size=batch_size, shuffle=True, num_workers=0) optimizer = optim.Adam(unet.parameters(), lr=lr) writer = SummaryWriter(f'runs/{exp_name}') train(unet, optimizer, loader, epochs=epochs, writer=writer) x, y = next(iter(loader)) with torch.no_grad(): all_embeddings = [] all_labels = [] for x, y in loader: x , y = x.to(device), y.to(device) embeddings = unet.get_features(x).view(-1, 128*28*28) all_embeddings.append(embeddings) all_labels.append(y) if len(all_embeddings)>6: break embeddings = torch.cat(all_embeddings) labels = torch.cat(all_labels) writer.add_embedding(embeddings, label_img=labels, global_step=1) writer.add_graph(unet, x.to(device)) # Save model weights torch.save(unet.state_dict(), 'unet.pth') Try to run your code on your local machine for one or two minibatches to check that everything is working.","title":"Training script"},{"location":"gcloud.html#training-on-gcloud","text":"You now have everything to run your code on GCloud. Fire up your GCloud instance. On a terminal, connect to your instance using the following command (replace the zone and instance name with yours): gcloud compute ssh --zone \"europe-west1-d\" \"your_instance_name\" Create a folder Workspace on your virtual machine: mkdir Workspace You can copy a file from your local machine to the virtual machine using the following command on your local terminal: gcloud compute scp [your_file_path] your_instance_name:Workspace/ --zone \"europe-west1-d\" Conversly, you can copy a from your virtual machine to your local machine the following command on your local terminal: gcloud compute scp bsf.pth your_instance_name:Workspace/ --zone \"europe-west1-d\" Add the --recurse argument to your command if you want to copy a folder. Copy the folder containing all your code into your virtual machine's Workspace folder. Also, copy the download_landscapes.sh file into your VM and execute it. You should now be able to run your python script and thus learn to colorize images. Run your script for one entire epoch to check that everything is working fine. We will now run our script for a few more epochs, but before that, we will create an ssh tunnel between our local machine and the virtual machine. Run the following command on your local machine with the correct name for your virtual machine (use your correct zone). gcloud compute ssh --ssh-flag=\"-L 8898:localhost:8898\" --zone \"us-central1-b\" \"example_instance_name\" This command connects you to your virtual machine and forwards its port 8898 to your local machine's port 8898 (you can change the port value if needed). Thanks to that, you will get access to the tensorboard interface running on the virtual machine through your local machine. Now on this terminal window, run the following command: tensorboard --logdir runs --port 8898 On another terminal, connect to your virtual machine and run your script with a few more epochs (like 10 for instance). On your web browser, go to the following adress: http://localhost:8898/ You should be able to access tensorboard. Check on your network graph. You should see the U shape of your Unet. You can now visualize the progression of your network while it is training in the images tab. Once training is over, download your model's weights on your local machine. We will use them to create the web app.","title":"Training on GCloud"},{"location":"gcloud.html#web-app-with-gradio","text":"Complete the colorize_app.py file to create a web app that colorizes black and white images and run your application with the following command: python colorize_app.py --weights_path [path_to_the weights] You can test your app with random balck and white images from the net. For exemple one of these . Do you have any idea why the colors are so dull?","title":"Web app with Gradio"},{"location":"gcloud.html#bonus-synchronize-with-rsync","text":"An easy way to synchronize your code with your VM is to use rsync Install rsync on your virtual machine : sudo apt-get install rsync Add the public key you\u2019re going to use to connect to the VM to the VM\u2019s ~/.ssh/authorized_keys On the virtual machine: touch ~/.ssh/authorized_keys nano ~/.ssh/authorized_keys Copy the content of your public key (It is usually located in ~/.ssh/id_rsa.pub on your local machine) in the opened file. Press ctrl+x then y then enter to save. Now to synchronize a folder, find the IP of your virtual machine in the GCloud interface: To synchronize your folder on your local machine (for instance, named test_gcloud ) with a distant folder on the virtual machine (located, for example, in Workspace/test_gcloud): rsync -r [VM_IP_ADRESS]:Workspace/test_gcloud/ test_gcloud/ To synchronize a distant folder on the virtual machine with a folder on your local machine : rsync -r test_gcloud/ [VM_IP_ADRESS]:Workspace/test_gcloud/ You can find more information on the rsync command here (in french) You can stop your VM using the GCloud interface or just by running the following command: sudo shutdown -h now","title":"Bonus synchronize with Rsync"},{"location":"gcloud2021.html","text":"Development for Data Scientist: Introduction to Google Cloud Computing Course Slides Practical Session In this session, you will train a neural network to colorize black and white images using virtual machines on Google Cloud . You will have to: Set up a new GCloud instance with GPU capacities Write your Python scripts on your local machine Send your code to your GCloud Instance Run your code on the cloud virtual machine Monitor your code running on the virtual machine Get your results and send them to your local machine The solution is available here. Try to complete the practical session without looking at it! Set up your virtual machine First follow the GCloud setup process described here . The python script Cloud providers charge by the hour, so cloud computing can quickly get expensive. A good practice consists of doing most of the code development on your local hardware before sending it to your cloud instances. That is what you are going to do in this practical session. You will run one small iteration of your code on your local machine to test your code and then send it to your virtual machine. We will be working with the Landscapes dataset composed of 4000 images in seven categories of landscapes (city, road, mountain, lake, ocean, field, and forest). Instead of using it to train a classifier, we will use it to train a neural network to colorize black and white images. Create a script download_landscapes.sh with the following content and execute it to download and extract the dataset. cd data wget https://github.com/ml5js/ml5-data-and-models/raw/master/datasets/images/landscapes/landscapes_small.zip mkdir landscapes unzip landscapes_small.zip -d landscapes rm landscapes_small.zip rm -r landscapes/__MACOSX cd .. We will use a particular category of neural networks to perform the colorization operation: Unets . Initially designed for Biomedical Image Segmentation, Unets offer state-of-the-art performances in many segmentation tasks. These performances are mainly due to the skip connections used in UNets architectures. Indeed, Unets are a particular form of Auto-Encoders using skip connections between corresponding layers of the encoder and the decoder. Create a new file named unet.py where you will define the following Unet network: Help yourself with the above image to implement a Unet network using the following template: import torch import torch.nn as nn import torch.nn.functional as F def double_conv(in_channels, out_channels): # returns a block compsed of two Convolution layers with ReLU activation function return nn.Sequential( nn.Conv2d(in_channels, out_channels, 3, padding=1), nn.ReLU(), nn.Conv2d(out_channels, out_channels, 3, padding=1), nn.ReLU() ) class DownSampleBlock(nn.Module): def __init__(self, in_channels, out_channels): super().__init__() self.conv_block = ... self.maxpool = ... def forward(self, x): x_skip = ... out = ... return out , x_skip class UpSampleBlock(nn.Module): def __init__(self, in_channels, out_channels): super().__init__() self.conv_block = ... self.upsample = ... # use nn.Upsample def forward(self, x, x_skip): x = self.upsample(x) x = torch.cat([x, x_skip], dim=1) # concatenates x and x_skip x = self.conv_block(x) return x class UNet(nn.Module): def __init__(self): super().__init__() self.downsample_block_1 = ... self.downsample_block_2 = ... self.downsample_block_3 = ... self.middle_conv_block = double_conv(128, 256) self.upsample_block_3 = ... self.upsample_block_2 = ... self.upsample_block_1 = ... self.last_conv = nn.Conv2d(32, 3, 1) def forward(self, x): x, x_skip1 = ... x, x_skip2 = ... x, x_skip3 = ... x = self.middle_conv_block(x) x = #use upsampleblock_3 and x_skip3 x = #use upsampleblock_2 and x_skip2 x = #use upsampleblock_1 and x_skip1 out = self.(x) return out if __name__=='__main__': x = torch.rand(16,1,224,224) net = UNet() y = net(x) assert y.shape == (16,3,224,224) print('Shapes OK') Check that your network is producing correct outputs by running your file with: python unet.py The Training script You will now implement the training procedure. Training a network to colorize images is a supervised regression problem. Consider \\(x\\) a grayscaled image and \\(y\\) its corresponding colored image. Training a parametrized network \\(f_\\theta\\) to predict colorized images \\(\u0177\\) amounts to minimizing the distance between the prediction \\(\u0177\\) and the actual \\(y\\) . That is to say minimizing \\(MSE(y, f_\\theta(x))\\) . Create a new file data_utils.py that will handle the dataset: from torchvision.datasets.folder import ImageFolder, default_loader, IMG_EXTENSIONS from torch.utils.data import DataLoader import torchvision.transforms as transforms class ImageFolderGrayColor(ImageFolder): def __init__( self, root, transform=None, target_transform=None, ): super(ImageFolder, self).__init__(root=root, loader=default_loader, transform=transform, extensions=IMG_EXTENSIONS, target_transform=target_transform) #TODO \u00e0 modifier def __getitem__(self, index): \"\"\" Args: index (int): Index Returns: tuple: (sample, target) where target is class_index of the target class. \"\"\" path, _ = self.samples[index] sample = self.loader(path) if self.target_transform is not None: target = self.target_transform(sample) if self.transform is not None: sample = self.transform(sample) return sample, target def get_colorized_dataset_loader(path, **kwargs): source_process = transforms.Compose( [transforms.Resize((224, 224)), transforms.Grayscale(num_output_channels=1), transforms.ToTensor(), transforms.Normalize(mean=[0.5], std=[0.5])]) target_process = transforms.Compose( [transforms.Resize((224, 224)), transforms.ToTensor()]) dataset = ImageFolderGrayColor(path, source_process, target_process) return DataLoader(dataset, **kwargs) Create a new file colorize.py and fill the train method in the following canvas (you can inspire yourself from the one in the MNIST example. Be careful, however, in your criterion choice): import argparse # to parse script arguments from statistics import mean # to compute the mean of a list from tqdm import tqdm #used to generate progress bar during training import torch import torch.optim as optim from torch.utils.tensorboard import SummaryWriter from torchvision.utils import make_grid #to generate image grids, will be used in tensorboard from data_utils import get_colorized_dataset_loader # dataloarder from unet import UNet # setting device on GPU if available, else CPU device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') def train(net, optimizer, loader, epochs=5, writer=None): criterion = ... for epoch in range(epochs): running_loss = [] t = tqdm(loader) for x, y in t: # x: black and white image, y: colored image ... ... ... ... ... ... ... ... if writer is not None: #Logging loss in tensorboard writer.add_scalar('training loss', mean(running_loss), epoch) # Logging a sample of inputs in tensorboard input_grid = make_grid(x[:16].detach().cpu()) writer.add_image('Input', input_grid, epoch) # Logging a sample of predicted outputs in tensorboard colorized_grid = make_grid(outputs[:16].detach().cpu()) writer.add_image('Predicted', colorized_grid, epoch) # Logging a sample of ground truth in tensorboard original_grid = make_grid(y[:16].detach().cpu()) writer.add_image('Ground truth', original_grid, epoch) return mean(running_loss) if __name__=='__main__': parser = argparse.ArgumentParser() parser.add_argument('--data_path', type=str, default = 'data/landscapes', help='dataset path') parser.add_argument('--batch_size', type=int, default = int(32), help='batch_size') parser.add_argument('--epochs', type=int, default = int(10), help='number of epochs') parser.add_argument('--lr', type=float, default = float(1e-3), help='learning rate') args = parser.parse_args() data_path = args.data_path batch_size = args.batch_size epochs = args.epochs lr = args.lr unet = UNet().cuda() loader = get_colorized_dataset_loader(path=data_path, batch_size=batch_size, shuffle=True, num_workers=4) optimizer = optim.Adam(unet.parameters(), lr=lr) writer = SummaryWriter('runs/UNet') train(unet, optimizer, loader, epochs=epochs, writer=writer) writer.add_graph(unet) # Save model weights torch.save(unet.state_dict(), 'unet.pth') Training on GCloud You now have everything to run your code on GCloud. Fire up your GCloud instance. On a terminal, connect to your instance using the following command (replace the zone and instance name with yours): gcloud compute ssh --zone \"europe-west1-d\" \"your_instance_name\" Create a folder Workspace on your virtual machine: mkdir Workspace You can copy a file from your local machine to the virtual machine using the following command on your local terminal: gcloud compute scp [your_file_path] your_instance_name:Workspace/ --zone \"europe-west1-d\" Conversly, you can copy a from your virtual machine to your local machine the following command on your local terminal: gcloud compute scp bsf.pth your_instance_name:Workspace/ --zone \"europe-west1-d\" Add the --recurse argument to your command if you want to copy a folder. Copy the folder containing all your code into your virtual machine's Workspace folder. Also, copy the download_landscapes.sh file into your VM and execute it. You should now be able to run your python script and thus learn to colorize images. Run your script for one entire epoch to check that everything is working fine. We will now run our script for a few more epochs, but before that, we will create an ssh tunnel between our local machine and the virtual machine. Run the following command on your local machine with the correct name for your virtual machine (use your correct zone). gcloud compute ssh --ssh-flag=\"-L 8898:localhost:8898\" --zone \"us-central1-b\" \"example_instance_name\" This command connects you to your virtual machine and forwards its port 8898 to your local machine's port 8898 (you can change the port value if needed). Thanks to that, you will get access to the tensorboard interface running on the virtual machine through your local machine. Now on this terminal window, run the following command: tensorboard --logdir runs --port 8898 On another terminal, connect to your virtual machine and run your script with a few more epochs (like 10 for instance). On your web browser, go to the following adress: http://localhost:8898/ You should be able to access tensorboard. Check on your network graph. You should see the U shape of your Unet. You can now visualize the progression of your network while it is training in the images tab. Bonus synchronize with Rsync An easy way to synchronize your code with your VM is to use rsync Install rsync on your virtual machine : sudo apt-get install rsync Add the public key you\u2019re going to use to connect to the VM to the VM\u2019s ~/.ssh/authorized_keys On the virtual machine: touch ~/.ssh/authorized_keys nano ~/.ssh/authorized_keys Copy the content of your public key (It is usually located in ~/.ssh/id_rsa.pub on your local machine) in the opened file. Press ctrl+x then y then enter to save. Now to synchronize a folder, find the IP of your virtual machine in the GCloud interface: To synchronize your folder on your local machine (for instance, named test_gcloud ) with a distant folder on the virtual machine (located, for example, in Workspace/test_gcloud): rsync -r [VM_IP_ADRESS]:Workspace/test_gcloud/ test_gcloud/ To synchronize a distant folder on the virtual machine with a folder on your local machine : rsync -r test_gcloud/ [VM_IP_ADRESS]:Workspace/test_gcloud/ You can find more information on the rsync command here (in french) You can stop your VM using the GCloud interface or just by running the following command: sudo shutdown -h now","title":"Development for Data Scientist:"},{"location":"gcloud2021.html#development-for-data-scientist","text":"","title":"Development for Data Scientist:"},{"location":"gcloud2021.html#introduction-to-google-cloud-computing","text":"","title":"Introduction to Google Cloud Computing"},{"location":"gcloud2021.html#course","text":"Slides","title":"Course"},{"location":"gcloud2021.html#practical-session","text":"In this session, you will train a neural network to colorize black and white images using virtual machines on Google Cloud . You will have to: Set up a new GCloud instance with GPU capacities Write your Python scripts on your local machine Send your code to your GCloud Instance Run your code on the cloud virtual machine Monitor your code running on the virtual machine Get your results and send them to your local machine The solution is available here. Try to complete the practical session without looking at it!","title":"Practical Session"},{"location":"gcloud2021.html#set-up-your-virtual-machine","text":"First follow the GCloud setup process described here .","title":"Set up your virtual machine"},{"location":"gcloud2021.html#the-python-script","text":"Cloud providers charge by the hour, so cloud computing can quickly get expensive. A good practice consists of doing most of the code development on your local hardware before sending it to your cloud instances. That is what you are going to do in this practical session. You will run one small iteration of your code on your local machine to test your code and then send it to your virtual machine. We will be working with the Landscapes dataset composed of 4000 images in seven categories of landscapes (city, road, mountain, lake, ocean, field, and forest). Instead of using it to train a classifier, we will use it to train a neural network to colorize black and white images. Create a script download_landscapes.sh with the following content and execute it to download and extract the dataset. cd data wget https://github.com/ml5js/ml5-data-and-models/raw/master/datasets/images/landscapes/landscapes_small.zip mkdir landscapes unzip landscapes_small.zip -d landscapes rm landscapes_small.zip rm -r landscapes/__MACOSX cd .. We will use a particular category of neural networks to perform the colorization operation: Unets . Initially designed for Biomedical Image Segmentation, Unets offer state-of-the-art performances in many segmentation tasks. These performances are mainly due to the skip connections used in UNets architectures. Indeed, Unets are a particular form of Auto-Encoders using skip connections between corresponding layers of the encoder and the decoder. Create a new file named unet.py where you will define the following Unet network: Help yourself with the above image to implement a Unet network using the following template: import torch import torch.nn as nn import torch.nn.functional as F def double_conv(in_channels, out_channels): # returns a block compsed of two Convolution layers with ReLU activation function return nn.Sequential( nn.Conv2d(in_channels, out_channels, 3, padding=1), nn.ReLU(), nn.Conv2d(out_channels, out_channels, 3, padding=1), nn.ReLU() ) class DownSampleBlock(nn.Module): def __init__(self, in_channels, out_channels): super().__init__() self.conv_block = ... self.maxpool = ... def forward(self, x): x_skip = ... out = ... return out , x_skip class UpSampleBlock(nn.Module): def __init__(self, in_channels, out_channels): super().__init__() self.conv_block = ... self.upsample = ... # use nn.Upsample def forward(self, x, x_skip): x = self.upsample(x) x = torch.cat([x, x_skip], dim=1) # concatenates x and x_skip x = self.conv_block(x) return x class UNet(nn.Module): def __init__(self): super().__init__() self.downsample_block_1 = ... self.downsample_block_2 = ... self.downsample_block_3 = ... self.middle_conv_block = double_conv(128, 256) self.upsample_block_3 = ... self.upsample_block_2 = ... self.upsample_block_1 = ... self.last_conv = nn.Conv2d(32, 3, 1) def forward(self, x): x, x_skip1 = ... x, x_skip2 = ... x, x_skip3 = ... x = self.middle_conv_block(x) x = #use upsampleblock_3 and x_skip3 x = #use upsampleblock_2 and x_skip2 x = #use upsampleblock_1 and x_skip1 out = self.(x) return out if __name__=='__main__': x = torch.rand(16,1,224,224) net = UNet() y = net(x) assert y.shape == (16,3,224,224) print('Shapes OK') Check that your network is producing correct outputs by running your file with: python unet.py The","title":"The python script"},{"location":"gcloud2021.html#training-script","text":"You will now implement the training procedure. Training a network to colorize images is a supervised regression problem. Consider \\(x\\) a grayscaled image and \\(y\\) its corresponding colored image. Training a parametrized network \\(f_\\theta\\) to predict colorized images \\(\u0177\\) amounts to minimizing the distance between the prediction \\(\u0177\\) and the actual \\(y\\) . That is to say minimizing \\(MSE(y, f_\\theta(x))\\) . Create a new file data_utils.py that will handle the dataset: from torchvision.datasets.folder import ImageFolder, default_loader, IMG_EXTENSIONS from torch.utils.data import DataLoader import torchvision.transforms as transforms class ImageFolderGrayColor(ImageFolder): def __init__( self, root, transform=None, target_transform=None, ): super(ImageFolder, self).__init__(root=root, loader=default_loader, transform=transform, extensions=IMG_EXTENSIONS, target_transform=target_transform) #TODO \u00e0 modifier def __getitem__(self, index): \"\"\" Args: index (int): Index Returns: tuple: (sample, target) where target is class_index of the target class. \"\"\" path, _ = self.samples[index] sample = self.loader(path) if self.target_transform is not None: target = self.target_transform(sample) if self.transform is not None: sample = self.transform(sample) return sample, target def get_colorized_dataset_loader(path, **kwargs): source_process = transforms.Compose( [transforms.Resize((224, 224)), transforms.Grayscale(num_output_channels=1), transforms.ToTensor(), transforms.Normalize(mean=[0.5], std=[0.5])]) target_process = transforms.Compose( [transforms.Resize((224, 224)), transforms.ToTensor()]) dataset = ImageFolderGrayColor(path, source_process, target_process) return DataLoader(dataset, **kwargs) Create a new file colorize.py and fill the train method in the following canvas (you can inspire yourself from the one in the MNIST example. Be careful, however, in your criterion choice): import argparse # to parse script arguments from statistics import mean # to compute the mean of a list from tqdm import tqdm #used to generate progress bar during training import torch import torch.optim as optim from torch.utils.tensorboard import SummaryWriter from torchvision.utils import make_grid #to generate image grids, will be used in tensorboard from data_utils import get_colorized_dataset_loader # dataloarder from unet import UNet # setting device on GPU if available, else CPU device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') def train(net, optimizer, loader, epochs=5, writer=None): criterion = ... for epoch in range(epochs): running_loss = [] t = tqdm(loader) for x, y in t: # x: black and white image, y: colored image ... ... ... ... ... ... ... ... if writer is not None: #Logging loss in tensorboard writer.add_scalar('training loss', mean(running_loss), epoch) # Logging a sample of inputs in tensorboard input_grid = make_grid(x[:16].detach().cpu()) writer.add_image('Input', input_grid, epoch) # Logging a sample of predicted outputs in tensorboard colorized_grid = make_grid(outputs[:16].detach().cpu()) writer.add_image('Predicted', colorized_grid, epoch) # Logging a sample of ground truth in tensorboard original_grid = make_grid(y[:16].detach().cpu()) writer.add_image('Ground truth', original_grid, epoch) return mean(running_loss) if __name__=='__main__': parser = argparse.ArgumentParser() parser.add_argument('--data_path', type=str, default = 'data/landscapes', help='dataset path') parser.add_argument('--batch_size', type=int, default = int(32), help='batch_size') parser.add_argument('--epochs', type=int, default = int(10), help='number of epochs') parser.add_argument('--lr', type=float, default = float(1e-3), help='learning rate') args = parser.parse_args() data_path = args.data_path batch_size = args.batch_size epochs = args.epochs lr = args.lr unet = UNet().cuda() loader = get_colorized_dataset_loader(path=data_path, batch_size=batch_size, shuffle=True, num_workers=4) optimizer = optim.Adam(unet.parameters(), lr=lr) writer = SummaryWriter('runs/UNet') train(unet, optimizer, loader, epochs=epochs, writer=writer) writer.add_graph(unet) # Save model weights torch.save(unet.state_dict(), 'unet.pth')","title":"Training script"},{"location":"gcloud2021.html#training-on-gcloud","text":"You now have everything to run your code on GCloud. Fire up your GCloud instance. On a terminal, connect to your instance using the following command (replace the zone and instance name with yours): gcloud compute ssh --zone \"europe-west1-d\" \"your_instance_name\" Create a folder Workspace on your virtual machine: mkdir Workspace You can copy a file from your local machine to the virtual machine using the following command on your local terminal: gcloud compute scp [your_file_path] your_instance_name:Workspace/ --zone \"europe-west1-d\" Conversly, you can copy a from your virtual machine to your local machine the following command on your local terminal: gcloud compute scp bsf.pth your_instance_name:Workspace/ --zone \"europe-west1-d\" Add the --recurse argument to your command if you want to copy a folder. Copy the folder containing all your code into your virtual machine's Workspace folder. Also, copy the download_landscapes.sh file into your VM and execute it. You should now be able to run your python script and thus learn to colorize images. Run your script for one entire epoch to check that everything is working fine. We will now run our script for a few more epochs, but before that, we will create an ssh tunnel between our local machine and the virtual machine. Run the following command on your local machine with the correct name for your virtual machine (use your correct zone). gcloud compute ssh --ssh-flag=\"-L 8898:localhost:8898\" --zone \"us-central1-b\" \"example_instance_name\" This command connects you to your virtual machine and forwards its port 8898 to your local machine's port 8898 (you can change the port value if needed). Thanks to that, you will get access to the tensorboard interface running on the virtual machine through your local machine. Now on this terminal window, run the following command: tensorboard --logdir runs --port 8898 On another terminal, connect to your virtual machine and run your script with a few more epochs (like 10 for instance). On your web browser, go to the following adress: http://localhost:8898/ You should be able to access tensorboard. Check on your network graph. You should see the U shape of your Unet. You can now visualize the progression of your network while it is training in the images tab.","title":"Training on GCloud"},{"location":"gcloud2021.html#bonus-synchronize-with-rsync","text":"An easy way to synchronize your code with your VM is to use rsync Install rsync on your virtual machine : sudo apt-get install rsync Add the public key you\u2019re going to use to connect to the VM to the VM\u2019s ~/.ssh/authorized_keys On the virtual machine: touch ~/.ssh/authorized_keys nano ~/.ssh/authorized_keys Copy the content of your public key (It is usually located in ~/.ssh/id_rsa.pub on your local machine) in the opened file. Press ctrl+x then y then enter to save. Now to synchronize a folder, find the IP of your virtual machine in the GCloud interface: To synchronize your folder on your local machine (for instance, named test_gcloud ) with a distant folder on the virtual machine (located, for example, in Workspace/test_gcloud): rsync -r [VM_IP_ADRESS]:Workspace/test_gcloud/ test_gcloud/ To synchronize a distant folder on the virtual machine with a folder on your local machine : rsync -r test_gcloud/ [VM_IP_ADRESS]:Workspace/test_gcloud/ You can find more information on the rsync command here (in french) You can stop your VM using the GCloud interface or just by running the following command: sudo shutdown -h now","title":"Bonus synchronize with Rsync"},{"location":"gcloud_set_up.html","text":"Set up GCloud: Google sponsors this course with free GCloud credits through the Cloud Higher Education Programs. Go to this link to claim your coupon code for the credits associated with this course. Once you have your coupon code go to this link to get your credits (you will need a Google account, if needed, you can create one using your INSA mail address). Once you are on the GCloud homepage, start by creating a new project: Once your project is created go to Compute Engine -> VM instances and activate Compute Engine API for your project . You now need to add GPU capacity to your project. Go to IAM and admin -> Quotas . On the filter type the following Quota:GPUs (all regions) select the resulting service and click on modify quotas . Increase the limit to one and add a short description to your request: Hi, I am a student enrolled at INSA Toulouse. The GCloud Higher education program granted us free credits. These credits are associated with a course in which the practical sessions will require access to GPU machines. Would you please increase my GPU quota so I can participate to the practical sessions? Many thanks, You name This process may take some time. Therefore, be sure sure to complete every steps at least a few days before the practical session. You will also need to install the Cloud SDK Command-line interface. It should already be installed on the INSA's machines. If you are using your personal computer, follow the corresponding installation procedure available here . Once the GCloud SDK is configured on your local machine, go to your GCloud interface and go to Compute engine ->VM instances Click on the create new instance button to create your first instance. Be sure at this point that your quota request has been approved or you will not be able to attach a GPU to your Virtual Machine. Now create a new instance on the same region you asked for your GPU quota. You may follow the following parameters settings for the practical session. (click on plate-forme du CPU et GPU and add a K80 GPU) Select the following hard drive options Finally, check the two checkboxes at the bottom of the page to allow Http/Https traffic. After a few minutes your instance should be created. It should appear in the VM instance panel: If its status is in green, the virtual machine is started and is now consuming your free credit. Go to ssh -> show ssh the gcloud command and copy the command on your terminal (you may need to run the gcloud init command before if it does not work at first). You should now be connected to your virtual machine and see the following output on your terminal: Type 'y' to install the nvidia drivers. If the drivers failed to be installed and you obtain the following message: Wait a few minutes and type: cd /opt/deeplearning/ sudo ./install-driver.sh To verify that everything is correctly installed type the following command nvidia-smi If you see something like +-----------------------------------------------------------------------------+ | NVIDIA-SMI 460.73.01 Driver Version: 460.73.01 CUDA Version: 11.2 | |-------------------------------+----------------------+----------------------+ | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. | | | | MIG M. | |===============================+======================+======================| | 0 Tesla K80 Off | 00000000:00:04.0 Off | 0 | | N/A 36C P0 69W / 149W | 0MiB / 11441MiB | 100% Default | | | | N/A | +-------------------------------+----------------------+----------------------+ +-----------------------------------------------------------------------------+ | Processes: | | GPU GI CI PID Type Process name GPU Memory | | ID ID Usage | |=============================================================================| | No running processes found | +-----------------------------------------------------------------------------+ then your installation is almost complete. Run the following commands to install the libraries needed for the practical session: pip3 install torch torchvision --extra-index-url https://download.pytorch.org/whl/cu113 pip install tqdm gradio tensorboard Everything should be OK now. You can stop your virtual machine until the practical session using the GCloud web interface or typing the following command: sudo shutdown -h now Be sur to do all this process before the practical session, it will save you many time and remember at least to ask for your quotas as soon as possible!","title":"Set up GCoud"},{"location":"gcloud_set_up.html#set-up-gcloud","text":"Google sponsors this course with free GCloud credits through the Cloud Higher Education Programs. Go to this link to claim your coupon code for the credits associated with this course. Once you have your coupon code go to this link to get your credits (you will need a Google account, if needed, you can create one using your INSA mail address). Once you are on the GCloud homepage, start by creating a new project: Once your project is created go to Compute Engine -> VM instances and activate Compute Engine API for your project . You now need to add GPU capacity to your project. Go to IAM and admin -> Quotas . On the filter type the following Quota:GPUs (all regions) select the resulting service and click on modify quotas . Increase the limit to one and add a short description to your request: Hi, I am a student enrolled at INSA Toulouse. The GCloud Higher education program granted us free credits. These credits are associated with a course in which the practical sessions will require access to GPU machines. Would you please increase my GPU quota so I can participate to the practical sessions? Many thanks, You name This process may take some time. Therefore, be sure sure to complete every steps at least a few days before the practical session. You will also need to install the Cloud SDK Command-line interface. It should already be installed on the INSA's machines. If you are using your personal computer, follow the corresponding installation procedure available here . Once the GCloud SDK is configured on your local machine, go to your GCloud interface and go to Compute engine ->VM instances Click on the create new instance button to create your first instance. Be sure at this point that your quota request has been approved or you will not be able to attach a GPU to your Virtual Machine. Now create a new instance on the same region you asked for your GPU quota. You may follow the following parameters settings for the practical session. (click on plate-forme du CPU et GPU and add a K80 GPU) Select the following hard drive options Finally, check the two checkboxes at the bottom of the page to allow Http/Https traffic. After a few minutes your instance should be created. It should appear in the VM instance panel: If its status is in green, the virtual machine is started and is now consuming your free credit. Go to ssh -> show ssh the gcloud command and copy the command on your terminal (you may need to run the gcloud init command before if it does not work at first). You should now be connected to your virtual machine and see the following output on your terminal: Type 'y' to install the nvidia drivers. If the drivers failed to be installed and you obtain the following message: Wait a few minutes and type: cd /opt/deeplearning/ sudo ./install-driver.sh To verify that everything is correctly installed type the following command nvidia-smi If you see something like +-----------------------------------------------------------------------------+ | NVIDIA-SMI 460.73.01 Driver Version: 460.73.01 CUDA Version: 11.2 | |-------------------------------+----------------------+----------------------+ | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. | | | | MIG M. | |===============================+======================+======================| | 0 Tesla K80 Off | 00000000:00:04.0 Off | 0 | | N/A 36C P0 69W / 149W | 0MiB / 11441MiB | 100% Default | | | | N/A | +-------------------------------+----------------------+----------------------+ +-----------------------------------------------------------------------------+ | Processes: | | GPU GI CI PID Type Process name GPU Memory | | ID ID Usage | |=============================================================================| | No running processes found | +-----------------------------------------------------------------------------+ then your installation is almost complete. Run the following commands to install the libraries needed for the practical session: pip3 install torch torchvision --extra-index-url https://download.pytorch.org/whl/cu113 pip install tqdm gradio tensorboard Everything should be OK now. You can stop your virtual machine until the practical session using the GCloud web interface or typing the following command: sudo shutdown -h now Be sur to do all this process before the practical session, it will save you many time and remember at least to ask for your quotas as soon as possible!","title":"Set up GCloud:"},{"location":"git_intro.html","text":"Introduction to git Slides","title":"Introduction to Git"},{"location":"git_intro.html#introduction-to-git","text":"Slides","title":"Introduction to git"},{"location":"interpretability.html","text":"Interpretability in Machine Learning Practical session Practical session Solution","title":"Interpretability in Machine Learning"},{"location":"interpretability.html#interpretability-in-machine-learning","text":"","title":"Interpretability in Machine Learning"},{"location":"interpretability.html#practical-session","text":"Practical session Solution","title":"Practical session"},{"location":"introduction.html","text":"Course Introduction:","title":"Course Introduction"},{"location":"introduction.html#course-introduction","text":"","title":"Course Introduction:"},{"location":"policy_gradient.html","text":"Introduction to Reinforcement Learning: Policy Gradient Slides Practical session","title":"Introduction to Reinforcement Learning:"},{"location":"policy_gradient.html#introduction-to-reinforcement-learning","text":"","title":"Introduction to Reinforcement Learning:"},{"location":"policy_gradient.html#policy-gradient","text":"Slides Practical session","title":"Policy Gradient"},{"location":"q_learning.html","text":"Introduction to Reinforcement Learning: From policy iteration to Deep Q-Learning Slides Practical sessions: Policy iteration and Value Iteration Q-learning Deep Q-learning","title":"Introduction to Reinforcement Learning:"},{"location":"q_learning.html#introduction-to-reinforcement-learning","text":"","title":"Introduction to Reinforcement Learning:"},{"location":"q_learning.html#from-policy-iteration-to-deep-q-learning","text":"Slides Practical sessions: Policy iteration and Value Iteration Q-learning Deep Q-learning","title":"From policy iteration to Deep Q-Learning"},{"location":"rec_sys.html","text":"Recommendation Systems Slides <!-- * Practical session 1 Practical session 2 --> Practical session Solution","title":"Recommendation Systems"},{"location":"rec_sys.html#recommendation-systems","text":"Slides <!-- * Practical session 1 Practical session 2 --> Practical session Solution","title":"Recommendation Systems"},{"location":"rl.html","text":"Introduction to Reinforcement Learning: Slides Intro Practical sessions: , , ,","title":"Introduction to Reinforcement Learning"},{"location":"rl.html#introduction-to-reinforcement-learning","text":"Slides Intro Practical sessions: , , ,","title":"Introduction to Reinforcement Learning:"},{"location":"schedule.html","text":"Schedule Lectures : 10 hours Practical Sessions : 30 hours. Fives days are dedicated to the practical sessions. All the lectures associated to each sessions are available in video. You must have seen the corresponding videos before each session. At the start of each practical session, approximately 15 minutes will be devoted to questions about the lectures. Session 0 Course Introduction Session 1 - 26/09/2022 (9h30-12h15 & 14h00-16h45) Development for Data Scientist : Introduction to Pytorch and Python scripts Development for Data Scientist : Github Reminder Development for Data Scientist : Introduction to GCloud Development for Data Scientist : Introduction to Docker Session 2 - 10/10/2022 (9h30-12h15 & 14h00-16h45) Introduction to Reinforcement learning: Introduction to Reinforcement Learning Session 3 - 24/10/2022 (9h30-12h15 & 14h00-16h45) Introduction to Natural language processing : (comming soon) Session 4 - 14/11/2021 (9h30-12h15 & 14h00-16h45) Introduction to recommender systems: (comming soon) Session 5 - 12/12/2021 (9h30-12h15 & 14h00-16h45) Explainable AI : Interpretability in Machine Learning","title":"Schedule"},{"location":"schedule.html#schedule","text":"Lectures : 10 hours Practical Sessions : 30 hours. Fives days are dedicated to the practical sessions. All the lectures associated to each sessions are available in video. You must have seen the corresponding videos before each session. At the start of each practical session, approximately 15 minutes will be devoted to questions about the lectures.","title":"Schedule"},{"location":"schedule.html#session-0","text":"Course Introduction","title":"Session 0"},{"location":"schedule.html#session-1-26092022","text":"","title":"Session 1 - 26/09/2022"},{"location":"schedule.html#9h30-12h15-14h00-16h45","text":"Development for Data Scientist : Introduction to Pytorch and Python scripts Development for Data Scientist : Github Reminder Development for Data Scientist : Introduction to GCloud Development for Data Scientist : Introduction to Docker","title":"(9h30-12h15 &amp; 14h00-16h45)"},{"location":"schedule.html#session-2-10102022","text":"","title":"Session 2 - 10/10/2022"},{"location":"schedule.html#9h30-12h15-14h00-16h45_1","text":"Introduction to Reinforcement learning: Introduction to Reinforcement Learning","title":"(9h30-12h15 &amp; 14h00-16h45)"},{"location":"schedule.html#session-3-24102022","text":"","title":"Session 3 - 24/10/2022"},{"location":"schedule.html#9h30-12h15-14h00-16h45_2","text":"Introduction to Natural language processing : (comming soon)","title":"(9h30-12h15 &amp; 14h00-16h45)"},{"location":"schedule.html#session-4-14112021","text":"","title":"Session 4 - 14/11/2021"},{"location":"schedule.html#9h30-12h15-14h00-16h45_3","text":"Introduction to recommender systems: (comming soon)","title":"(9h30-12h15 &amp; 14h00-16h45)"},{"location":"schedule.html#session-5-12122021","text":"","title":"Session 5 - 12/12/2021"},{"location":"schedule.html#9h30-12h15-14h00-16h45_4","text":"Explainable AI : Interpretability in Machine Learning","title":"(9h30-12h15 &amp; 14h00-16h45)"},{"location":"text1.html","text":"Text Cleaning and Text Vectorization Slides Practical session","title":"Text Cleaning and Text Vectorization"},{"location":"text1.html#text-cleaning-and-text-vectorization","text":"Slides Practical session","title":"Text Cleaning and Text Vectorization"},{"location":"text2.html","text":"Words Embedding Slides Practical session","title":"Words Embedding"},{"location":"text2.html#words-embedding","text":"Slides Practical session","title":"Words Embedding"},{"location":"text3.html","text":"Text Recurrent Network Slides Practical session","title":"Text Recurrent Network"},{"location":"text3.html#text-recurrent-network","text":"Slides Practical session","title":"Text Recurrent Network"}]}