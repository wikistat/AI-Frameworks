{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<a href=\"http://www.insa-toulouse.fr/\" ><img src=\"http://www.math.univ-toulouse.fr/~besse/Wikistat/Images/logo-insa.jpg\" style=\"float:left; max-width: 120px; display: inline\" alt=\"INSA\"/></a> \n",
    "\n",
    "<a href=\"http://wikistat.fr/\" ><img src=\"http://www.math.univ-toulouse.fr/~besse/Wikistat/Images/wikistat.jpg\" style=\"max-width: 250px; display: inline\"  alt=\"Wikistat\"/></a>\n",
    "\n",
    "<a href=\"http://www.math.univ-toulouse.fr/\" ><img src=\"http://www.math.univ-toulouse.fr/~besse/Wikistat/Images/logo_imt.jpg\" style=\"float:right; max-width: 200px; display: inline\" alt=\"IMT\"/> </a>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Ateliers: Technologies des grosses data](https://github.com/wikistat/Ateliers-Big-Data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Reconnaissance d'Activité Humaine](https://github.com/wikistat/Ateliers-Big-Data/5-HumanActivityRecognition) ([*HAR*](https://archive.ics.uci.edu/ml/datasets/Human+Activity+Recognition+Using+Smartphones)) en <a href=\"https://www.python.org/\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/f/f8/Python_logo_and_wordmark.svg/390px-Python_logo_and_wordmark.svg.png\" style=\"max-width: 120px; display: inline\" alt=\"Python\"/></a> \n",
    "##  Seconde partie:  étude des signaux bruts - Exploration\n",
    "\n",
    "Ce notebook présente la partie exploratoire des données. Pour la prédiction, se référé au calepin suivant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  1 Introduction\n",
    "### 1.1  Contexte\n",
    "Les données sont issues de la communauté qui vise la reconnaissance d'activités humaines (*Human activity recognition, HAR*) à partir d’enregistrements, par exemple du gyroscope et de l'accéléromètre d'un smartphone, objet connecté précurseur et dont la fonctionnalité de téléphonie devient très secondaire.\n",
    "Voir à ce propos l'[article](https://www.elen.ucl.ac.be/Proceedings/esann/esannpdf/es2013-11.pdf) relatant un colloque de 2013.  \n",
    "\n",
    "Les données publiques disponibles et largement étudiées ont été acquises, décrites et analysées par [Anguita et al. (2013)]().\n",
    "Elles sont accessibles sur le [dépôt](https://archive.ics.uci.edu/ml/datasets/Human+Activity+Recognition+Using+Smartphones) de l'University California Irvine (UCI) consacré à l'apprentissage machine ainsi que sur le site *Kaggle*.\n",
    "\n",
    "L'archive contient les données brutes: accélérations en x, y, et z, chacun de 128 colonnes. D'autres fichiers en y soustrayant la gravité naturelle ainsi que les accélérations angulaires en x, y, et z soit en tout 9 fichiers. Mais 6 utiles avec 6*128=768 mesures.\n",
    "\n",
    "C'est sur ces données brutes que nous allons appliquer des méthodes d'apprentissage\n",
    "\n",
    "### 1.2 Objectifs\n",
    "Cette deuxième étape s'intéresse aux données brutes. Est-il possible d'économiser le travail préliminaire de définition des variables métier en utilisant par exemple les ressources de décompositions systématiques sur une base d'ondelette ou un algorihtme d'apprentissage profond? L'enjeu est important, le calcul en temps réel des variables métier consomme beaucoup d'énergie et l'objectif serait de pouvoir embarquer (faire porter) un système économe défini par exemple par un réseau de neurones après son apprentissage.\n",
    "\n",
    "Comme pour les données métier, l'étude commence par une exploration: affichage et visualisation des données, réduction de dimension.\n",
    "\n",
    "### 1.3 Librairies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import copy\n",
    "import random\n",
    "import itertools\n",
    "\n",
    "#Utils Sklearn\n",
    "import sklearn.linear_model as lm\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb\n",
    "sb.set()\n",
    "\n",
    "\n",
    "import sklearn.decomposition as sdec \n",
    "import sklearn.preprocessing as sprep\n",
    "import sklearn.discriminant_analysis as sda\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Prise en charge des données\n",
    "### 2.1 Source\n",
    "\n",
    "Les données sont celles originales du dépôt de l'[UCI](https://archive.ics.uci.edu/ml/datasets/Human+Activity+Recognition+Using+Smartphones). Elle peuvent être téléchargées en cliquant [ici](https://archive.ics.uci.edu/ml/machine-learning-databases/00240/UCI%20HAR%20Dataset.zip).\n",
    "\n",
    "Elles contiennent deux jeux de dimensions différentes, chacun partagé en apprentissage et test.\n",
    "\n",
    "1. Multidimensionel: un individus est constitué de 9 Séries Temporelles de *dimensions* $(N, 128, 9)$\n",
    "2. Unidimensionnel: Les 9 Séries Temporelles sont concaténées pour constituer un vecteur de 128*9 = 1152 variables de *dimensions* $(N, 1152)*\n",
    "        \n",
    "Deux objets différents sont construits pour définir la variable $Y$ réponse car les librairies `Scikit-learn` et `Keras` prennent en compte des structures différentes: \n",
    "    \n",
    "1. `Scikit-Learn`  Un vecteur de dimension $(N, 1)$ avec, pour chaque individu le numéro du label de l'activité de 0 à 5.\n",
    "2. `Keras` Une matrice de dimension $(N, 6)$ des indicatrices (0 ou 1) des modalités de $Y$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Structurer les données\n",
    "Définir le chemin d'accès aux données puis les fonctions utiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-3-12cbe044a64c>, line 8)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-3-12cbe044a64c>\"\u001b[0;36m, line \u001b[0;32m8\u001b[0m\n\u001b[0;31m    filename = f'{data_dir}/{subset}/Inertial Signals/{signal}_{subset}.txt'\u001b[0m\n\u001b[0m                                                                           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "DATADIR_UCI = '/home-local/pbesse/Documents/2-Data_files/HAR/UCI_HAR_Dataset'\n",
    "SIGNALS = [ \"body_acc_x\", \"body_acc_y\", \"body_acc_z\", \"body_gyro_x\", \"body_gyro_y\", \"body_gyro_z\", \"total_acc_x\", \"total_acc_y\", \"total_acc_z\"]\n",
    "\n",
    "def my_read_csv(filename):\n",
    "    return pd.read_csv(filename, delim_whitespace=True, header=None)\n",
    "\n",
    "def load_signal(data_dir, subset, signal):\n",
    "    filename = f'{data_dir}/{subset}/Inertial Signals/{signal}_{subset}.txt'\n",
    "    x = my_read_csv(filename).as_matrix()\n",
    "    return x \n",
    "\n",
    "def load_signals(data_dir, subset, flatten = False):\n",
    "    signals_data = []\n",
    "    for signal in SIGNALS:\n",
    "        signals_data.append(load_signal(data_dir, subset, signal)) \n",
    "    \n",
    "    if flatten :\n",
    "        X = np.hstack(signals_data)\n",
    "    else:\n",
    "        X = np.transpose(signals_data, (1, 2, 0))\n",
    "        \n",
    "    return X \n",
    "\n",
    "def load_y(data_dir, subset, dummies = False):\n",
    "    filename = f'{data_dir}/{subset}/y_{subset}.txt'\n",
    "    y = my_read_csv(filename)[0]\n",
    "    \n",
    "    \n",
    "    if dummies:\n",
    "        Y = pd.get_dummies(y).as_matrix()\n",
    "    else:\n",
    "        Y = y.as_matrix()\n",
    "    \n",
    "    return Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lecture des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Multidimensional Data\n",
    "X_train, X_test = load_signals(DATADIR_UCI, 'train'), load_signals(DATADIR_UCI, 'test')\n",
    "# Flattened Data\n",
    "X_train_flatten, X_test_flatten = load_signals(DATADIR_UCI, 'train', flatten=True), load_signals(DATADIR_UCI, 'test', flatten=True)\n",
    "\n",
    "# Label Y\n",
    "Y_train_label, Y_test_label = load_y(DATADIR_UCI, 'train', dummies = False), load_y(DATADIR_UCI, 'test', dummies = False)\n",
    "#Dummies Y (For Keras)\n",
    "Y_train_dummies, Y_test_dummies = load_y(DATADIR_UCI, 'train', dummies = True), load_y(DATADIR_UCI, 'test', dummies = True)\n",
    "\n",
    "N_train = X_train.shape[0]\n",
    "N_test = X_test.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vérification des dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Dimension\")\n",
    "print(\"Données Multidimensionelles, : \" + str(X_train.shape))\n",
    "print(\"Données Unimensionelles, : \" + str(X_train_flatten.shape))\n",
    "print(\"Vecteur réponse (scikit-learn) : \" + str(Y_train_label.shape))\n",
    "print(\"Matrice réponse(Keras) : \" + str(Y_train_dummies.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Visualisations\n",
    "### 3.1 Fonctions utiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "CMAP = plt.get_cmap(\"Accent\")\n",
    "SIGNALS = [\"body_acc x\", \"body_acc y\", \"body_acc z\", \n",
    "                \"body_gyro x\", \"body_gyro y\", \"body_gyro z\", \n",
    "               \"total_acc x\", \"total_acc y\", \"total_acc z\"] \n",
    "\n",
    "\n",
    "ACTIVITY_DIC = {1 : \"WALKING\",\n",
    "2 : \"WALKING UPSTAIRS\",\n",
    "3 : \"WALKING DOWNSTAIRS\",\n",
    "4 : \"SITTING\",\n",
    "5 : \"STANDING\",\n",
    "6 : \"LAYING\"}\n",
    "\n",
    "def plot_one_axe(X, fig, ax, sample_to_plot, cmap):\n",
    "    for act,Xgb in X.groupby(\"Activity\"):\n",
    "        Xgb_first_values = Xgb.values[:sample_to_plot,:-1]\n",
    "        x = Xgb_first_values[0]\n",
    "        ax.plot(x, linewidth=1, color=cmap(act-1), label = label_dic[act])\n",
    "        for x in Xgb_first_values[1:]:\n",
    "            ax.plot(x, linewidth=1, color=cmap(act-1))\n",
    "\n",
    "            \n",
    "def plot_one_axe_shuffle(X, fig, ax, sample_to_plot, cmap):\n",
    "    plot_data = []\n",
    "    for act,Xgb in X.groupby(\"Activity\"):\n",
    "        Xgb_first_values = Xgb.values[:sample_to_plot,:-1]\n",
    "        x = Xgb_first_values[0]\n",
    "        ax.plot(x, linewidth=1, color=cmap(act-1), label = label_dic[act])\n",
    "        for x in Xgb_first_values[1:]:\n",
    "            plot_data.append([x,cmap(act-1)])\n",
    "    random.shuffle(plot_data)\n",
    "    for x,color in plot_data:\n",
    "        ax.plot(x, linewidth=1, color=color)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 3.2 Tous les signaux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_to_plot = 50\n",
    "index_per_act = [list(zip(np.repeat(act, sample_to_plot), np.where(Y_train_label==act)[0][:sample_to_plot])) for act in range(1,7)]\n",
    "index_to_plot = list(itertools.chain.from_iterable(index_per_act))\n",
    "random.shuffle(index_to_plot)\n",
    "\n",
    "fig = plt.figure(figsize=(15,10))\n",
    "for isignal in range(9):\n",
    "    ax = fig.add_subplot(3,3,isignal+1)\n",
    "    for act , i in index_to_plot:\n",
    "        ax.plot(range(128), X_train[i,:,isignal],color=CMAP(act-1), linewidth=1)\n",
    "        ax.set_title(SIGNALS[isignal])\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Par signal "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_to_plot = 10\n",
    "isignal = 1\n",
    "index_per_act_dict = dict([(act, np.where(Y_train_label==act)[0][:sample_to_plot]) for act in range(1,7)])\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(15,8), num=SIGNALS[isignal])\n",
    "for act , index in index_per_act_dict.items():\n",
    "    ax = fig.add_subplot(3,2,act)\n",
    "    for x in X_train[index]:\n",
    "        ax.plot(range(128), x[:,0],color=CMAP(act-1), linewidth=1)\n",
    "    ax.set_title(ACTIVITY_DIC[act])\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q** Observer l'un des signaux, par exemple `Walking upstairs`. Qu'est ce qui fait que, pour ces signaux ou courbes, une métrique euclidienne classique ($L_2$) est inopérante? \n",
    "\n",
    "**Q** Corrélativement pourquoi est-il intéressant de décomposer un signal dans le domaine des fréquances?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_pca(X_R, ytrain, fig, ax, nbc, nbc2, label_dic=ACTIVITY_DIC, cmaps = plt.get_cmap(\"Accent\")\n",
    "):\n",
    "    for i in range(6):\n",
    "        xs = X_R[ytrain==i+1,nbc-1]\n",
    "        ys = X_R[ytrain==i+1, nbc2-1]\n",
    "        label = label_dic[i+1]\n",
    "        color = cmaps(i)\n",
    "        ax.scatter(xs, ys, color=color, alpha=.8, s=10, label=label)\n",
    "        ax.set_xlabel(\"PC%d : %.2f %%\" %(nbc,pca.explained_variance_ratio_[nbc-1]*100), fontsize=15)\n",
    "        ax.set_ylabel(\"PC%d : %.2f %%\" %(nbc2,pca.explained_variance_ratio_[nbc2-1]*100), fontsize=15)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Analyse en composantes principales\n",
    "Les ACP ne sont pas réduites. Refaire les calculs après réduction des variables : signaux.\n",
    "\n",
    "### 4.1 Sur un Signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = sdec.PCA()\n",
    "isignal = 4\n",
    "signal = SIGNALS[isignal]\n",
    "print(\"ACP Sur signal : \" +signal)\n",
    "X_r = pca.fit_transform(X_train[:,:,isignal])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15,10))\n",
    "ax = fig.add_subplot(1,2,1)\n",
    "ax.bar(range(10), pca.explained_variance_ratio_[:10]*100, align='center',\n",
    "        color='grey', ecolor='black')\n",
    "ax.set_xticks(range(10))\n",
    "ax.set_ylabel(\"Variance\")\n",
    "ax.set_title(\"\", fontsize=35)\n",
    "ax.set_title(\"Pourcentage de variance expliquée \\n par les premières composantes\", fontsize=20)\n",
    "\n",
    "ax = fig.add_subplot(1,2,2)\n",
    "box=ax.boxplot(X_r[:,0:10],whis=100)\n",
    "ax.set_title(\"Distribution des premières composantes\", fontsize=20)\n",
    "\n",
    "fig.suptitle(\"Résultat ACP sur Signal : \" + signal, fontsize=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Attetnion**: les diagrammes boîtes sont très perturbés pas les distributions des composantes avec une très forte concentration autour de 0 et énormément de valeurs atypiques. D'où l'utilisation du paramètre `whis=100` pour rallonger les moustaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,10), )\n",
    "ax = fig.add_subplot(1,1,1)\n",
    "plot_pca(X_r, Y_train_label,fig ,ax ,1 ,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Sur tous les signaux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = sdec.PCA()\n",
    "print(\"ACP Sur tous les signaux\")\n",
    "X_r = pca.fit_transform(X_train_flatten)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15,10))\n",
    "ax = fig.add_subplot(1,2,1)\n",
    "ax.bar(range(10), pca.explained_variance_ratio_[:10]*100, align='center',\n",
    "        color='grey', ecolor='black')\n",
    "ax.set_xticks(range(10))\n",
    "ax.set_ylabel(\"Variance\")\n",
    "ax.set_title(\"\", fontsize=35)\n",
    "ax.set_title(\"Pourcentage de variance expliqué \\n des premières composantes\", fontsize=20)\n",
    "\n",
    "ax = fig.add_subplot(1,2,2)\n",
    "box=ax.boxplot(X_r[:,0:10])\n",
    "ax.set_title(\"Distribution des premières composante\", fontsize=20)\n",
    "\n",
    "fig.suptitle(\"Résultat ACP sur Signal : \" + signal, fontsize=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,10), )\n",
    "ax = fig.add_subplot(1,1,1)\n",
    "plot_pca(X_r, Y_train_label,fig ,ax ,1 ,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 Analyse factorielle discriminante\n",
    "\n",
    "### 5.1 Sur un signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "isignal = 0\n",
    "signal = SIGNALS[isignal]\n",
    "print(\"LDA Sur signal : \" +signal)\n",
    "\n",
    "method = sda.LinearDiscriminantAnalysis() \n",
    "lda=method.fit(X_train[:,:,isignal],Y_train_label)\n",
    "X_r2=lda.transform(X_train[:,:,isignal])\n",
    "\n",
    "fig = plt.figure(figsize= (20,20))\n",
    "count = 0\n",
    "for count,(nbc, nbc2)in enumerate([(1,2), (2,3), (3,4), (1,3), (2,4), (1,4)]) :\n",
    "    ax = fig.add_subplot(3,3,count+1)\n",
    "    plot_pca(X_r2, Y_train_label , fig,ax,nbc,nbc2)\n",
    "plt.legend(loc='upper right', bbox_to_anchor=(1.5, 2.2))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2  Sur tous les signaux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"LDA Sur tous les signaux\")\n",
    "\n",
    "method = sda.LinearDiscriminantAnalysis() \n",
    "lda=method.fit(X_train_flatten,Y_train_label)\n",
    "X_r2=lda.transform(X_train_flatten)\n",
    "\n",
    "fig = plt.figure(figsize= (20,20))\n",
    "count = 0\n",
    "for count,(nbc, nbc2)in enumerate([(1,2), (2,3), (3,4), (1,3), (2,4), (1,4)]) :\n",
    "    ax = fig.add_subplot(3,3,count+1)\n",
    "    plot_pca(X_r2, Y_train_label , fig,ax,nbc,nbc2)\n",
    "plt.legend(loc='upper right', bbox_to_anchor=(1.5, 2.2))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
